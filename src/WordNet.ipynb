{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "a dull unattractive unpleasant girl or woman\n",
      "informal term for a man\n",
      "someone who is morally reprehensible\n",
      "a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
      "a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      "metal supports for logs in a fireplace\n",
      "go after with the intent to catch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deeksha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#bypasses certification issue\n",
    "#comment this out if you don't have any problem\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#download WordNet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    " \n",
    "#check for an example\n",
    "from nltk.corpus import wordnet\n",
    "synsets = wordnet.synsets('dog')\n",
    "\n",
    "for synset in synsets:\n",
    "    print(synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Define the target words\n",
    "target_words = ['good', 'bad', 'sad', 'happy', 'awesome', 'scary']\n",
    "\n",
    "#dictionary to store the related words for each target word\n",
    "#keys = target words\n",
    "#values = related words\n",
    "semantic_lexicon_dict = {}\n",
    "\n",
    "# Use WordNet to find related words through various semantic relations for each target word\n",
    "#loop over the target words\n",
    "for word in target_words:\n",
    "    #initialise empty related_words list which will hold \n",
    "    #all the semantically related words extracted from WordNet\n",
    "    related_words = []\n",
    "    #iterate through each synset (set of synonyms)\n",
    "    for syn in wn.synsets(word):\n",
    "        #iterate thorugh each lemma for the synset and append to related_words\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                related_words.append(lemma.name())\n",
    "        #check antonym for first lemma of the synset and append\n",
    "        if syn.lemmas()[0].antonyms():\n",
    "            related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "        #check for hyponym and append\n",
    "        for hypo in syn.hyponyms():\n",
    "            for lemma in hypo.lemmas():\n",
    "                related_words.append(lemma.name())\n",
    "        #check for hypernym and append\n",
    "        for hyper in syn.hypernyms():\n",
    "            for lemma in hyper.lemmas():\n",
    "                related_words.append(lemma.name())\n",
    "        #check for meronym and append\n",
    "        for part in syn.part_meronyms():\n",
    "            for lemma in part.lemmas():\n",
    "                related_words.append(lemma.name())\n",
    "        #check for holonym and append\n",
    "        for whole in syn.part_holonyms():\n",
    "            for lemma in whole.lemmas():\n",
    "                related_words.append(lemma.name())\n",
    "        #iterate through each lemma for the current synset\n",
    "        #for each lemma not the same as target word\n",
    "        #find all lemmas that have same spelling and append\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.name() != word:\n",
    "                homonyms = wn.lemmas(lemma.name())\n",
    "                for homonym in homonyms:\n",
    "                    related_words.append(homonym.name())\n",
    "    semantic_lexicon_dict[word] = related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': ['common_good', 'commonweal', 'advantage', 'vantage', 'goodness', 'evil', 'beneficence', 'benignity', 'benignancy', 'graciousness', 'kindness', 'saintliness', 'summum_bonum', 'virtue', 'virtuousness', 'moral_excellence', 'virtue', 'morality', 'goodness', 'goodness', 'goodness', 'bad', 'benefit', 'welfare', 'better', 'better', 'desirability', 'desirableness', 'optimum', 'wisdom', 'wiseness', 'soundness', 'worthiness', 'quality', 'goodness', 'goodness', 'commodity', 'trade_good', 'basic', 'staple', 'consumer_goods', 'drygoods', 'soft_goods', 'entrant', 'export', 'exportation', 'fancy_goods', 'fungible', 'future', 'import', 'importation', 'merchandise', 'ware', 'product', 'middling', 'salvage', 'shopping', 'sporting_goods', 'worldly_possession', 'worldly_good', 'artifact', 'artefact', 'commodity', 'trade_good', 'bad', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'evil', 'estimable', 'honorable', 'respectable', 'estimable', 'estimable', 'estimable', 'honorable', 'honorable', 'honorable', 'honorable', 'respectable', 'respectable', 'respectable', 'beneficial', 'beneficial', 'just', 'upright', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'upright', 'upright', 'upright', 'upright', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'adept', 'adept', 'expert', 'expert', 'expert', 'practiced', 'practiced', 'proficient', 'proficient', 'skillful', 'skillful', 'skilful', 'dear', 'near', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'dependable', 'safe', 'secure', 'dependable', 'dependable', 'dependable', 'dependable', 'safe', 'safe', 'safe', 'safe', 'safe', 'safe', 'safe', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'right', 'ripe', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'ripe', 'ripe', 'ripe', 'ripe', 'ripe', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'effective', 'in_effect', 'in_force', 'effective', 'effective', 'effective', 'effective', 'effective', 'effective', 'in_effect', 'in_effect', 'in_force', 'serious', 'serious', 'serious', 'serious', 'serious', 'serious', 'serious', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'salutary', 'salutary', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'undecomposed', 'unspoiled', 'unspoiled', 'unspoilt', 'well', 'ill', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'thoroughly', 'soundly', 'thoroughly', 'thoroughly', 'soundly', 'soundly'], 'bad': ['badness', 'good', 'evil', 'inadvisability', 'liability', 'undesirability', 'unsoundness', 'unworthiness', 'worse', 'quality', 'badness', 'badness', 'badness', 'good', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'spoiled', 'spoilt', 'spoiled', 'spoiled', 'spoilt', 'spoilt', 'spoilt', 'regretful', 'sorry', 'unregretful', 'regretful', 'sorry', 'sorry', 'sorry', 'sorry', 'uncollectible', 'uncollectible', 'risky', 'high-risk', 'speculative', 'risky', 'risky', 'high-risk', 'speculative', 'speculative', 'speculative', 'unfit', 'unsound', 'unfit', 'unfit', 'unfit', 'unfit', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'forged', 'forged', 'defective', 'defective', 'defective', 'defective', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly'], 'sad': ['glad', 'deplorable', 'distressing', 'lamentable', 'pitiful', 'sorry', 'deplorable', 'deplorable', 'deplorable', 'distressing', 'distressing', 'lamentable', 'pitiful', 'pitiful', 'pitiful', 'sorry', 'sorry', 'sorry', 'sorry'], 'happy': ['unhappy', 'felicitous', 'felicitous', 'felicitous', 'glad', 'glad', 'glad', 'glad', 'glad', 'glad', 'well-chosen', 'well-chosen'], 'awesome': ['amazing', 'awe-inspiring', 'awful', 'awing', 'amazing', 'amazing', 'awe-inspiring', 'awful', 'awful', 'awful', 'awful', 'awful', 'awful', 'awful', 'awing'], 'scary': ['chilling', 'scarey', 'shivery', 'shuddery', 'chilling', 'chilling', 'scarey', 'shivery', 'shivery', 'shuddery']}\n",
      "['badness', 'good', 'evil', 'inadvisability', 'liability', 'undesirability', 'unsoundness', 'unworthiness', 'worse', 'quality', 'badness', 'badness', 'badness', 'good', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'spoiled', 'spoilt', 'spoiled', 'spoiled', 'spoilt', 'spoilt', 'spoilt', 'regretful', 'sorry', 'unregretful', 'regretful', 'sorry', 'sorry', 'sorry', 'sorry', 'uncollectible', 'uncollectible', 'risky', 'high-risk', 'speculative', 'risky', 'risky', 'high-risk', 'speculative', 'speculative', 'speculative', 'unfit', 'unsound', 'unfit', 'unfit', 'unfit', 'unfit', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'forged', 'forged', 'defective', 'defective', 'defective', 'defective', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly']\n"
     ]
    }
   ],
   "source": [
    "#prints entire semantic lexicon represented as a dictionary\n",
    "print(semantic_lexicon_dict)\n",
    "\n",
    "#prints one example from the semantic lexicon\n",
    "print(semantic_lexicon_dict['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic lexicon as a matrix\n",
    "\n",
    "#a set (set because duplicates are removed) of unique semantic relations\n",
    "#these relations will serve as columns in the matrix\n",
    "semantic_relations = set()\n",
    "for rel_word in semantic_lexicon_dict.values():\n",
    "    semantic_relations.update(rel_word)\n",
    "#print(semantic_relations)\n",
    "\n",
    "#a list (list because we want order of input to be preserved) \n",
    "# of unique target words that will serve as rows in the matrix\n",
    "target_words_list = list(target_words)\n",
    "#print(target_words_list)\n",
    "\n",
    "#matrix filled with zeroes\n",
    "#each row = one target word\n",
    "#each column = one semantic relation (extracted from WordNet)\n",
    "'''to manipulate dimension of matrix'''\n",
    "semantic_lexicon_matrix = np.zeros((len(target_words_list), len(semantic_relations)), dtype=int)\n",
    "\n",
    "#loop over the target_words_list and gets the word and its index\n",
    "for i, target_word in enumerate(target_words_list):\n",
    "    #retrieve semantic related words for current target word from the semantic_lexicon_dict\n",
    "    related_words = semantic_lexicon_dict.get(target_word)\n",
    "    #loop over eaach related word for the current target word\n",
    "    for related_word in related_words:\n",
    "        #get the index of the related_woord from the semantic_relations\n",
    "        #converted to list so that index() can be used\n",
    "        j = list(semantic_relations).index(related_word)\n",
    "        #set element (i, j) to 1 to indicate relationship between\n",
    "        #the target word and the semantically related word\n",
    "        semantic_lexicon_matrix[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
      "  0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n",
      "  1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1]\n",
      " [1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#returns the semantic lexicon as a matrix\n",
    "#with each row representing a target word\n",
    "#and each column representing a semantically related word from WordNet \n",
    "print(semantic_lexicon_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
