{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deeksha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bypasses certification issue\n",
    "#comment this out if you don't have any problem\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#download WordNet\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "a dull unattractive unpleasant girl or woman\n",
      "informal term for a man\n",
      "someone who is morally reprehensible\n",
      "a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
      "a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      "metal supports for logs in a fireplace\n",
      "go after with the intent to catch\n"
     ]
    }
   ],
   "source": [
    "#check for an example\n",
    "#from nltk.corpus import wordnet\n",
    "synsets = wn.synsets('dog')\n",
    "\n",
    "#check for definition of \"dog\"\n",
    "for synset in synsets:\n",
    "    print(synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating a semantic lexicon dictionary \n",
    "def semantic_lexicon_dict(target_words):\n",
    "    #dictionary to store the related words for each target word\n",
    "    #keys = target words\n",
    "    #values = related words\n",
    "    lexicon = {}\n",
    "\n",
    "    #Use WordNet to find related words through various semantic relations for each target word\n",
    "    #loop over the target words\n",
    "    for word in target_words:\n",
    "        #initialise empty related_words list which will hold \n",
    "        #all the semantically related words extracted from WordNet\n",
    "        related_words = []\n",
    "        #iterate through each synset (set of synonyms)\n",
    "        for syn in wn.synsets(word):\n",
    "            #iterate thorugh each lemma for the synset and append to related_words\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    related_words.append(lemma.name())\n",
    "            #check antonym for first lemma of the synset and append\n",
    "            if syn.lemmas()[0].antonyms():\n",
    "                related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            #check for hyponym and append\n",
    "            for hypo in syn.hyponyms():\n",
    "                for lemma in hypo.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #check for hypernym and append\n",
    "            for hyper in syn.hypernyms():\n",
    "                for lemma in hyper.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #check for meronym and append\n",
    "            for part in syn.part_meronyms():\n",
    "                for lemma in part.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #check for holonym and append\n",
    "            for whole in syn.part_holonyms():\n",
    "                for lemma in whole.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #iterate through each lemma for the current synset\n",
    "            #for each lemma not the same as target word\n",
    "            #find all lemmas that have same spelling and append\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    homonyms = wn.lemmas(lemma.name())\n",
    "                    for homonym in homonyms:\n",
    "                        related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating a semantic lexicon matrix\n",
    "def semantic_lexicon_mat(semantic_lexicon_dictionary, target_words_list, NEW_ROWS, NEW_COLUMNS):\n",
    "    #a set (set because duplicates are removed) of unique semantic relations\n",
    "    #these relations will serve as columns in the matrix\n",
    "    semantic_relations = set()\n",
    "    for rel_word in semantic_lexicon_dictionary.values():\n",
    "        semantic_relations.update(rel_word)\n",
    "    \n",
    "    #Create a matrix filled with zeroes\n",
    "    #Each row represents a target word\n",
    "    #Each column represents a semantic relation\n",
    "    ROWS = target_words_list\n",
    "    COLUMNS = semantic_relations\n",
    "    semantic_lexicon_matrix = np.zeros((len(ROWS), len(COLUMNS)), dtype=int)\n",
    "    \n",
    "    #loop over the target_words_list and gets the word and its index\n",
    "    for i, target_word in enumerate(target_words_list):\n",
    "        #retrieve semantic related words for current target word from the semantic_lexicon_dict\n",
    "        related_words = semantic_lexicon_dictionary.get(target_word, [])\n",
    "        #loop over each related word for the current target word\n",
    "        for related_word in related_words:\n",
    "            #get the index of the related word from the semantic_relations\n",
    "            #converted to list so that index() can be used\n",
    "            j = list(semantic_relations).index(related_word)\n",
    "            #set the element (i, j) to 1 to indicate the relationship between\n",
    "            #the target word and the semantically related word\n",
    "            semantic_lexicon_matrix[i, j] = 1\n",
    "    \n",
    "    return semantic_lexicon_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': ['common_good', 'commonweal', 'advantage', 'vantage', 'goodness', 'evil', 'beneficence', 'benignity', 'benignancy', 'graciousness', 'kindness', 'saintliness', 'summum_bonum', 'virtue', 'virtuousness', 'moral_excellence', 'virtue', 'morality', 'goodness', 'goodness', 'goodness', 'bad', 'benefit', 'welfare', 'better', 'better', 'desirability', 'desirableness', 'optimum', 'wisdom', 'wiseness', 'soundness', 'worthiness', 'quality', 'goodness', 'goodness', 'commodity', 'trade_good', 'basic', 'staple', 'consumer_goods', 'drygoods', 'soft_goods', 'entrant', 'export', 'exportation', 'fancy_goods', 'fungible', 'future', 'import', 'importation', 'merchandise', 'ware', 'product', 'middling', 'salvage', 'shopping', 'sporting_goods', 'worldly_possession', 'worldly_good', 'artifact', 'artefact', 'commodity', 'trade_good', 'bad', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'evil', 'estimable', 'honorable', 'respectable', 'estimable', 'estimable', 'estimable', 'honorable', 'honorable', 'honorable', 'honorable', 'respectable', 'respectable', 'respectable', 'beneficial', 'beneficial', 'just', 'upright', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'upright', 'upright', 'upright', 'upright', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'adept', 'adept', 'expert', 'expert', 'expert', 'practiced', 'practiced', 'proficient', 'proficient', 'skillful', 'skillful', 'skilful', 'dear', 'near', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'dependable', 'safe', 'secure', 'dependable', 'dependable', 'dependable', 'dependable', 'safe', 'safe', 'safe', 'safe', 'safe', 'safe', 'safe', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'right', 'ripe', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'ripe', 'ripe', 'ripe', 'ripe', 'ripe', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'effective', 'in_effect', 'in_force', 'effective', 'effective', 'effective', 'effective', 'effective', 'effective', 'in_effect', 'in_effect', 'in_force', 'serious', 'serious', 'serious', 'serious', 'serious', 'serious', 'serious', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'salutary', 'salutary', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'undecomposed', 'unspoiled', 'unspoiled', 'unspoilt', 'well', 'ill', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'thoroughly', 'soundly', 'thoroughly', 'thoroughly', 'soundly', 'soundly'], 'bad': ['badness', 'good', 'evil', 'inadvisability', 'liability', 'undesirability', 'unsoundness', 'unworthiness', 'worse', 'quality', 'badness', 'badness', 'badness', 'good', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'spoiled', 'spoilt', 'spoiled', 'spoiled', 'spoilt', 'spoilt', 'spoilt', 'regretful', 'sorry', 'unregretful', 'regretful', 'sorry', 'sorry', 'sorry', 'sorry', 'uncollectible', 'uncollectible', 'risky', 'high-risk', 'speculative', 'risky', 'risky', 'high-risk', 'speculative', 'speculative', 'speculative', 'unfit', 'unsound', 'unfit', 'unfit', 'unfit', 'unfit', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'forged', 'forged', 'defective', 'defective', 'defective', 'defective', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly'], 'sad': ['glad', 'deplorable', 'distressing', 'lamentable', 'pitiful', 'sorry', 'deplorable', 'deplorable', 'deplorable', 'distressing', 'distressing', 'lamentable', 'pitiful', 'pitiful', 'pitiful', 'sorry', 'sorry', 'sorry', 'sorry'], 'happy': ['unhappy', 'felicitous', 'felicitous', 'felicitous', 'glad', 'glad', 'glad', 'glad', 'glad', 'glad', 'well-chosen', 'well-chosen'], 'awesome': ['amazing', 'awe-inspiring', 'awful', 'awing', 'amazing', 'amazing', 'awe-inspiring', 'awful', 'awful', 'awful', 'awful', 'awful', 'awful', 'awful', 'awing'], 'scary': ['chilling', 'scarey', 'shivery', 'shuddery', 'chilling', 'chilling', 'scarey', 'shivery', 'shivery', 'shuddery']}\n",
      "['badness', 'good', 'evil', 'inadvisability', 'liability', 'undesirability', 'unsoundness', 'unworthiness', 'worse', 'quality', 'badness', 'badness', 'badness', 'good', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'spoiled', 'spoilt', 'spoiled', 'spoiled', 'spoilt', 'spoilt', 'spoilt', 'regretful', 'sorry', 'unregretful', 'regretful', 'sorry', 'sorry', 'sorry', 'sorry', 'uncollectible', 'uncollectible', 'risky', 'high-risk', 'speculative', 'risky', 'risky', 'high-risk', 'speculative', 'speculative', 'speculative', 'unfit', 'unsound', 'unfit', 'unfit', 'unfit', 'unfit', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'forged', 'forged', 'defective', 'defective', 'defective', 'defective', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly']\n"
     ]
    }
   ],
   "source": [
    "#Define the target words (sentiment analysis task)\n",
    "\n",
    "target_words = ['good', 'bad', 'sad', 'happy', 'awesome', 'scary']\n",
    "\n",
    "#Create the semantic lexicon dictionary by passing the target words as argument\n",
    "semantic_lexicon_dictionary = semantic_lexicon_dict(target_words)\n",
    "\n",
    "#prints the semantic lexicon dictionary\n",
    "print(semantic_lexicon_dictionary)\n",
    "\n",
    "#check the semantically related words for \"bad\"\n",
    "print(semantic_lexicon_dictionary.get(\"bad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
      "  1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
      "  1 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
      "  1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1\n",
      "  0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#uses the same semantic lexicon dictionary created earlier\n",
    "#assigned to semantic_lexicon just for readability\n",
    "semantic_lexicon = semantic_lexicon_dictionary\n",
    "\n",
    "#adjust dimensionality of matrix\n",
    "'''dimensionality to be manipulated here to match word embeddings'''\n",
    "NEW_ROWS = 100\n",
    "NEW_COLUMNS = 150\n",
    "\n",
    "#create the semantic lexicon matrix by passing the semantic lexicon dictionary and target words as arguments\n",
    "semantic_lexicon_matrix = semantic_lexicon_mat(semantic_lexicon, target_words, NEW_ROWS, NEW_COLUMNS)\n",
    "\n",
    "#returns the semantic lexicon as a matrix\n",
    "#with each row representing a target word\n",
    "#and each column representing a semantically related word from WordNet \n",
    "print(semantic_lexicon_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#numpy.ndarray = dense matrix\n",
    "print(type(semantic_lexicon_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
