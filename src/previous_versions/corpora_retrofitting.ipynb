{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\ninan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import urllib.request\n",
    "import io\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, matutils\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()\n",
    "  \n",
    "\n",
    "  \n",
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    first_line = True\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # Skip the first line\n",
    "      if first_line:\n",
    "        first_line =False\n",
    "        continue\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "  ''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptut file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()\n",
    "\n",
    "''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.lower().strip().split()\n",
    "            lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the same format for the toy corpus as for the provided word embeddings\n",
    "def convert_matrix_to_dict(wordVecMat, wordList):\n",
    "    wordVecs = {}\n",
    "\n",
    "    for i, word in enumerate(wordList):\n",
    "        wordVecs[word] = wordVecMat[i]\n",
    "\n",
    "    return wordVecs\n",
    "\n",
    "def convert_dict_to_matrix(wordVecs):\n",
    "    wordVecMat = np.stack(list(wordVecs.values()))\n",
    "    return wordVecMat\n",
    "\n",
    "def vectorize_list(corpus):\n",
    "    corpus_vecs = [model[word] for word in corpus]\n",
    "\n",
    "    return corpus_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same input format as the real corpus\n",
    "toy_corpus = [\"cat\", \"tiger\", \"computer\", \"keyboard\", \"plane\", \"car\", \"doctor\", \"nurse\", \"love\", \"sex\"]\n",
    "toy_corpus_list_vecs = vectorize_list(toy_corpus)\n",
    "toy_wordVecs = convert_matrix_to_dict(toy_corpus_list_vecs, toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    similarity = dot_product / norm_product\n",
    "    return similarity\n",
    "\n",
    "def generate_cosine_similarity_matrix(dict_vecs): \n",
    "    num_vectors = len(dict_vecs)\n",
    "    similarity_matrix = np.zeros((num_vectors, num_vectors))\n",
    "    for i, word1 in enumerate(dict_vecs):\n",
    "        for j, word2 in enumerate(dict_vecs):\n",
    "            similarity_matrix[i, j] = calculate_cosine_similarity(dict_vecs[word1], dict_vecs[word2])\n",
    "    return similarity_matrix\n",
    "\n",
    "def print_vec_similarities(wordList, similarity_matrix):\n",
    "    for word, vec in zip(wordList, similarity_matrix):\n",
    "        print(f'Similarities with \"{word}\":')\n",
    "        for i in range(len(vec)):\n",
    "            similarity = vec[i]\n",
    "            print(f'  - \"{wordList[i]}\": {similarity:.4f}')\n",
    "        print()\n",
    "\n",
    "def print_similarity_difference(similarity_matrix, retrofitted_similarity_matrix):\n",
    "    difference = np.abs(similarity_matrix - retrofitted_similarity_matrix)\n",
    "    print(\"Similarity Difference Matrix:\")\n",
    "    print(difference)\n",
    "\n",
    "def cosine_similarity_matrix(matrix1, matrix2):\n",
    "    dot_product = np.sum(matrix1 * matrix2)\n",
    "    norm_matrix1 = np.linalg.norm(matrix1)\n",
    "    norm_matrix2 = np.linalg.norm(matrix2)\n",
    "    cosine_similarity = dot_product / (norm_matrix1 * norm_matrix2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5173\n",
      "  - \"computer\": 0.1732\n",
      "  - \"keyboard\": 0.1834\n",
      "  - \"plane\": 0.1833\n",
      "  - \"car\": 0.2153\n",
      "  - \"doctor\": 0.1292\n",
      "  - \"nurse\": 0.1594\n",
      "  - \"love\": 0.1406\n",
      "  - \"sex\": 0.1368\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0677\n",
      "  - \"keyboard\": 0.0654\n",
      "  - \"plane\": 0.1660\n",
      "  - \"car\": 0.1672\n",
      "  - \"doctor\": 0.0835\n",
      "  - \"nurse\": 0.1111\n",
      "  - \"love\": 0.0871\n",
      "  - \"sex\": 0.2222\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732\n",
      "  - \"tiger\": 0.0677\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.3964\n",
      "  - \"plane\": 0.1909\n",
      "  - \"car\": 0.2461\n",
      "  - \"doctor\": 0.1628\n",
      "  - \"nurse\": 0.2178\n",
      "  - \"love\": 0.0573\n",
      "  - \"sex\": 0.1853\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834\n",
      "  - \"tiger\": 0.0654\n",
      "  - \"computer\": 0.3964\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1006\n",
      "  - \"car\": 0.1498\n",
      "  - \"doctor\": 0.0850\n",
      "  - \"nurse\": 0.1220\n",
      "  - \"love\": 0.1591\n",
      "  - \"sex\": 0.0943\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833\n",
      "  - \"tiger\": 0.1660\n",
      "  - \"computer\": 0.1909\n",
      "  - \"keyboard\": 0.1006\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.3780\n",
      "  - \"doctor\": 0.1879\n",
      "  - \"nurse\": 0.0978\n",
      "  - \"love\": 0.1080\n",
      "  - \"sex\": 0.0587\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153\n",
      "  - \"tiger\": 0.1672\n",
      "  - \"computer\": 0.2461\n",
      "  - \"keyboard\": 0.1498\n",
      "  - \"plane\": 0.3780\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1895\n",
      "  - \"nurse\": 0.1306\n",
      "  - \"love\": 0.0842\n",
      "  - \"sex\": 0.1169\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292\n",
      "  - \"tiger\": 0.0835\n",
      "  - \"computer\": 0.1628\n",
      "  - \"keyboard\": 0.0850\n",
      "  - \"plane\": 0.1879\n",
      "  - \"car\": 0.1895\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6320\n",
      "  - \"love\": 0.0831\n",
      "  - \"sex\": 0.1994\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594\n",
      "  - \"tiger\": 0.1111\n",
      "  - \"computer\": 0.2178\n",
      "  - \"keyboard\": 0.1220\n",
      "  - \"plane\": 0.0978\n",
      "  - \"car\": 0.1306\n",
      "  - \"doctor\": 0.6320\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.0631\n",
      "  - \"sex\": 0.1997\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406\n",
      "  - \"tiger\": 0.0871\n",
      "  - \"computer\": 0.0573\n",
      "  - \"keyboard\": 0.1591\n",
      "  - \"plane\": 0.1080\n",
      "  - \"car\": 0.0842\n",
      "  - \"doctor\": 0.0831\n",
      "  - \"nurse\": 0.0631\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.2639\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368\n",
      "  - \"tiger\": 0.2222\n",
      "  - \"computer\": 0.1853\n",
      "  - \"keyboard\": 0.0943\n",
      "  - \"plane\": 0.0587\n",
      "  - \"car\": 0.1169\n",
      "  - \"doctor\": 0.1994\n",
      "  - \"nurse\": 0.1997\n",
      "  - \"love\": 0.2639\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = generate_cosine_similarity_matrix(toy_wordVecs)\n",
    "print_vec_similarities(toy_corpus, similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_lexicon(target_words, relation_types):\n",
    "    lexicon = {}\n",
    "        \n",
    "    for word in target_words:\n",
    "        related_words = []\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Skip word if no synsets found\n",
    "        if not word_synsets:\n",
    "            continue\n",
    "\n",
    "        for syn in word_synsets:\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    if \"synonyms\" in relation_types:\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"antonyms\" in relation_types:\n",
    "                if syn.lemmas()[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            if \"hyponyms\" in relation_types:\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"hypernyms\" in relation_types:\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"meronyms\" in relation_types:\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"holonyms\" in relation_types:\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"homonyms\" in relation_types:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wordnet.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecMat = convert_dict_to_matrix(toy_wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(toy_corpus_list_vecs)) \n",
    "\n",
    "print(type(wordVecMat)) \n",
    "print(wordVecMat.shape)  \n",
    "print(wordVecMat.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 10)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(similarity_matrix)) \n",
    "print(similarity_matrix.shape)  \n",
    "print(similarity_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for the big corpus to retrive the word list from the keys\n",
    "def get_embeddings_words(wordVecs):\n",
    "    wordList = list(wordVecs.keys()) # TODO: or set?\n",
    "    return wordList\n",
    "\n",
    "wordList = get_embeddings_words(toy_wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighbors_embedding_matrix(wordList, relation_type):\n",
    "    # Retrieve synonyms for each word\n",
    "    neighbors_dict = get_wordnet_lexicon(wordList, relation_type)\n",
    "    \n",
    "    # Compute average embedding\n",
    "    average_embeddings = []\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        embeddings = [\n",
    "            model.get_vector(neighbor)\n",
    "            for neighbor in neighbors\n",
    "            if model.has_index_for(neighbor)\n",
    "        ]\n",
    "        if len(embeddings) > 0:\n",
    "            average_embedding = np.sum(embeddings, axis=0) / len(embeddings)\n",
    "        else:\n",
    "            # Handle the case where a word has no embeddings for its synonyms\n",
    "            average_embedding = np.zeros(model.vector_size)  # Use a zero vector\n",
    "        average_embeddings.append(average_embedding)\n",
    "    \n",
    "    # Create the word embedding matrix\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "\n",
    "    return neighbors_embedding_matrix\n",
    "\n",
    "   \n",
    "    \n",
    "neighbors_matrix = create_neighbors_embedding_matrix(wordList, \"synonyms\")\n",
    "\n",
    "# récupérer la liste des syn dans wordnet\n",
    "# vectorise chaque syn\n",
    "# BOW des synonymes (sum) pour n'avoir qu'un embedding \n",
    "# BOW_syn_cat\n",
    "# BOW_syn_dog= neighbors_matrix, shape (10, embedding_size) donc same size as wordVecs_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(neighbors_matrix))  # <class 'numpy.ndarray'>\n",
    "print(neighbors_matrix.shape)  # (m, n)\n",
    "print(neighbors_matrix.ndim)   # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat))  # <class 'numpy.ndarray'>\n",
    "print(wordVecMat.shape)  # (m, n)\n",
    "print(wordVecMat.ndim)   # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00676925  0.17245371 -0.32560504 -0.02995695  0.24918167  0.06591797\n",
      "  0.07754347  0.02062197  0.12026186 -0.24899179  0.11663705 -0.43598995\n",
      " -0.0165247  -0.43618888  0.05141873 -0.2046328   0.09973597  0.0461245\n",
      " -0.4420053   0.08787028  0.26338252 -0.1518453   0.11536521 -0.14642108\n",
      " -0.04931188  0.32405486 -0.15808557  0.31547716  0.40427201 -0.007934\n",
      " -0.00195312 -0.20903128 -0.03433369 -0.0849519   0.01340795  0.15198545\n",
      " -0.07754234  0.04350902  0.00148463  0.24564164  0.00093107  0.03174506\n",
      " -0.14347048  0.13828702  0.08990253 -0.03038646  0.37447442 -0.05503337\n",
      "  0.00611821  0.00164738 -0.15880896  0.13665545  0.34953252  0.14862174\n",
      " -0.0103189  -0.14141733  0.15832859 -0.30018898  0.35803392  0.18776052\n",
      "  0.28351056  0.18667716 -0.01274052  0.19207764  0.04131345  0.04772498\n",
      "  0.20808016  0.02882668 -0.10890771 -0.12867793  0.40634721 -0.03548855\n",
      " -0.15468343 -0.01318077 -0.11528298  0.38514766  0.11844098 -0.08239746\n",
      " -0.02005401 -0.06692618  0.08503554 -0.00346544  0.1995228  -0.07893711\n",
      " -0.34383251  0.04050474 -0.2636391  -0.14694101 -0.1783899  -0.0918257\n",
      " -0.0131429   0.04139088 -0.06987395 -0.25662345 -0.17748967 -0.27476671\n",
      " -0.01672363 -0.17666287 -0.33676034 -0.18375199 -0.24108435 -0.02301817\n",
      " -0.0048376   0.25613178  0.07612101 -0.20470513  0.0953064  -0.03844798\n",
      "  0.04892759 -0.11653646 -0.07707384  0.29826298  0.11636692  0.32834201\n",
      " -0.31238471 -0.00066913 -0.02505154 -0.23873901 -0.22574163 -0.04350577\n",
      " -0.03468753 -0.05264395 -0.30785455  0.02505323 -0.34844179  0.11226626\n",
      "  0.10000073  0.1189044   0.04702872 -0.14758527  0.13923419  0.04321967\n",
      " -0.15519799 -0.06542969 -0.02879789 -0.11844042 -0.02837584  0.09594444\n",
      "  0.10183038  0.10198523 -0.06081022  0.02417896 -0.15294054  0.04794516\n",
      "  0.06066442  0.15922716 -0.19703731  0.17048589 -0.12367983 -0.15320898\n",
      "  0.22272971  0.09056261 -0.31849727 -0.08702935 -0.13079947 -0.05045121\n",
      "  0.09113679  0.05342385  0.03645833  0.22512252 -0.10272612 -0.05701814\n",
      "  0.09566696 -0.13046604 -0.12130398  0.00603117 -0.04856138  0.21248373\n",
      "  0.17837637  0.02974899 -0.08555999 -0.03059218 -0.04345082  0.23660165\n",
      " -0.08013295  0.09815696  0.07034867 -0.12642416 -0.242515    0.27940539\n",
      " -0.16617132  0.01541816 -0.00099126  0.0524677  -0.03739194  0.07791251\n",
      "  0.20973601  0.3004286   0.12838844  0.06714884  0.02089154 -0.16434733\n",
      "  0.12680845  0.06278935 -0.01593244  0.184226    0.07969835 -0.18987359\n",
      " -0.11193918  0.18708067  0.06552409 -0.27893744 -0.0675354  -0.05583897\n",
      "  0.04787643  0.11910897 -0.01438636 -0.13045473  0.10323758 -0.26696325\n",
      " -0.40234262 -0.02973316 -0.0515894  -0.08254327 -0.11705977 -0.0447789\n",
      "  0.16213056 -0.31804127 -0.10855222  0.12037037  0.18612897  0.11705413\n",
      " -0.22371081 -0.27236599 -0.2145137  -0.07454851  0.07460587 -0.07304269\n",
      "  0.22288344  0.00952374  0.10791355  0.03955135 -0.03007846  0.04184525\n",
      " -0.28511386  0.05631058  0.27960488  0.02073415 -0.08864961  0.066971\n",
      "  0.23477738 -0.20298824  0.03927584  0.03217344  0.33202221  0.05400481\n",
      "  0.05493726 -0.036039    0.22249632 -0.13921215 -0.33608415  0.18813239\n",
      " -0.28481038  0.02863178 -0.04690326  0.27920871 -0.1286395   0.1268627\n",
      " -0.15716192  0.07503933 -0.08402846  0.04506429 -0.04430474 -0.1314799\n",
      "  0.06542573  0.24106775 -0.10216381  0.02992079 -0.27746017 -0.02798801\n",
      " -0.07106696  0.04219563 -0.15462466  0.13976994  0.05472141  0.09482377\n",
      "  0.1267994   0.16184228  0.17293295 -0.14490651  0.2610078   0.05841742\n",
      " -0.20719062  0.28466684  0.13873743 -0.08083654 -0.09813182  0.04846644\n",
      "  0.08657498  0.13033097 -0.25310149  0.11868851 -0.1590384   0.03733656\n",
      "  0.42320421 -0.01760412  0.18546778  0.49357605 -0.276461    0.01063255]\n"
     ]
    }
   ],
   "source": [
    "difference = toy_corpus_list_vecs[0] - neighbors_matrix[0]\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter):\n",
    "    # Create a deep copy of wordVecMat \n",
    "    newWordVecMat = np.copy(wordVecMat, order='K')\n",
    "    updates = []\n",
    "    \n",
    "    for _ in range(nb_iter):\n",
    "        # Calculate the number of neighbors for each word\n",
    "        # numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\n",
    "        \n",
    "        # Update the word embeddings using retrofitting formula\n",
    "        newWordVecMat = (alpha * newWordVecMat + beta * neighbors_mean_matrix) / (alpha + beta)\n",
    "\n",
    "        # Calculate the updates\n",
    "        update = newWordVecMat - wordVecMat\n",
    "        updates.append(update)\n",
    "\n",
    "        # Update the wordVecMat for the next iteration\n",
    "        wordVecMat = newWordVecMat\n",
    "        # TODO: calculer similarité après chaque itération\n",
    "        # Stoping criterion\n",
    "        if np.linalg.norm(updates) < 1e-2:\n",
    "            break # TODO: return the embedding\n",
    "\n",
    "    # Convert the matrix back to a dictionary of word vectors\n",
    "    # retrofitted_wordVecs = dict(zip(wordList, newWordVecMat))\n",
    "\n",
    "    return newWordVecMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = get_wordnet_lexicon(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'furbish_up': ['doctor'],\n",
       " 'dear': ['love'],\n",
       " 'sexuality': ['sex'],\n",
       " 'spew': ['cat'],\n",
       " 'enjoy': ['love'],\n",
       " 'Arabian_tea': ['cat'],\n",
       " \"cat-o'-nine-tails\": ['cat'],\n",
       " 'gender': ['sex'],\n",
       " 'dearest': ['love'],\n",
       " 'MD': ['doctor'],\n",
       " 'aeroplane': ['plane'],\n",
       " 'bed': ['love'],\n",
       " 'tiger': ['Panthera_tigris'],\n",
       " 'get_it_on': ['love'],\n",
       " 'sheet': ['plane'],\n",
       " 'sexual_activity': ['sex'],\n",
       " 'have_a_go_at_it': ['love'],\n",
       " 'big_cat': ['cat'],\n",
       " 'bonk': ['love'],\n",
       " 'railroad_car': ['car'],\n",
       " 'Doctor': ['doctor'],\n",
       " 'vomit': ['cat'],\n",
       " 'khat': ['cat'],\n",
       " 'arouse': ['sex'],\n",
       " 'flat': ['plane'],\n",
       " 'mend': ['doctor'],\n",
       " 'CAT': ['cat'],\n",
       " 'guy': ['cat'],\n",
       " 'nurse': ['hold',\n",
       "  'entertain',\n",
       "  'harbour',\n",
       "  'nursemaid',\n",
       "  'harbor',\n",
       "  'suck',\n",
       "  'nanny',\n",
       "  'suckle',\n",
       "  'breastfeed',\n",
       "  'lactate',\n",
       "  'give_suck',\n",
       "  'wet-nurse'],\n",
       " 'sleep_together': ['love'],\n",
       " 'bushel': ['doctor'],\n",
       " 'CT': ['cat'],\n",
       " 'medico': ['doctor'],\n",
       " 'suck': ['nurse'],\n",
       " 'data_processor': ['computer'],\n",
       " 'cat': ['regorge',\n",
       "  'regurgitate',\n",
       "  'spew',\n",
       "  'Arabian_tea',\n",
       "  'puke',\n",
       "  \"cat-o'-nine-tails\",\n",
       "  'disgorge',\n",
       "  'qat',\n",
       "  'true_cat',\n",
       "  'honk',\n",
       "  'retch',\n",
       "  'be_sick',\n",
       "  'chuck',\n",
       "  'vomit_up',\n",
       "  'big_cat',\n",
       "  'sick',\n",
       "  'vomit',\n",
       "  'computed_axial_tomography',\n",
       "  'khat',\n",
       "  'upchuck',\n",
       "  'Caterpillar',\n",
       "  'computed_tomography',\n",
       "  'computerized_tomography',\n",
       "  'cast',\n",
       "  'computerized_axial_tomography',\n",
       "  'CAT',\n",
       "  'quat',\n",
       "  'purge',\n",
       "  'guy',\n",
       "  'hombre',\n",
       "  'throw_up',\n",
       "  'CT',\n",
       "  'kat',\n",
       "  'bozo',\n",
       "  'barf',\n",
       "  'spue',\n",
       "  'African_tea'],\n",
       " 'bang': ['love'],\n",
       " 'railway_car': ['car'],\n",
       " 'hump': ['love'],\n",
       " 'love_life': ['love'],\n",
       " 'wind_up': ['sex'],\n",
       " 'automobile': ['car'],\n",
       " 'electronic_computer': ['computer'],\n",
       " 'sleep_with': ['love'],\n",
       " 'regorge': ['cat'],\n",
       " 'physician': ['doctor'],\n",
       " 'do_it': ['love'],\n",
       " 'sex_activity': ['sex'],\n",
       " 'disgorge': ['cat'],\n",
       " 'planing_machine': ['plane'],\n",
       " 'fix': ['doctor'],\n",
       " 'erotic_love': ['love'],\n",
       " 'retch': ['cat'],\n",
       " 'eff': ['love'],\n",
       " 'restore': ['doctor'],\n",
       " 'wet-nurse': ['nurse'],\n",
       " 'sick': ['cat'],\n",
       " 'gondola': ['car'],\n",
       " 'make_love': ['love'],\n",
       " 'computed_axial_tomography': ['cat'],\n",
       " 'Caterpillar': ['cat'],\n",
       " 'sexual_practice': ['sex'],\n",
       " 'cast': ['cat'],\n",
       " 'information_processing_system': ['computer'],\n",
       " 'purge': ['cat'],\n",
       " 'entertain': ['nurse'],\n",
       " 'throw_up': ['cat'],\n",
       " 'cable_car': ['car'],\n",
       " 'excite': ['sex'],\n",
       " 'harbour': ['nurse'],\n",
       " 'repair': ['doctor'],\n",
       " 'kat': ['cat'],\n",
       " 'sex': ['wind_up',\n",
       "  'sexuality',\n",
       "  'turn_on',\n",
       "  'arouse',\n",
       "  'excite',\n",
       "  'sexual_practice',\n",
       "  'sexual_urge',\n",
       "  'sexual_activity',\n",
       "  'sex_activity',\n",
       "  'gender'],\n",
       " 'touch_on': ['doctor'],\n",
       " 'doc': ['doctor'],\n",
       " 'barf': ['cat'],\n",
       " 'skim': ['plane'],\n",
       " 'have_sex': ['love'],\n",
       " 'elevator_car': ['car'],\n",
       " 'lovemaking': ['love'],\n",
       " 'African_tea': ['cat'],\n",
       " 'have_intercourse': ['love'],\n",
       " 'level': ['plane'],\n",
       " 'regurgitate': ['cat'],\n",
       " 'reckoner': ['computer'],\n",
       " 'Dr.': ['doctor'],\n",
       " 'nursemaid': ['nurse'],\n",
       " 'car': ['automobile',\n",
       "  'railroad_car',\n",
       "  'cable_car',\n",
       "  'railcar',\n",
       "  'auto',\n",
       "  'motorcar',\n",
       "  'railway_car',\n",
       "  'gondola',\n",
       "  'elevator_car',\n",
       "  'machine'],\n",
       " 'estimator': ['computer'],\n",
       " 'true_cat': ['cat'],\n",
       " 'honk': ['cat'],\n",
       " 'be_intimate': ['love'],\n",
       " 'turn_on': ['sex'],\n",
       " 'harbor': ['nurse'],\n",
       " 'computing_machine': ['computer'],\n",
       " 'chuck': ['cat'],\n",
       " 'sexual_love': ['love'],\n",
       " 'auto': ['car'],\n",
       " 'know': ['love'],\n",
       " 'jazz': ['love'],\n",
       " 'sexual_urge': ['sex'],\n",
       " 'passion': ['love'],\n",
       " 'computer': ['electronic_computer',\n",
       "  'reckoner',\n",
       "  'computing_machine',\n",
       "  'data_processor',\n",
       "  'computing_device',\n",
       "  'estimator',\n",
       "  'information_processing_system',\n",
       "  'figurer',\n",
       "  'calculator'],\n",
       " 'lie_with': ['love'],\n",
       " 'doctor_up': ['doctor'],\n",
       " 'nanny': ['nurse'],\n",
       " 'suckle': ['nurse'],\n",
       " 'bozo': ['cat'],\n",
       " 'lactate': ['nurse'],\n",
       " 'machine': ['car'],\n",
       " 'Panthera_tigris': ['tiger'],\n",
       " 'screw': ['love'],\n",
       " 'fuck': ['love'],\n",
       " 'puke': ['cat'],\n",
       " 'qat': ['cat'],\n",
       " 'love': ['have_intercourse',\n",
       "  'dear',\n",
       "  'sleep_with',\n",
       "  'screw',\n",
       "  'do_it',\n",
       "  'love_life',\n",
       "  'enjoy',\n",
       "  'fuck',\n",
       "  'dearest',\n",
       "  'beloved',\n",
       "  'erotic_love',\n",
       "  'be_intimate',\n",
       "  'bed',\n",
       "  'eff',\n",
       "  'have_it_away',\n",
       "  'get_it_on',\n",
       "  'have_it_off',\n",
       "  'have_a_go_at_it',\n",
       "  'bonk',\n",
       "  'roll_in_the_hay',\n",
       "  'make_love',\n",
       "  'jazz',\n",
       "  'know',\n",
       "  'passion',\n",
       "  'make_out',\n",
       "  'lie_with',\n",
       "  'sleep_together',\n",
       "  'making_love',\n",
       "  'get_laid',\n",
       "  'bang',\n",
       "  'have_sex',\n",
       "  'honey',\n",
       "  'hump',\n",
       "  'sexual_love',\n",
       "  'lovemaking'],\n",
       " 'woodworking_plane': ['plane'],\n",
       " 'railcar': ['car'],\n",
       " 'be_sick': ['cat'],\n",
       " 'motorcar': ['car'],\n",
       " 'have_it_away': ['love'],\n",
       " 'give_suck': ['nurse'],\n",
       " 'vomit_up': ['cat'],\n",
       " 'have_it_off': ['love'],\n",
       " 'planer': ['plane'],\n",
       " 'roll_in_the_hay': ['love'],\n",
       " 'calculator': ['computer'],\n",
       " 'hold': ['nurse'],\n",
       " 'plane': ['aeroplane',\n",
       "  'level',\n",
       "  'woodworking_plane',\n",
       "  'shave',\n",
       "  'flat',\n",
       "  'sheet',\n",
       "  'airplane',\n",
       "  'skim',\n",
       "  \"carpenter's_plane\",\n",
       "  'planing_machine',\n",
       "  'planer'],\n",
       " 'upchuck': ['cat'],\n",
       " 'computed_tomography': ['cat'],\n",
       " 'computerized_tomography': ['cat'],\n",
       " 'computerized_axial_tomography': ['cat'],\n",
       " 'breastfeed': ['nurse'],\n",
       " 'doctor': ['furbish_up',\n",
       "  'bushel',\n",
       "  'doctor_up',\n",
       "  'medico',\n",
       "  'Doctor_of_the_Church',\n",
       "  'Dr.',\n",
       "  'repair',\n",
       "  'restore',\n",
       "  'physician',\n",
       "  'touch_on',\n",
       "  'doc',\n",
       "  'mend',\n",
       "  'sophisticate',\n",
       "  'Doctor',\n",
       "  'MD',\n",
       "  'fix'],\n",
       " 'figurer': ['computer'],\n",
       " 'quat': ['cat'],\n",
       " 'make_out': ['love'],\n",
       " 'hombre': ['cat'],\n",
       " 'making_love': ['love'],\n",
       " 'Doctor_of_the_Church': ['doctor'],\n",
       " 'shave': ['plane'],\n",
       " 'get_laid': ['love'],\n",
       " 'computing_device': ['computer'],\n",
       " 'airplane': ['plane'],\n",
       " 'keyboard': [],\n",
       " 'spue': ['cat'],\n",
       " 'sophisticate': ['doctor'],\n",
       " \"carpenter's_plane\": ['plane'],\n",
       " 'honey': ['love'],\n",
       " 'beloved': ['love']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_graph_from_synonyms(synonyms_dict):\n",
    "    graph = {}\n",
    "    \n",
    "    # Create a set of all unique words in the dictionary\n",
    "    words = set(synonyms_dict.keys()).union(*synonyms_dict.values())\n",
    "    \n",
    "    # Initialize an empty adjacency dictionary for each word\n",
    "    for word in words:\n",
    "        graph[word] = set()\n",
    "    \n",
    "    # Iterate through the synonyms dictionary\n",
    "    for word, synonyms in synonyms_dict.items():\n",
    "        # Add synonyms to the adjacency set for the word\n",
    "        graph[word].update(synonyms)\n",
    "        \n",
    "        # Add the word as a synonym to each synonym's adjacency set\n",
    "        for synonym in synonyms:\n",
    "            graph[synonym].add(word)\n",
    "    \n",
    "    # Convert the adjacency sets to lists\n",
    "    graph = {word: list(adjacency_set) for word, adjacency_set in graph.items()}\n",
    "    \n",
    "    return graph\n",
    "\n",
    "graph = generate_graph_from_synonyms(lexicon)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs_article(Q, Q_hat, graph, alpha, beta, num_iterations=10):\n",
    "    num_words = Q.shape[0]\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        Q_new = np.zeros_like(Q)\n",
    "        for i in range(num_words):\n",
    "            neighbors = graph[i]\n",
    "            numerator = np.sum(beta[i, j] * Q[j] for j in neighbors) + alpha[i] * Q_hat[i]\n",
    "            denominator = np.sum(beta[i, j] for j in neighbors) + alpha[i]\n",
    "            Q_new[i] = numerator / denominator\n",
    "        Q = Q_new\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrofitted_toy_vecs = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newVecs = retrofitting_wordVecs_article(wordVecMat, neighbors_matrix, graph, alpha=1, beta=1, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 6\n",
      "Number of edges: 5\n",
      "Neighbors of cat: ['kitten', 'animal', 'pet']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_to_keep = [\"cat\", \"kitten\", \"dog\", \"puppy\", \"animal\", \"pet\"]\n",
    "# Construct an empty graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(words_to_keep)\n",
    "\n",
    "# Add edges between related words\n",
    "graph.add_edges_from([('cat', 'kitten'), ('dog', 'puppy'), ('cat', 'animal'), ('dog', 'animal'), ('cat', 'pet')])\n",
    "\n",
    "# Print the graph information\n",
    "print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "print(\"Number of edges:\", graph.number_of_edges())\n",
    "\n",
    "# Access neighbors of a word\n",
    "word = 'cat'\n",
    "neighbors = graph.neighbors(word)\n",
    "print(\"Neighbors of\", word + \":\", list(neighbors))\n",
    "\n",
    "# Create the layout for the graph\n",
    "layout = nx.spring_layout(graph)\n",
    "\n",
    "# Draw the nodes\n",
    "nx.draw_networkx_nodes(graph, pos=layout, node_color='lightblue', node_size=500)\n",
    "\n",
    "# Draw the edges\n",
    "nx.draw_networkx_edges(graph, pos=layout, edge_color='gray')\n",
    "\n",
    "# Add labels to the nodes\n",
    "nx.draw_networkx_labels(graph, pos=layout, font_color='black')\n",
    "\n",
    "# Set plot properties\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5173\n",
      "  - \"computer\": 0.1732\n",
      "  - \"keyboard\": 0.1834\n",
      "  - \"plane\": 0.1833\n",
      "  - \"car\": 0.2153\n",
      "  - \"doctor\": 0.1292\n",
      "  - \"nurse\": 0.1594\n",
      "  - \"love\": 0.1406\n",
      "  - \"sex\": 0.1368\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0677\n",
      "  - \"keyboard\": 0.0654\n",
      "  - \"plane\": 0.1660\n",
      "  - \"car\": 0.1672\n",
      "  - \"doctor\": 0.0835\n",
      "  - \"nurse\": 0.1111\n",
      "  - \"love\": 0.0871\n",
      "  - \"sex\": 0.2222\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732\n",
      "  - \"tiger\": 0.0677\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.3964\n",
      "  - \"plane\": 0.1909\n",
      "  - \"car\": 0.2461\n",
      "  - \"doctor\": 0.1628\n",
      "  - \"nurse\": 0.2178\n",
      "  - \"love\": 0.0573\n",
      "  - \"sex\": 0.1853\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834\n",
      "  - \"tiger\": 0.0654\n",
      "  - \"computer\": 0.3964\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1006\n",
      "  - \"car\": 0.1498\n",
      "  - \"doctor\": 0.0850\n",
      "  - \"nurse\": 0.1220\n",
      "  - \"love\": 0.1591\n",
      "  - \"sex\": 0.0943\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833\n",
      "  - \"tiger\": 0.1660\n",
      "  - \"computer\": 0.1909\n",
      "  - \"keyboard\": 0.1006\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.3780\n",
      "  - \"doctor\": 0.1879\n",
      "  - \"nurse\": 0.0978\n",
      "  - \"love\": 0.1080\n",
      "  - \"sex\": 0.0587\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153\n",
      "  - \"tiger\": 0.1672\n",
      "  - \"computer\": 0.2461\n",
      "  - \"keyboard\": 0.1498\n",
      "  - \"plane\": 0.3780\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1895\n",
      "  - \"nurse\": 0.1306\n",
      "  - \"love\": 0.0842\n",
      "  - \"sex\": 0.1169\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292\n",
      "  - \"tiger\": 0.0835\n",
      "  - \"computer\": 0.1628\n",
      "  - \"keyboard\": 0.0850\n",
      "  - \"plane\": 0.1879\n",
      "  - \"car\": 0.1895\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6320\n",
      "  - \"love\": 0.0831\n",
      "  - \"sex\": 0.1994\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594\n",
      "  - \"tiger\": 0.1111\n",
      "  - \"computer\": 0.2178\n",
      "  - \"keyboard\": 0.1220\n",
      "  - \"plane\": 0.0978\n",
      "  - \"car\": 0.1306\n",
      "  - \"doctor\": 0.6320\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.0631\n",
      "  - \"sex\": 0.1997\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406\n",
      "  - \"tiger\": 0.0871\n",
      "  - \"computer\": 0.0573\n",
      "  - \"keyboard\": 0.1591\n",
      "  - \"plane\": 0.1080\n",
      "  - \"car\": 0.0842\n",
      "  - \"doctor\": 0.0831\n",
      "  - \"nurse\": 0.0631\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.2639\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368\n",
      "  - \"tiger\": 0.2222\n",
      "  - \"computer\": 0.1853\n",
      "  - \"keyboard\": 0.0943\n",
      "  - \"plane\": 0.0587\n",
      "  - \"car\": 0.1169\n",
      "  - \"doctor\": 0.1994\n",
      "  - \"nurse\": 0.1997\n",
      "  - \"love\": 0.2639\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrofitted_toy_matrix = convert_dict_to_matrix(retrofitted_toy_vecs)\n",
    "retrofitted_similarity_matrix = generate_cosine_similarity_matrix(toy_wordVecs)\n",
    "print_vec_similarities(toy_corpus, retrofitted_similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172961950302124"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrofitted_similarity_matrix[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5173 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1732 -> 0.1732 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1834 -> 0.1834 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1833 -> 0.1833 (Difference: 0.0000)\n",
      "  - \"car\": 0.2153 -> 0.2153 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1292 -> 0.1292 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1594 -> 0.1594 (Difference: 0.0000)\n",
      "  - \"love\": 0.1406 -> 0.1406 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1368 -> 0.1368 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5173 (Difference: 0.0000)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0677 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0654 -> 0.0654 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1660 -> 0.1660 (Difference: 0.0000)\n",
      "  - \"car\": 0.1672 -> 0.1672 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0835 -> 0.0835 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1111 -> 0.1111 (Difference: 0.0000)\n",
      "  - \"love\": 0.0871 -> 0.0871 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2222 -> 0.2222 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.1732 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0677 -> 0.0677 (Difference: 0.0000)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.3964 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1909 -> 0.1909 (Difference: 0.0000)\n",
      "  - \"car\": 0.2461 -> 0.2461 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1628 -> 0.1628 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.2178 -> 0.2178 (Difference: 0.0000)\n",
      "  - \"love\": 0.0573 -> 0.0573 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1853 -> 0.1853 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.1834 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0654 -> 0.0654 (Difference: 0.0000)\n",
      "  - \"computer\": 0.3964 -> 0.3964 (Difference: 0.0000)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1006 (Difference: 0.0000)\n",
      "  - \"car\": 0.1498 -> 0.1498 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0850 -> 0.0850 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1220 -> 0.1220 (Difference: 0.0000)\n",
      "  - \"love\": 0.1591 -> 0.1591 (Difference: 0.0000)\n",
      "  - \"sex\": 0.0943 -> 0.0943 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.1833 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1660 -> 0.1660 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1909 -> 0.1909 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1006 -> 0.1006 (Difference: 0.0000)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"car\": 0.3780 -> 0.3780 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1879 -> 0.1879 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.0978 -> 0.0978 (Difference: 0.0000)\n",
      "  - \"love\": 0.1080 -> 0.1080 (Difference: 0.0000)\n",
      "  - \"sex\": 0.0587 -> 0.0587 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2153 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1672 -> 0.1672 (Difference: 0.0000)\n",
      "  - \"computer\": 0.2461 -> 0.2461 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1498 -> 0.1498 (Difference: 0.0000)\n",
      "  - \"plane\": 0.3780 -> 0.3780 (Difference: 0.0000)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1895 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1306 -> 0.1306 (Difference: 0.0000)\n",
      "  - \"love\": 0.0842 -> 0.0842 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1169 -> 0.1169 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.1292 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0835 -> 0.0835 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1628 -> 0.1628 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0850 -> 0.0850 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1879 -> 0.1879 (Difference: 0.0000)\n",
      "  - \"car\": 0.1895 -> 0.1895 (Difference: 0.0000)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6320 (Difference: 0.0000)\n",
      "  - \"love\": 0.0831 -> 0.0831 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1994 -> 0.1994 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.1594 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1111 -> 0.1111 (Difference: 0.0000)\n",
      "  - \"computer\": 0.2178 -> 0.2178 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1220 -> 0.1220 (Difference: 0.0000)\n",
      "  - \"plane\": 0.0978 -> 0.0978 (Difference: 0.0000)\n",
      "  - \"car\": 0.1306 -> 0.1306 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.6320 -> 0.6320 (Difference: 0.0000)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.0631 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1997 -> 0.1997 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.1406 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0871 -> 0.0871 (Difference: 0.0000)\n",
      "  - \"computer\": 0.0573 -> 0.0573 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1591 -> 0.1591 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1080 -> 0.1080 (Difference: 0.0000)\n",
      "  - \"car\": 0.0842 -> 0.0842 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0831 -> 0.0831 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.0631 -> 0.0631 (Difference: 0.0000)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.2639 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.1368 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.2222 -> 0.2222 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1853 -> 0.1853 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0943 -> 0.0943 (Difference: 0.0000)\n",
      "  - \"plane\": 0.0587 -> 0.0587 (Difference: 0.0000)\n",
      "  - \"car\": 0.1169 -> 0.1169 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1994 -> 0.1994 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1997 -> 0.1997 (Difference: 0.0000)\n",
      "  - \"love\": 0.2639 -> 0.2639 (Difference: 0.0000)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "def print_vec_difference(wordList, similarity_matrix1, similarity_matrix2):\n",
    "    for i, word in enumerate(wordList):\n",
    "        print(f\"\\nSimilarities with \\\"{word}\\\":\")\n",
    "        for j, neighbor in enumerate(wordList):\n",
    "            similarity1 = similarity_matrix1[i, j]\n",
    "            similarity2 = similarity_matrix2[i, j]\n",
    "            difference = similarity2 - similarity1  # Calculate the difference\n",
    "            print(f\"  - \\\"{neighbor}\\\": {similarity1:.4f} -> {similarity2:.4f} (Difference: {difference:.4f})\")\n",
    "\n",
    "print_vec_difference(toy_corpus, similarity_matrix, retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Difference Matrix:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print_similarity_difference(similarity_matrix, retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between wordVecMat and retrofitted_toy_vec\n",
    "# similarity_score = cosine_similarity_matrix(wordVecMat, retrofitted_toy_vecs)\n",
    "# print(\"Cosine Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average embedding update: 0.12658860989283166\n"
     ]
    }
   ],
   "source": [
    "def measure_embedding_updates(original_matrix, retrofitted_matrix):\n",
    "    absolute_diff = np.abs(original_matrix - retrofitted_matrix)\n",
    "    mean_absolute_diff = np.mean(absolute_diff)\n",
    "    return mean_absolute_diff\n",
    "\n",
    "# Example usage\n",
    "update_measure = measure_embedding_updates(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Average embedding update:\", update_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation score: 0.42055888661904023\n",
      "Pearson correlation score: 0.4232245375810048\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def spearman_measure_embedding_similarity(original_matrix, retrofitted_matrix):\n",
    "    original_flat = original_matrix.flatten()\n",
    "    retrofitted_flat = retrofitted_matrix.flatten()\n",
    "    correlation, _ = spearmanr(original_flat, retrofitted_flat)\n",
    "    return correlation\n",
    "\n",
    "def pearson_measure_embedding_similarity(original_matrix, retrofitted_matrix):\n",
    "    original_flat = original_matrix.flatten()\n",
    "    retrofitted_flat = retrofitted_matrix.flatten()\n",
    "    correlation, _ = pearsonr(original_flat, retrofitted_flat)\n",
    "    return correlation\n",
    "\n",
    "similarity_score = spearman_measure_embedding_similarity(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Spearman correlation score:\", similarity_score)\n",
    "similarity_score = pearson_measure_embedding_similarity(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Pearson correlation score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,300) (100,250) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m beta \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0.1\u001b[39m, \u001b[39m5.1\u001b[39m, \u001b[39m0.2\u001b[39m):\n\u001b[0;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m nb_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m         retrofitted_toy_vec, _ \u001b[39m=\u001b[39m retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n\u001b[0;32m     22\u001b[0m         cosine_sim \u001b[39m=\u001b[39m cosine_similarity(wordVecMat, retrofitted_toy_vec)\n\u001b[0;32m     24\u001b[0m         \u001b[39m# Calculate Spearman correlation against human evaluation scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mretrofitting_wordVecs\u001b[1;34m(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m updates \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_iter):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Calculate the number of neighbors for each word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[39m# Update the word embeddings using retrofitting formula\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     newWordVecMat \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39;49m newWordVecMat \u001b[39m+\u001b[39;49m beta \u001b[39m*\u001b[39;49m neighbors_mean_matrix) \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m beta)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Calculate the updates\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     update \u001b[39m=\u001b[39m newWordVecMat \u001b[39m-\u001b[39m wordVecMat\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,300) (100,250) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load human evaluation scores\n",
    "eval_file_path = r\"C:\\Users\\ninan\\OneDrive\\Bureau\\Université Paris Cité\\S2\\NLP project\\Improving-vector-space-representations-using-semantic-resources\\data\\English\\lexicon\\ws353_lexical_similarity.txt\"\n",
    "eval_scores = {}\n",
    "with open(eval_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        word1, word2, score = line.strip().split('\\t')\n",
    "        eval_scores[(word1, word2)] = float(score)\n",
    "\n",
    "# Find best values for hyperparameters\n",
    "best_similarity_score = -1  # Variable to store the best similarity score\n",
    "best_params = {}  # Dictionary to store the best hyperparameter values\n",
    "iteration_count = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1, 16):\n",
    "            retrofitted_toy_vec = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "            cosine_sim = cosine_similarity(wordVecMat, retrofitted_toy_vec)\n",
    "\n",
    "            # Calculate Spearman correlation against human evaluation scores\n",
    "            eval_scores_list = []\n",
    "            cosine_sim_list = []\n",
    "            for (word1, word2), score in eval_scores.items():\n",
    "                if word1 in wordList and word2 in wordList:\n",
    "                    word1_index = wordList.index(word1)\n",
    "                    word2_index = wordList.index(word2)\n",
    "                    eval_scores_list.append(score)\n",
    "                    cosine_sim_list.append(cosine_sim[word1_index, word2_index])\n",
    "\n",
    "            # Check if there are valid pairs for comparison\n",
    "            if len(eval_scores_list) > 0 and len(cosine_sim_list) > 0:\n",
    "                correlation, _ = spearmanr(eval_scores_list, cosine_sim_list)\n",
    "                # print(\"alpha =\", alpha, \"beta =\", beta, \"nb_iter =\", nb_iter, \"correlation =\", correlation)\n",
    "\n",
    "                # Update best similarity score and parameters if improved\n",
    "                if correlation > best_similarity_score:\n",
    "                    best_similarity_score = correlation\n",
    "                    best_params = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "            iteration_count += 1\n",
    "            if iteration_count >= 100:\n",
    "                break\n",
    "        if iteration_count >= 100:\n",
    "            break\n",
    "    if iteration_count >= 100:\n",
    "        break\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best Spearman correlation score:\", best_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.1, 'beta': 1.1000000000000003, 'nb_iter': 15}\n",
      "Best embedding update: 0.12671235242449622\n"
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update = -1  # Variable to store the best similarity score\n",
    "best_params = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_toy_vec = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "            embed_update = measure_embedding_updates(wordVecMat, retrofitted_toy_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update > best_embed_update:\n",
    "                best_embed_update = embed_update\n",
    "                best_params = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best embedding update:\", best_embed_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5134\n",
      "  - \"computer\": 0.2147\n",
      "  - \"keyboard\": 0.2270\n",
      "  - \"plane\": 0.2878\n",
      "  - \"car\": 0.2492\n",
      "  - \"doctor\": 0.2437\n",
      "  - \"nurse\": 0.3233\n",
      "  - \"love\": 0.3254\n",
      "  - \"sex\": 0.2202\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5134\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0783\n",
      "  - \"keyboard\": 0.1118\n",
      "  - \"plane\": 0.1769\n",
      "  - \"car\": 0.1518\n",
      "  - \"doctor\": 0.0866\n",
      "  - \"nurse\": 0.1714\n",
      "  - \"love\": 0.1518\n",
      "  - \"sex\": 0.2417\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.2147\n",
      "  - \"tiger\": 0.0783\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.4058\n",
      "  - \"plane\": 0.2883\n",
      "  - \"car\": 0.3039\n",
      "  - \"doctor\": 0.2093\n",
      "  - \"nurse\": 0.2180\n",
      "  - \"love\": 0.1358\n",
      "  - \"sex\": 0.1602\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.2270\n",
      "  - \"tiger\": 0.1118\n",
      "  - \"computer\": 0.4058\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1872\n",
      "  - \"car\": 0.1700\n",
      "  - \"doctor\": 0.1131\n",
      "  - \"nurse\": 0.1637\n",
      "  - \"love\": 0.2198\n",
      "  - \"sex\": 0.1140\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.2878\n",
      "  - \"tiger\": 0.1769\n",
      "  - \"computer\": 0.2883\n",
      "  - \"keyboard\": 0.1872\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.4366\n",
      "  - \"doctor\": 0.2202\n",
      "  - \"nurse\": 0.1756\n",
      "  - \"love\": 0.1935\n",
      "  - \"sex\": 0.0994\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2492\n",
      "  - \"tiger\": 0.1518\n",
      "  - \"computer\": 0.3039\n",
      "  - \"keyboard\": 0.1700\n",
      "  - \"plane\": 0.4366\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1865\n",
      "  - \"nurse\": 0.1453\n",
      "  - \"love\": 0.1766\n",
      "  - \"sex\": 0.1273\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.2437\n",
      "  - \"tiger\": 0.0866\n",
      "  - \"computer\": 0.2093\n",
      "  - \"keyboard\": 0.1131\n",
      "  - \"plane\": 0.2202\n",
      "  - \"car\": 0.1865\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6105\n",
      "  - \"love\": 0.1844\n",
      "  - \"sex\": 0.1923\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.3233\n",
      "  - \"tiger\": 0.1714\n",
      "  - \"computer\": 0.2180\n",
      "  - \"keyboard\": 0.1637\n",
      "  - \"plane\": 0.1756\n",
      "  - \"car\": 0.1453\n",
      "  - \"doctor\": 0.6105\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.2675\n",
      "  - \"sex\": 0.3084\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.3254\n",
      "  - \"tiger\": 0.1518\n",
      "  - \"computer\": 0.1358\n",
      "  - \"keyboard\": 0.2198\n",
      "  - \"plane\": 0.1935\n",
      "  - \"car\": 0.1766\n",
      "  - \"doctor\": 0.1844\n",
      "  - \"nurse\": 0.2675\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.3760\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.2202\n",
      "  - \"tiger\": 0.2417\n",
      "  - \"computer\": 0.1602\n",
      "  - \"keyboard\": 0.1140\n",
      "  - \"plane\": 0.0994\n",
      "  - \"car\": 0.1273\n",
      "  - \"doctor\": 0.1923\n",
      "  - \"nurse\": 0.3084\n",
      "  - \"love\": 0.3760\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_retrofitted_toy_matrix = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha=0.1, beta=0.1, nb_iter=1)\n",
    "new_retrofitted_toy_dict = convert_matrix_to_dict(new_retrofitted_toy_matrix, wordList)\n",
    "new_retrofitted_similarity_matrix = generate_cosine_similarity_matrix(new_retrofitted_toy_dict)\n",
    "print_vec_similarities(toy_corpus, new_retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"computer\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"keyboard\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"plane\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"car\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"doctor\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"nurse\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"love\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"sex\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"keyboard\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"plane\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"car\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"doctor\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"nurse\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"love\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"sex\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"tiger\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"plane\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"car\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"doctor\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"nurse\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"love\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"sex\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"tiger\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"computer\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"car\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"doctor\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"nurse\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"love\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"sex\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"tiger\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"computer\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"keyboard\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"car\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"doctor\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"nurse\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"love\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"sex\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"tiger\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"computer\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"keyboard\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"plane\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"nurse\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"love\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"sex\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"tiger\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"computer\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"keyboard\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"plane\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"car\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"love\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"sex\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"tiger\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"computer\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"keyboard\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"plane\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"car\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"doctor\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"sex\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"tiger\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"computer\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"keyboard\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"plane\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"car\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"doctor\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"nurse\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "  - \"tiger\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "  - \"computer\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "  - \"keyboard\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "  - \"plane\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "  - \"car\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "  - \"doctor\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "  - \"nurse\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "  - \"love\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and after tuning hyperparam\n",
    "print_vec_difference(toy_corpus, similarity_matrix, new_retrofitted_similarity_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"computer\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"keyboard\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"plane\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"car\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"doctor\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"nurse\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"love\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"sex\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"keyboard\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"plane\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"car\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"doctor\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"nurse\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"love\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"sex\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"tiger\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"plane\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"car\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"doctor\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"nurse\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"love\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"sex\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"tiger\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"computer\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"car\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"doctor\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"nurse\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"love\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"sex\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"tiger\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"computer\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"keyboard\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"car\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"doctor\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"nurse\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"love\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"sex\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"tiger\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"computer\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"keyboard\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"plane\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"nurse\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"love\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"sex\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"tiger\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"computer\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"keyboard\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"plane\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"car\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"love\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"sex\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"tiger\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"computer\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"keyboard\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"plane\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"car\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"doctor\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"sex\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"tiger\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"computer\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"keyboard\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"plane\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"car\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"doctor\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"nurse\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "  - \"tiger\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "  - \"computer\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "  - \"keyboard\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "  - \"plane\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "  - \"car\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "  - \"doctor\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "  - \"nurse\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "  - \"love\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Difference between retrofitted embeddings and after tuning hyperaparams\n",
    "print_vec_difference(toy_corpus, retrofitted_similarity_matrix, new_retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>tiger</th>\n",
       "      <th>computer</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>plane</th>\n",
       "      <th>car</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "      <th>love</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>Before: 0.29245239862991185, After: 1.00000000...</td>\n",
       "      <td>Before: 0.35037322527996934, After: 0.20200897...</td>\n",
       "      <td>Before: 0.06136659928459301, After: 0.19719178...</td>\n",
       "      <td>Before: 0.18344955625364173, After: 0.20547455...</td>\n",
       "      <td>Before: 0.21134097025538556, After: 0.39445950...</td>\n",
       "      <td>Before: 0.136072027917602, After: 0.2634861035...</td>\n",
       "      <td>Before: 0.16547475026405636, After: 0.34338708...</td>\n",
       "      <td>Before: 0.24690899138830727, After: 0.54854860...</td>\n",
       "      <td>Before: 0.2989067535773432, After: 0.594053102...</td>\n",
       "      <td>Before: 0.1093022564011111, After: 0.259890845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>Before: 0.17347637872059773, After: 0.20200897...</td>\n",
       "      <td>Before: 0.48802072485209846, After: 1.00000000...</td>\n",
       "      <td>Before: 0.02942476361382193, After: 0.09025702...</td>\n",
       "      <td>Before: 0.06542581824273716, After: 0.18313975...</td>\n",
       "      <td>Before: 0.15090617196611955, After: 0.07307479...</td>\n",
       "      <td>Before: 0.16119598769364896, After: 0.10033040...</td>\n",
       "      <td>Before: 0.0774108002811773, After: 0.089083542...</td>\n",
       "      <td>Before: 0.1697249630498177, After: 0.202597136...</td>\n",
       "      <td>Before: 0.17516455047450735, After: 0.19102605...</td>\n",
       "      <td>Before: 0.14518818082220147, After: 0.18807845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>Before: 0.18984798734723213, After: 0.19719178...</td>\n",
       "      <td>Before: 0.05360514929979338, After: 0.09025702...</td>\n",
       "      <td>Before: 0.3047689400402037, After: 1.0</td>\n",
       "      <td>Before: 0.39639163439495995, After: 0.23934215...</td>\n",
       "      <td>Before: 0.28314903703275535, After: 0.40858220...</td>\n",
       "      <td>Before: 0.26826894486108244, After: 0.22732308...</td>\n",
       "      <td>Before: 0.18039329196329013, After: 0.20986951...</td>\n",
       "      <td>Before: 0.09297679298707379, After: 0.12068024...</td>\n",
       "      <td>Before: 0.1168711356729625, After: 0.234808759...</td>\n",
       "      <td>Before: 0.1158779845883439, After: 0.076953257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>Before: 0.20546589125510897, After: 0.20547455...</td>\n",
       "      <td>Before: 0.18314156317766062, After: 0.18313975...</td>\n",
       "      <td>Before: 0.2393287663091698, After: 0.239342157...</td>\n",
       "      <td>Before: 0.9999999999999996, After: 1.0</td>\n",
       "      <td>Before: 0.28971014677665063, After: 0.28970944...</td>\n",
       "      <td>Before: 0.15750632650840493, After: 0.15750846...</td>\n",
       "      <td>Before: 0.13804955762057758, After: 0.13804958...</td>\n",
       "      <td>Before: 0.16678031434047627, After: 0.16678347...</td>\n",
       "      <td>Before: 0.27407501845767246, After: 0.27407454...</td>\n",
       "      <td>Before: 0.10639718565916091, After: 0.10639914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>Before: 0.16761576142114898, After: 0.39445950...</td>\n",
       "      <td>Before: 0.06730278214710199, After: 0.07307479...</td>\n",
       "      <td>Before: 0.06606645196552532, After: 0.40858220...</td>\n",
       "      <td>Before: 0.10055138151211143, After: 0.28970944...</td>\n",
       "      <td>Before: 0.38405259310022044, After: 1.00000000...</td>\n",
       "      <td>Before: 0.3219357387657039, After: 0.354037032...</td>\n",
       "      <td>Before: 0.15146728859985834, After: 0.21833826...</td>\n",
       "      <td>Before: 0.10589313234551631, After: 0.28236372...</td>\n",
       "      <td>Before: 0.1936940498815496, After: 0.328525840...</td>\n",
       "      <td>Before: 0.05697047025775614, After: 0.11862340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>Before: 0.15859645192729996, After: 0.26348610...</td>\n",
       "      <td>Before: -0.049076379792710186, After: 0.100330...</td>\n",
       "      <td>Before: 0.13968323112249992, After: 0.22732308...</td>\n",
       "      <td>Before: 0.14983822223318854, After: 0.15750846...</td>\n",
       "      <td>Before: 0.2597415410728325, After: 0.354037032...</td>\n",
       "      <td>Before: 0.6158574248530795, After: 1.000000000...</td>\n",
       "      <td>Before: 0.13778205919388814, After: 0.19190468...</td>\n",
       "      <td>Before: 0.0865221800713605, After: 0.160778501...</td>\n",
       "      <td>Before: 0.22133439390681858, After: 0.24947266...</td>\n",
       "      <td>Before: 0.059270287407368415, After: 0.0662935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>Before: 0.25878907406446877, After: 0.34338708...</td>\n",
       "      <td>Before: 0.017639064485218965, After: 0.0890835...</td>\n",
       "      <td>Before: 0.0978054983995635, After: 0.209869514...</td>\n",
       "      <td>Before: 0.08500327165730943, After: 0.13804958...</td>\n",
       "      <td>Before: 0.13439994429915442, After: 0.21833826...</td>\n",
       "      <td>Before: 0.09431570687955, After: 0.19190468704...</td>\n",
       "      <td>Before: 0.6128107065755533, After: 1.0</td>\n",
       "      <td>Before: 0.27845097194129365, After: 0.34448646...</td>\n",
       "      <td>Before: 0.19325643013428553, After: 0.33078374...</td>\n",
       "      <td>Before: 0.0745189116843016, After: 0.175547118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>Before: 0.18217682480604822, After: 0.54854860...</td>\n",
       "      <td>Before: 0.07222178145580241, After: 0.20259713...</td>\n",
       "      <td>Before: 0.11697573887301807, After: 0.12068024...</td>\n",
       "      <td>Before: 0.12199094008346709, After: 0.16678347...</td>\n",
       "      <td>Before: 0.12790409339777273, After: 0.28236372...</td>\n",
       "      <td>Before: 0.07363428182232754, After: 0.16077850...</td>\n",
       "      <td>Before: 0.4308149649767604, After: 0.344486463...</td>\n",
       "      <td>Before: 0.378466332225932, After: 1.0000000000...</td>\n",
       "      <td>Before: 0.1708752362535986, After: 0.544714533...</td>\n",
       "      <td>Before: 0.1264203000118017, After: 0.409129275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>Before: 0.241072229246228, After: 0.5940531024...</td>\n",
       "      <td>Before: 0.10199943514005788, After: 0.19102605...</td>\n",
       "      <td>Before: 0.0730022470894344, After: 0.234808759...</td>\n",
       "      <td>Before: 0.15911448638969528, After: 0.27407454...</td>\n",
       "      <td>Before: 0.09505575751009387, After: 0.32852584...</td>\n",
       "      <td>Before: 0.11200247669649024, After: 0.24947266...</td>\n",
       "      <td>Before: 0.14771102908578468, After: 0.33078374...</td>\n",
       "      <td>Before: 0.3033847603340788, After: 0.544714533...</td>\n",
       "      <td>Before: 0.6110191309495465, After: 0.999999999...</td>\n",
       "      <td>Before: 0.30399420253175347, After: 0.36164712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>Before: 0.225917972105598, After: 0.2598908452...</td>\n",
       "      <td>Before: 0.1655264826766013, After: 0.188078452...</td>\n",
       "      <td>Before: 0.036750539503579295, After: 0.0769532...</td>\n",
       "      <td>Before: 0.09429740737651135, After: 0.10639914...</td>\n",
       "      <td>Before: 0.10212970436490482, After: 0.11862340...</td>\n",
       "      <td>Before: 0.13403080874265905, After: 0.06629354...</td>\n",
       "      <td>Before: 0.1436405909297276, After: 0.175547118...</td>\n",
       "      <td>Before: 0.26868181343357167, After: 0.40912927...</td>\n",
       "      <td>Before: 0.3049306009871644, After: 0.361647126...</td>\n",
       "      <td>Before: 0.4896246955281065, After: 1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        cat  \\\n",
       "cat       Before: 0.29245239862991185, After: 1.00000000...   \n",
       "tiger     Before: 0.17347637872059773, After: 0.20200897...   \n",
       "computer  Before: 0.18984798734723213, After: 0.19719178...   \n",
       "keyboard  Before: 0.20546589125510897, After: 0.20547455...   \n",
       "plane     Before: 0.16761576142114898, After: 0.39445950...   \n",
       "car       Before: 0.15859645192729996, After: 0.26348610...   \n",
       "doctor    Before: 0.25878907406446877, After: 0.34338708...   \n",
       "nurse     Before: 0.18217682480604822, After: 0.54854860...   \n",
       "love      Before: 0.241072229246228, After: 0.5940531024...   \n",
       "sex       Before: 0.225917972105598, After: 0.2598908452...   \n",
       "\n",
       "                                                      tiger  \\\n",
       "cat       Before: 0.35037322527996934, After: 0.20200897...   \n",
       "tiger     Before: 0.48802072485209846, After: 1.00000000...   \n",
       "computer  Before: 0.05360514929979338, After: 0.09025702...   \n",
       "keyboard  Before: 0.18314156317766062, After: 0.18313975...   \n",
       "plane     Before: 0.06730278214710199, After: 0.07307479...   \n",
       "car       Before: -0.049076379792710186, After: 0.100330...   \n",
       "doctor    Before: 0.017639064485218965, After: 0.0890835...   \n",
       "nurse     Before: 0.07222178145580241, After: 0.20259713...   \n",
       "love      Before: 0.10199943514005788, After: 0.19102605...   \n",
       "sex       Before: 0.1655264826766013, After: 0.188078452...   \n",
       "\n",
       "                                                   computer  \\\n",
       "cat       Before: 0.06136659928459301, After: 0.19719178...   \n",
       "tiger     Before: 0.02942476361382193, After: 0.09025702...   \n",
       "computer             Before: 0.3047689400402037, After: 1.0   \n",
       "keyboard  Before: 0.2393287663091698, After: 0.239342157...   \n",
       "plane     Before: 0.06606645196552532, After: 0.40858220...   \n",
       "car       Before: 0.13968323112249992, After: 0.22732308...   \n",
       "doctor    Before: 0.0978054983995635, After: 0.209869514...   \n",
       "nurse     Before: 0.11697573887301807, After: 0.12068024...   \n",
       "love      Before: 0.0730022470894344, After: 0.234808759...   \n",
       "sex       Before: 0.036750539503579295, After: 0.0769532...   \n",
       "\n",
       "                                                   keyboard  \\\n",
       "cat       Before: 0.18344955625364173, After: 0.20547455...   \n",
       "tiger     Before: 0.06542581824273716, After: 0.18313975...   \n",
       "computer  Before: 0.39639163439495995, After: 0.23934215...   \n",
       "keyboard             Before: 0.9999999999999996, After: 1.0   \n",
       "plane     Before: 0.10055138151211143, After: 0.28970944...   \n",
       "car       Before: 0.14983822223318854, After: 0.15750846...   \n",
       "doctor    Before: 0.08500327165730943, After: 0.13804958...   \n",
       "nurse     Before: 0.12199094008346709, After: 0.16678347...   \n",
       "love      Before: 0.15911448638969528, After: 0.27407454...   \n",
       "sex       Before: 0.09429740737651135, After: 0.10639914...   \n",
       "\n",
       "                                                      plane  \\\n",
       "cat       Before: 0.21134097025538556, After: 0.39445950...   \n",
       "tiger     Before: 0.15090617196611955, After: 0.07307479...   \n",
       "computer  Before: 0.28314903703275535, After: 0.40858220...   \n",
       "keyboard  Before: 0.28971014677665063, After: 0.28970944...   \n",
       "plane     Before: 0.38405259310022044, After: 1.00000000...   \n",
       "car       Before: 0.2597415410728325, After: 0.354037032...   \n",
       "doctor    Before: 0.13439994429915442, After: 0.21833826...   \n",
       "nurse     Before: 0.12790409339777273, After: 0.28236372...   \n",
       "love      Before: 0.09505575751009387, After: 0.32852584...   \n",
       "sex       Before: 0.10212970436490482, After: 0.11862340...   \n",
       "\n",
       "                                                        car  \\\n",
       "cat       Before: 0.136072027917602, After: 0.2634861035...   \n",
       "tiger     Before: 0.16119598769364896, After: 0.10033040...   \n",
       "computer  Before: 0.26826894486108244, After: 0.22732308...   \n",
       "keyboard  Before: 0.15750632650840493, After: 0.15750846...   \n",
       "plane     Before: 0.3219357387657039, After: 0.354037032...   \n",
       "car       Before: 0.6158574248530795, After: 1.000000000...   \n",
       "doctor    Before: 0.09431570687955, After: 0.19190468704...   \n",
       "nurse     Before: 0.07363428182232754, After: 0.16077850...   \n",
       "love      Before: 0.11200247669649024, After: 0.24947266...   \n",
       "sex       Before: 0.13403080874265905, After: 0.06629354...   \n",
       "\n",
       "                                                     doctor  \\\n",
       "cat       Before: 0.16547475026405636, After: 0.34338708...   \n",
       "tiger     Before: 0.0774108002811773, After: 0.089083542...   \n",
       "computer  Before: 0.18039329196329013, After: 0.20986951...   \n",
       "keyboard  Before: 0.13804955762057758, After: 0.13804958...   \n",
       "plane     Before: 0.15146728859985834, After: 0.21833826...   \n",
       "car       Before: 0.13778205919388814, After: 0.19190468...   \n",
       "doctor               Before: 0.6128107065755533, After: 1.0   \n",
       "nurse     Before: 0.4308149649767604, After: 0.344486463...   \n",
       "love      Before: 0.14771102908578468, After: 0.33078374...   \n",
       "sex       Before: 0.1436405909297276, After: 0.175547118...   \n",
       "\n",
       "                                                      nurse  \\\n",
       "cat       Before: 0.24690899138830727, After: 0.54854860...   \n",
       "tiger     Before: 0.1697249630498177, After: 0.202597136...   \n",
       "computer  Before: 0.09297679298707379, After: 0.12068024...   \n",
       "keyboard  Before: 0.16678031434047627, After: 0.16678347...   \n",
       "plane     Before: 0.10589313234551631, After: 0.28236372...   \n",
       "car       Before: 0.0865221800713605, After: 0.160778501...   \n",
       "doctor    Before: 0.27845097194129365, After: 0.34448646...   \n",
       "nurse     Before: 0.378466332225932, After: 1.0000000000...   \n",
       "love      Before: 0.3033847603340788, After: 0.544714533...   \n",
       "sex       Before: 0.26868181343357167, After: 0.40912927...   \n",
       "\n",
       "                                                       love  \\\n",
       "cat       Before: 0.2989067535773432, After: 0.594053102...   \n",
       "tiger     Before: 0.17516455047450735, After: 0.19102605...   \n",
       "computer  Before: 0.1168711356729625, After: 0.234808759...   \n",
       "keyboard  Before: 0.27407501845767246, After: 0.27407454...   \n",
       "plane     Before: 0.1936940498815496, After: 0.328525840...   \n",
       "car       Before: 0.22133439390681858, After: 0.24947266...   \n",
       "doctor    Before: 0.19325643013428553, After: 0.33078374...   \n",
       "nurse     Before: 0.1708752362535986, After: 0.544714533...   \n",
       "love      Before: 0.6110191309495465, After: 0.999999999...   \n",
       "sex       Before: 0.3049306009871644, After: 0.361647126...   \n",
       "\n",
       "                                                        sex  \n",
       "cat       Before: 0.1093022564011111, After: 0.259890845...  \n",
       "tiger     Before: 0.14518818082220147, After: 0.18807845...  \n",
       "computer  Before: 0.1158779845883439, After: 0.076953257...  \n",
       "keyboard  Before: 0.10639718565916091, After: 0.10639914...  \n",
       "plane     Before: 0.05697047025775614, After: 0.11862340...  \n",
       "car       Before: 0.059270287407368415, After: 0.0662935...  \n",
       "doctor    Before: 0.0745189116843016, After: 0.175547118...  \n",
       "nurse     Before: 0.1264203000118017, After: 0.409129275...  \n",
       "love      Before: 0.30399420253175347, After: 0.36164712...  \n",
       "sex                  Before: 0.4896246955281065, After: 1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(index=wordList, columns=wordList)\n",
    "\n",
    "# Loop over each word pair and calculate similarity scores\n",
    "for word1 in wordList:\n",
    "    for word2 in wordList:\n",
    "        word1_index = wordList.index(word1)\n",
    "        word2_index = wordList.index(word2)\n",
    "        \n",
    "        # Calculate similarity score before retrofitting\n",
    "        similarity_before = cosine_sim[word1_index, word2_index]\n",
    "        \n",
    "        # Calculate similarity score after retrofitting\n",
    "        retrofit_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "        word1_vec = retrofit_toy_vec[word1_index].reshape(1, -1)\n",
    "        word2_vec = retrofit_toy_vec[word2_index].reshape(1, -1)\n",
    "        similarity_after = cosine_similarity(word1_vec, word2_vec)[0, 0]\n",
    "        \n",
    "        # Store the scores in the DataFrame\n",
    "        results_df.loc[word1, word2] = f\"Before: {similarity_before}, After: {similarity_after}\"\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>tiger</th>\n",
       "      <th>computer</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>plane</th>\n",
       "      <th>car</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "      <th>love</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>Retrofitting: 0.20, Human: 0.73</td>\n",
       "      <td>Retrofitting: 1.00, Human: 1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.24, Human: 0.76</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.35, Human: 0.58</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.34, Human: 0.70</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.36, Human: 0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cat                            tiger  \\\n",
       "cat                                  None                             None   \n",
       "tiger     Retrofitting: 0.20, Human: 0.73  Retrofitting: 1.00, Human: 1.00   \n",
       "computer                             None                             None   \n",
       "keyboard                             None                             None   \n",
       "plane                                None                             None   \n",
       "car                                  None                             None   \n",
       "doctor                               None                             None   \n",
       "nurse                                None                             None   \n",
       "love                                 None                             None   \n",
       "sex                                  None                             None   \n",
       "\n",
       "         computer                         keyboard plane  \\\n",
       "cat          None                             None  None   \n",
       "tiger        None                             None  None   \n",
       "computer     None  Retrofitting: 0.24, Human: 0.76  None   \n",
       "keyboard     None                             None  None   \n",
       "plane        None                             None  None   \n",
       "car          None                             None  None   \n",
       "doctor       None                             None  None   \n",
       "nurse        None                             None  None   \n",
       "love         None                             None  None   \n",
       "sex          None                             None  None   \n",
       "\n",
       "                                      car doctor  \\\n",
       "cat                                  None   None   \n",
       "tiger                                None   None   \n",
       "computer                             None   None   \n",
       "keyboard                             None   None   \n",
       "plane     Retrofitting: 0.35, Human: 0.58   None   \n",
       "car                                  None   None   \n",
       "doctor                               None   None   \n",
       "nurse                                None   None   \n",
       "love                                 None   None   \n",
       "sex                                  None   None   \n",
       "\n",
       "                                    nurse  love  \\\n",
       "cat                                  None  None   \n",
       "tiger                                None  None   \n",
       "computer                             None  None   \n",
       "keyboard                             None  None   \n",
       "plane                                None  None   \n",
       "car                                  None  None   \n",
       "doctor    Retrofitting: 0.34, Human: 0.70  None   \n",
       "nurse                                None  None   \n",
       "love                                 None  None   \n",
       "sex                                  None  None   \n",
       "\n",
       "                                      sex  \n",
       "cat                                  None  \n",
       "tiger                                None  \n",
       "computer                             None  \n",
       "keyboard                             None  \n",
       "plane                                None  \n",
       "car                                  None  \n",
       "doctor                               None  \n",
       "nurse                                None  \n",
       "love      Retrofitting: 0.36, Human: 0.68  \n",
       "sex                                  None  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the evaluation scores from the file\n",
    "eval_scores = {}\n",
    "with open(eval_file_path, 'r') as eval_file:\n",
    "    for line in eval_file:\n",
    "        word1, word2, score = line.strip().split('\\t')\n",
    "        eval_scores[(word1, word2)] = float(score)\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(index=wordList, columns=wordList)\n",
    "\n",
    "# Loop over each word pair and calculate similarity scores\n",
    "for word1 in wordList:\n",
    "    for word2 in wordList:\n",
    "        word1_index = wordList.index(word1)\n",
    "        word2_index = wordList.index(word2)\n",
    "        \n",
    "        # Calculate similarity score after retrofitting\n",
    "        retrofit_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "        word1_vec = retrofit_toy_vec[word1_index].reshape(1, -1)\n",
    "        word2_vec = retrofit_toy_vec[word2_index].reshape(1, -1)\n",
    "        similarity_after = cosine_similarity(word1_vec, word2_vec)[0, 0]\n",
    "        \n",
    "        # Retrieve the evaluation score for the word pair\n",
    "        score = eval_scores.get((word1, word2))\n",
    "\n",
    "        # Scale the human score between 0 and 1\n",
    "        if score is not None:\n",
    "            scaled_score = score / 10.0\n",
    "        else:\n",
    "            scaled_score = None\n",
    "        \n",
    "        # Store the scores in the DataFrame\n",
    "        results_df.loc[word1, word2] = f\"Retrofitting: {similarity_after:.2f}, Human: {scaled_score:.2f}\" if scaled_score is not None else None\n",
    "\n",
    "# Print the results DataFrame\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean \n"
     ]
    }
   ],
   "source": [
    "wordVecs_gensim = read_word_vecs(\"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\")\n",
    "lexical_similarity = read_lexicon(\"../data/English/lexicon/ws353_lexical_similarity.txt\")\n",
    "output_file_gensim = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "outFileName_gensim = output_file_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_neighbors_embedding_matrix0(wordVecs, wordList, relation_type):\n",
    "    # Retrieve synonyms for each word\n",
    "    neighbors_dict = get_wordnet_lexicon(wordList, relation_type)\n",
    "\n",
    "    # Create a set of valid neighbors\n",
    "    valid_neighbors = set(neighbor for neighbors in neighbors_dict.values() for neighbor in neighbors) & set(wordList)\n",
    "    \n",
    "    # Get the embedding size\n",
    "    embedding_size = 250 #wordVecs[next(iter(wordVecs))].shape[0]\n",
    "    \n",
    "    # Compute average embedding\n",
    "    average_embeddings = []\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        if neighbors and any(neighbor in valid_neighbors for neighbor in neighbors):\n",
    "            embeddings = np.array([\n",
    "                wordVecs[wordList.index(neighbor)]\n",
    "                for neighbor in neighbors\n",
    "                if neighbor in valid_neighbors\n",
    "            ])\n",
    "            average_embedding = np.mean(embeddings, axis=0)\n",
    "            average_embeddings.append(average_embedding)\n",
    "    \n",
    "    # Create the word embedding matrix\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "\n",
    "    return neighbors_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_dict = get_wordnet_lexicon(wordList, \"synononys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_neighbors_embedding_matrix(wordVecMat, wordList, neighbors_dict):\n",
    "    valid_neighbors = set(neighbor for neighbors in neighbors_dict.values() for neighbor in neighbors) & set(wordList)\n",
    "\n",
    "    embedding_size = wordVecMat.shape[1]\n",
    "    average_embeddings = []\n",
    "\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        if neighbors and any(neighbor in valid_neighbors for neighbor in neighbors):\n",
    "            embeddings = np.array([\n",
    "                wordVecMat[wordList.index(neighbor)] if neighbor in wordList else np.zeros(embedding_size)\n",
    "                for neighbor in neighbors\n",
    "                if neighbor in valid_neighbors\n",
    "            ])\n",
    "            if embeddings.size > 0:\n",
    "                average_embedding = np.mean(embeddings, axis=0)\n",
    "                average_embeddings.append(average_embedding)\n",
    "        else:\n",
    "            average_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "    return neighbors_embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_gensim = get_embeddings_words(wordVecs_gensim)\n",
    "wordVecMat_gensim = convert_dict_to_matrix(wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix_gensim = retrieve_neighbors_embedding_matrix(wordVecMat_gensim, wordList_gensim, neighbors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create a small subset of wordVecs dictionary\n",
    "subset_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:100]}\n",
    "# subset_neighbors_dict = {word: neighbors_dict[word] for word in neighbors_dict[:100]}\n",
    "subset_wordVecMat = wordVecMat_gensim[:100] \n",
    "\n",
    "# Create a small subset of wordList\n",
    "subset_wordList = wordList_gensim[:100]\n",
    "\n",
    "# Test the function on the subset\n",
    "neighbors_matrix = retrieve_neighbors_embedding_matrix(subset_wordVecMat, subset_wordList, neighbors_dict)\n",
    "\n",
    "# Print the result\n",
    "print(neighbors_matrix)\n",
    "print(type(neighbors_matrix))  \n",
    "print(neighbors_matrix.shape)  \n",
    "print(neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(125776, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(neighbors_matrix_gensim))  \n",
    "print(neighbors_matrix_gensim.shape)  \n",
    "print(neighbors_matrix_gensim.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(125776, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat_gensim))  \n",
    "print(wordVecMat_gensim.shape) \n",
    "print(wordVecMat_gensim.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat))  \n",
    "print(wordVecMat.shape) \n",
    "print(wordVecMat.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_gensim = get_wordnet_lexicon(wordList_gensim, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix_gensim = generate_cosine_similarity_matrix(wordVecs_gensim)\n",
    "# retrofitted_similarity_matrix_gensim = generate_cosine_similarity_matrix(wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs_test(wordVecMat, neighbors_embedding_matrix, alpha=1, beta=1, nb_iter=10):\n",
    "    newWordVecMat = np.copy(wordVecMat)\n",
    "    for _ in range(nb_iter):\n",
    "        updates = alpha * neighbors_embedding_matrix + beta * newWordVecMat\n",
    "        newWordVecMat = updates / (alpha + beta)\n",
    "\n",
    "    return newWordVecMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_subset_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:50]}\n",
    "test_subset_wordVecMat = wordVecMat_gensim[:50] \n",
    "\n",
    "test_subset_wordList = wordList_gensim[:50]\n",
    "\n",
    "test_subset_neighbors_matrix= retrieve_neighbors_embedding_matrix(test_subset_wordVecMat, test_subset_wordList, neighbors_dict)\n",
    "\n",
    "print(test_subset_neighbors_matrix)\n",
    "print(type(test_subset_neighbors_matrix))  \n",
    "print(test_subset_neighbors_matrix.shape)  \n",
    "print(test_subset_neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \",\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"the\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \".\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"of\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"-\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"and\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"in\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"to\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"'\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"a\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \")\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"(\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"is\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"s\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"for\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"was\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"on\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"that\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"as\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"it\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"with\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"by\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"\"\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"at\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"he\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"from\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"be\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"this\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"i\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"an\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"his\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"are\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"not\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"has\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"have\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"but\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"or\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"utc\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"which\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"were\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"–\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"said\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"they\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"also\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"one\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"who\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"had\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"talk\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"new\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"their\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Temp\\ipykernel_10784\\4190515266.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / norm_product\n"
     ]
    }
   ],
   "source": [
    "test_before_retrofitted_gensim_dict = convert_matrix_to_dict(test_subset_neighbors_matrix, test_subset_wordList)\n",
    "test_before_retrofitted_gensim_similarity_matrix = generate_cosine_similarity_matrix(test_before_retrofitted_gensim_dict)\n",
    "print_vec_similarities(test_subset_wordList, test_before_retrofitted_gensim_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 240. MiB for an array with shape (125776, 250) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retrofitted_wordVecs_gensim \u001b[39m=\u001b[39m retrofitting_wordVecs(wordVecMat_gensim, neighbors_matrix_gensim, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, nb_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mretrofitting_wordVecs\u001b[1;34m(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m updates \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_iter):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Calculate the number of neighbors for each word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[39m# Update the word embeddings using retrofitting formula\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     newWordVecMat \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39;49m newWordVecMat \u001b[39m+\u001b[39;49m beta \u001b[39m*\u001b[39;49m neighbors_mean_matrix) \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m beta)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Calculate the updates\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     update \u001b[39m=\u001b[39m newWordVecMat \u001b[39m-\u001b[39m wordVecMat\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 240. MiB for an array with shape (125776, 250) and data type float64"
     ]
    }
   ],
   "source": [
    "retrofitted_wordVecs_gensim = retrofitting_wordVecs(wordVecMat_gensim, neighbors_matrix_gensim, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_cosine_similarity(X, Y, sample_size=10000):\n",
    "    np.random.seed(42)  # Set a random seed for reproducibility\n",
    "    sample_X = X[np.random.choice(X.shape[0], sample_size, replace=False)]\n",
    "    sample_Y = Y[np.random.choice(Y.shape[0], sample_size, replace=False)]\n",
    "    similarities = cosine_similarity(sample_X, sample_Y)\n",
    "    avg_cos_similarity = np.mean(similarities)\n",
    "    return avg_cos_similarity\n",
    "\n",
    "# Compute the average cosine similarity using a random sample\n",
    "avg_cos_similarity = calculate_average_cosine_similarity(wordVecMat_gensim, retrofitted_wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_retrofitted_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:50]}\n",
    "subset_retrofitted_wordVecMat = retrofitted_wordVecs_gensim[:50] \n",
    "\n",
    "subset_retrofitted_wordList = wordList_gensim[:50]\n",
    "\n",
    "neighbors_matrix = retrieve_neighbors_embedding_matrix(subset_retrofitted_wordVecMat, subset_retrofitted_wordList, neighbors_dict)\n",
    "\n",
    "print(neighbors_matrix)\n",
    "print(type(neighbors_matrix))  \n",
    "print(neighbors_matrix.shape)  \n",
    "print(neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_new_retrofitted_gensim_dict = convert_matrix_to_dict(subset_retrofitted_wordVecMat, subset_retrofitted_wordList)\n",
    "new_retrofitted_gensim_similarity_matrix = generate_cosine_similarity_matrix(subset_new_retrofitted_gensim_dict)\n",
    "print_vec_similarities(wordList_gensim, new_retrofitted_gensim_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only considering the words in the lexicon similarity file\n",
    "# Retrive words from the lexical similarity file\n",
    "\n",
    "def print_lexical_similarities(wordVecs, lines):\n",
    "    # Create a list to store the words\n",
    "    word_list = []\n",
    "\n",
    "    # Iterate over the lines and extract the words\n",
    "    for line in lines:\n",
    "        words = line.strip().split('\\t')\n",
    "        word1 = words[0]\n",
    "        word2 = words[1]\n",
    "        word_list.append((word1, word2))  # Store the words as a tuple\n",
    "\n",
    "    # Determine the subset of words present in the wordVecs file while preserving the order\n",
    "    subset = [word for word in word_list if word[0] in wordVecs and word[1] in wordVecs]\n",
    "\n",
    "    # Create a dictionary to map words to indices\n",
    "    w2i = {word: index for index, word in enumerate(wordVecs)}\n",
    "\n",
    "    # Create an empty list to store the similarities\n",
    "    similarities = []\n",
    "\n",
    "    # Iterate over each tuple in the subset\n",
    "    for word1, word2 in subset:\n",
    "        if word1 in wordVecs and word2 in wordVecs:\n",
    "            # Retrieve the embeddings for the words\n",
    "            embedding1 = wordVecs[word1]\n",
    "            embedding2 = wordVecs[word2]\n",
    "\n",
    "            # Calculate the similarity between the embeddings\n",
    "            similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "            # Append the similarity value to the list of similarities\n",
    "            similarities.append(similarity_score)\n",
    "\n",
    "    # Print the similarities\n",
    "    for i, similarity_score in enumerate(similarities):\n",
    "        print(f\"Similarity between {subset[i][0]} and {subset[i][1]}: {similarity_score[0][0]}\")\n",
    "\n",
    "\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print_lexical_similarities(wordVecs_gensim, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_similarities(wordVecs, lines, subset):\n",
    "    similarities = []\n",
    "\n",
    "    for word1, word2 in subset:\n",
    "        if word1 in wordVecs and word2 in wordVecs:\n",
    "            embedding1 = wordVecs[word1]\n",
    "            embedding2 = wordVecs[word2]\n",
    "\n",
    "            similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "            similarities.append((word1, word2, similarity_score[0][0]))\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "word_list = []\n",
    "for line in lines:\n",
    "    words = line.strip().split('\\t')\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    word_list.append((word1, word2))\n",
    "\n",
    "subset = [(word1, word2) for word1, word2 in word_list if word1 in wordVecs_gensim and word2 in wordVecs_gensim]\n",
    "\n",
    "similarities = get_lexical_similarities(wordVecs_gensim, lines, subset)\n",
    "\n",
    "for word1, word2, similarity_score in similarities:\n",
    "    print(f\"Similarity between {word1} and {word2}: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update_EN = -1  # Variable to store the best similarity score\n",
    "best_params_EN = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_EN_vec = retrofitting_wordVecs_test(wordVecMat_gensim, neighbors_matrix_gensim, alpha, beta, nb_iter)\n",
    "            embed_update_EN = measure_embedding_updates(wordVecMat_gensim, retrofitted_EN_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update_EN > best_embed_update_EN:\n",
    "                best_embed_update_EN = embed_update_EN\n",
    "                best_params_EN = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params_EN)\n",
    "print(\"Best embedding update:\", best_embed_update_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gensim = convert_matrix_to_dict(retrofitted_wordVecs_gensim, wordList_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_word_vecs(output_gensim, outFileName_gensim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs_FR = read_word_vecs(\"../data/French/word_embeddings/vecs100-linear-frwiki\")\n",
    "lexical_similarity_FR = read_lexicon(\"../data/French/lexicon/rg65_french.txt\")\n",
    "output_file_FR = \"../data/French/output_vectors/output_vectors.txt\"\n",
    "outFileName_FR = output_file_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_lexicon_FR(target_words, relation_types):\n",
    "    lexicon = {}\n",
    "        \n",
    "    for word in target_words:\n",
    "        related_words = []\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Skip word if no synsets found\n",
    "        if not word_synsets:\n",
    "            continue\n",
    "\n",
    "        for syn in word_synsets:\n",
    "            for lemma in syn.lemmas('fra'):\n",
    "                if lemma.name() != word:\n",
    "                    if \"synonyms\" in relation_types:\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"antonyms\" in relation_types:\n",
    "                if syn.lemmas('fra')[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            if \"hyponyms\" in relation_types:\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"hypernyms\" in relation_types:\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"meronyms\" in relation_types:\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"holonyms\" in relation_types:\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"homonyms\" in relation_types:\n",
    "                for lemma in syn.lemmas('fra'):\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wordnet.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_dict = get_wordnet_lexicon_FR(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_FR = get_embeddings_words(wordVecs_FR)\n",
    "wordVecMat_FR = convert_dict_to_matrix(wordVecs_FR)\n",
    "neighbors_dict_FR = get_wordnet_lexicon(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix_FR = retrieve_neighbors_embedding_matrix(wordVecMat_FR, wordList_FR, neighbors_dict_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrofitted_wordVecs_FR, updates_FR= retrofitting_wordVecs_test(wordVecMat_FR, neighbors_matrix_FR, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cos_similarity = calculate_average_cosine_similarity(wordVecMat_FR, retrofitted_wordVecs_FR)\n",
    "avg_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/French/lexicon/rg65_french.txt', 'r') as file:\n",
    "    lines_FR = file.readlines()\n",
    "\n",
    "print_lexical_similarities(wordVecs_FR, lines_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update_FR = -1  # Variable to store the best similarity score\n",
    "best_params_FR = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_FR_vec = retrofitting_wordVecs_test(wordVecMat_FR, neighbors_matrix_FR, alpha, beta, nb_iter)\n",
    "            embed_update_FR = measure_embedding_updates(wordVecMat_FR, retrofitted_FR_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update_FR > best_embed_update_FR:\n",
    "                best_embed_update_FR = embed_update_FR\n",
    "                best_params_FR = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params_FR)\n",
    "print(\"Best embedding update:\", best_embed_update_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_FR = convert_matrix_to_dict(retrofitted_wordVecs_FR, wordList_FR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
