{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/deeksha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/deeksha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bypasses certification issue\n",
    "#comment this out if you don't have any problem\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#download WordNet\n",
    "nltk.download('wordnet')\n",
    "#download multilingual Wordnet\n",
    "nltk.download('omw-1.4') \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#synset() is used to retrieve synsets\\n#synset=set of synonyms that share a common meaning\\n\\n#fetch all synsets of \"dog\"\\ndog = wn.synsets(\\'dog\\')\\n#fetch all synsets of \"dog\" with restricted POS\\n#dog = wn.synsets(\\'dog\\', pos=\\'v\\')\\n#prints a list of all the words related to \\'tiger\\'\\nprint(dog)\\nprint(\" \")\\n\\n#for every synset of the word tiger\\n#get the synset name(id), definition and examples of its usage\\nfor synset in dog:\\n    print(f\"Synset: {synset.name()}\")\\n    print(f\"Definition: {synset.definition()}\")\\n    print(f\"Example: {synset.examples()}\\n\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#synset() is used to retrieve synsets \n",
    "#synset=set of synonyms that share a common meaning\n",
    "\n",
    "#fetch all synsets of \"dog\"\n",
    "dog = wn.synsets('dog')\n",
    "#fetch all synsets of \"dog\" with restricted POS\n",
    "#dog = wn.synsets('dog', pos='v')\n",
    "#prints a list of all the words related to 'tiger'\n",
    "print(dog)\n",
    "print(\" \")\n",
    "\n",
    "#for every synset of the word tiger\n",
    "#get the synset name(id), definition and examples of its usage\n",
    "for synset in dog:\n",
    "    print(f\"Synset: {synset.name()}\")\n",
    "    print(f\"Definition: {synset.definition()}\")\n",
    "    print(f\"Example: {synset.examples()}\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndog = wn.synset('dog.n.01') #noun1\\ncat = wn.synset('cat.n.01') #noun2\\ntiger = wn.synset('tiger.n.01') #noun3\\nchase = wn.synset('chase.v.01') #verb1 from 'dog'\\n\\n#fetch synonyms of each word sense of 'tiger'\\nprint(wn.synonyms('tiger'))\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dog = wn.synset('dog.n.01') #noun1\n",
    "cat = wn.synset('cat.n.01') #noun2\n",
    "tiger = wn.synset('tiger.n.01') #noun3\n",
    "chase = wn.synset('chase.v.01') #verb1 from 'dog'\n",
    "\n",
    "#fetch synonyms of each word sense of 'tiger'\n",
    "print(wn.synonyms('tiger'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#path_similarity measures the semantic similarity between word senses (synsets)\\n#computes the shortest path between two synsets in the WordNet hierarchy\\n#shorter path = higher similarity\\n#similarity = 1 / (path_distance + 1)\\n#ranges between 0 and 1\\n#O = no similarity and 1 = max similarity (or same word)\\n#(synset1.path_similarity(synset2))\\nprint(dog.path_similarity(dog))\\nprint(dog.path_similarity(cat))\\nprint(cat.path_similarity(tiger))\\nprint(dog.path_similarity(chase))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#path_similarity measures the semantic similarity between word senses (synsets)\n",
    "#computes the shortest path between two synsets in the WordNet hierarchy\n",
    "#shorter path = higher similarity\n",
    "#similarity = 1 / (path_distance + 1)\n",
    "#ranges between 0 and 1\n",
    "#O = no similarity and 1 = max similarity (or same word)\n",
    "#(synset1.path_similarity(synset2))\n",
    "print(dog.path_similarity(dog))\n",
    "print(dog.path_similarity(cat))\n",
    "print(cat.path_similarity(tiger))\n",
    "print(dog.path_similarity(chase))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#list of synsets for the target word\\nbank_synsets = wn.synsets(\\'bank\\')\\n\\n#for every synset in the list of synsets\\nfor synset in bank_synsets:\\n    #printthe synset name and the corresponding definition\\n    print(synset.name(), \":\", synset.definition())\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#list of synsets for the target word\n",
    "bank_synsets = wn.synsets('bank')\n",
    "\n",
    "#for every synset in the list of synsets\n",
    "for synset in bank_synsets:\n",
    "    #printthe synset name and the corresponding definition\n",
    "    print(synset.name(), \":\", synset.definition())\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#function for creating a semantic lexicon dictionary \n",
    "def semantic_lexicon_dict(target_words):\n",
    "    #dictionary to store the related words for each target word\n",
    "    #keys = target words\n",
    "    #values = list related words\n",
    "    lexicon = {}\n",
    "\n",
    "    #Use WordNet to find related words through various semantic relations for each target word\n",
    "    #loop over the target words\n",
    "    for word in target_words:\n",
    "        #initialise empty related_words list which will hold \n",
    "        #all the semantically related words extracted from WordNet\n",
    "        related_words = []\n",
    "        #iterate through each synset (set of synonyms)\n",
    "        for syn in wn.synsets(word):\n",
    "            #iterate thorugh each lemma (individual word) for the synset and append to related_words\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    related_words.append(lemma.name())\n",
    "            #check antonym for first lemma of the synset and append\n",
    "            if syn.lemmas()[0].antonyms():\n",
    "                related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            #check for hyponym and append\n",
    "            for hypo in syn.hyponyms():\n",
    "                for lemma in hypo.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #check for hypernym and append\n",
    "            for hyper in syn.hypernyms():\n",
    "                for lemma in hyper.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #check for meronym and append\n",
    "            for part in syn.part_meronyms():\n",
    "                for lemma in part.lemmas():\n",
    "                    related_words.append(lemma.name())\n",
    "            #check for holonym and append\n",
    "            for whole in syn.part_holonyms():\n",
    "                for lemma in whole.lemmas(): \n",
    "                    related_words.append(lemma.name())\n",
    "            #iterate through each lemma for the current synset\n",
    "            #for each lemma not the same as target word\n",
    "            #find all lemmas that have same spelling and append\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    homonyms = wn.lemmas(lemma.name())\n",
    "                    for homonym in homonyms:\n",
    "                        related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating a semantic lexicon dictionary \n",
    "def semantic_lexicon_dict(target_words, synonyms=True, antonyms=True, hyponyms=True, hypernyms=True, meronyms=True, holonyms=True, homonyms=True):\n",
    "    #dictionary to store the related words for each target word\n",
    "    #keys = target words\n",
    "    #values = list related words\n",
    "    lexicon = {}\n",
    "\n",
    "    #Use WordNet to find related words through various semantic relations for each target word\n",
    "    #loop over the target words\n",
    "    for word in target_words:\n",
    "        #initialise empty related_words list which will hold \n",
    "        #all the semantically related words extracted from WordNet\n",
    "        related_words = []\n",
    "        #iterate through each synset (set of synonyms)\n",
    "        \n",
    "        for syn in wn.synsets(word):\n",
    "            #iterate thorugh each lemma (individual word) for the synset and append to related_words\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    #if you want to extract synonyms\n",
    "                    if synonyms:\n",
    "                        related_words.append(lemma.name())\n",
    "\n",
    "            if antonyms:           \n",
    "                #check antonym for first lemma of the synset and append\n",
    "                if syn.lemmas()[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            \n",
    "            #if you want to extract hyponyms\n",
    "            if hyponyms:\n",
    "                #check for hyponym and append\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "\n",
    "            #if you want to extract hypernyms\n",
    "            if hypernyms:            \n",
    "                #check for hypernym and append\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "\n",
    "            #if you want to extract meronyms\n",
    "            if meronyms:             \n",
    "                #check for meronym and append\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "\n",
    "            #if you want to extract hypernyms\n",
    "            if holonyms:             \n",
    "                #check for holonym and append\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas(): \n",
    "                        related_words.append(lemma.name())\n",
    "\n",
    "            #if you want to extract homonyms\n",
    "            if homonyms:             \n",
    "                #iterate through each lemma for the current synset\n",
    "                #for each lemma not the same as target word\n",
    "                #find all lemmas that have same spelling and append\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wn.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': ['goodness', 'goodness', 'commodity', 'trade_good', 'full', 'estimable', 'honorable', 'respectable', 'beneficial', 'just', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'dear', 'near', 'dependable', 'safe', 'secure', 'right', 'ripe', 'well', 'effective', 'in_effect', 'in_force', 'serious', 'sound', 'salutary', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'well', 'thoroughly', 'soundly'], 'bad': ['badness', 'big', 'tough', 'spoiled', 'spoilt', 'regretful', 'sorry', 'uncollectible', 'risky', 'high-risk', 'speculative', 'unfit', 'unsound', 'forged', 'defective', 'badly', 'badly'], 'sad': ['deplorable', 'distressing', 'lamentable', 'pitiful', 'sorry'], 'happy': ['felicitous', 'glad', 'well-chosen'], 'awesome': ['amazing', 'awe-inspiring', 'awful', 'awing'], 'scary': ['chilling', 'scarey', 'shivery', 'shuddery'], 'interesting': ['interest', 'concern', 'interest', 'occupy', 'worry', 'matter_to', 'interest'], 'boring': ['drilling', 'drilling', 'oil_production', 'bore', 'tire', 'bore', 'drill', 'deadening', 'dull', 'ho-hum', 'irksome', 'slow', 'tedious', 'tiresome', 'wearisome']}\n",
      "['drilling', 'drilling', 'oil_production', 'bore', 'tire', 'bore', 'drill', 'deadening', 'dull', 'ho-hum', 'irksome', 'slow', 'tedious', 'tiresome', 'wearisome']\n"
     ]
    }
   ],
   "source": [
    "#Define the target words (sentiment analysis task)\n",
    "\n",
    "target_words = ['good', 'bad', 'sad', 'happy', 'awesome', 'scary', 'interesting', 'boring']\n",
    "\n",
    "#Create the semantic lexicon dictionary by passing the target words as argument\n",
    "#and choosing which relation type to extract\n",
    "semantic_lexicon_dictionary = semantic_lexicon_dict(target_words, synonyms=True, antonyms=False, hyponyms=False, hypernyms=False, meronyms=False, holonyms=False, homonyms=False)\n",
    "\n",
    "#prints the semantic lexicon dictionary\n",
    "print(semantic_lexicon_dictionary)\n",
    "\n",
    "#check the semantically related words for \"bad\"\n",
    "print(semantic_lexicon_dictionary.get(\"boring\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'good': ['common_good', 'commonweal', 'advantage', 'vantage', 'goodness', 'evil', 'beneficence', 'benignity', 'benignancy', 'graciousness', 'kindness', 'saintliness', 'summum_bonum', 'virtue', 'virtuousness', 'moral_excellence', 'virtue', 'morality', 'goodness', 'goodness', 'goodness', 'bad', 'benefit', 'welfare', 'better', 'better', 'desirability', 'desirableness', 'optimum', 'wisdom', 'wiseness', 'soundness', 'worthiness', 'quality', 'goodness', 'goodness', 'commodity', 'trade_good', 'basic', 'staple', 'consumer_goods', 'drygoods', 'soft_goods', 'entrant', 'export', 'exportation', 'fancy_goods', 'fungible', 'future', 'import', 'importation', 'merchandise', 'ware', 'product', 'middling', 'salvage', 'shopping', 'sporting_goods', 'worldly_possession', 'worldly_good', 'artifact', 'artefact', 'commodity', 'trade_good', 'bad', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'full', 'evil', 'estimable', 'honorable', 'respectable', 'estimable', 'estimable', 'estimable', 'honorable', 'honorable', 'honorable', 'honorable', 'respectable', 'respectable', 'respectable', 'beneficial', 'beneficial', 'just', 'upright', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'just', 'upright', 'upright', 'upright', 'upright', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'adept', 'adept', 'expert', 'expert', 'expert', 'practiced', 'practiced', 'proficient', 'proficient', 'skillful', 'skillful', 'skilful', 'dear', 'near', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'dear', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'dependable', 'safe', 'secure', 'dependable', 'dependable', 'dependable', 'dependable', 'safe', 'safe', 'safe', 'safe', 'safe', 'safe', 'safe', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'secure', 'right', 'ripe', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'ripe', 'ripe', 'ripe', 'ripe', 'ripe', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'effective', 'in_effect', 'in_force', 'effective', 'effective', 'effective', 'effective', 'effective', 'effective', 'in_effect', 'in_effect', 'in_force', 'serious', 'serious', 'serious', 'serious', 'serious', 'serious', 'serious', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'sound', 'salutary', 'salutary', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'undecomposed', 'unspoiled', 'unspoiled', 'unspoilt', 'well', 'ill', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'thoroughly', 'soundly', 'thoroughly', 'thoroughly', 'soundly', 'soundly'], 'bad': ['badness', 'good', 'evil', 'inadvisability', 'liability', 'undesirability', 'unsoundness', 'unworthiness', 'worse', 'quality', 'badness', 'badness', 'badness', 'good', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'big', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'tough', 'spoiled', 'spoilt', 'spoiled', 'spoiled', 'spoilt', 'spoilt', 'spoilt', 'regretful', 'sorry', 'unregretful', 'regretful', 'sorry', 'sorry', 'sorry', 'sorry', 'uncollectible', 'uncollectible', 'risky', 'high-risk', 'speculative', 'risky', 'risky', 'high-risk', 'speculative', 'speculative', 'speculative', 'unfit', 'unsound', 'unfit', 'unfit', 'unfit', 'unfit', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'unsound', 'forged', 'forged', 'defective', 'defective', 'defective', 'defective', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly', 'badly'], 'sad': ['glad', 'deplorable', 'distressing', 'lamentable', 'pitiful', 'sorry', 'deplorable', 'deplorable', 'deplorable', 'distressing', 'distressing', 'lamentable', 'pitiful', 'pitiful', 'pitiful', 'sorry', 'sorry', 'sorry', 'sorry'], 'happy': ['unhappy', 'felicitous', 'felicitous', 'felicitous', 'glad', 'glad', 'glad', 'glad', 'glad', 'glad', 'well-chosen', 'well-chosen'], 'awesome': ['amazing', 'awe-inspiring', 'awful', 'awing', 'amazing', 'amazing', 'awe-inspiring', 'awful', 'awful', 'awful', 'awful', 'awful', 'awful', 'awful', 'awing'], 'scary': ['chilling', 'scarey', 'shivery', 'shuddery', 'chilling', 'chilling', 'scarey', 'shivery', 'shivery', 'shuddery'], 'interesting': ['interest', 'bore', 'absorb', 'engross', 'engage', 'occupy', 'fascinate', 'transfix', 'grip', 'spellbind', 'arouse', 'elicit', 'enkindle', 'kindle', 'evoke', 'fire', 'raise', 'provoke', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'concern', 'interest', 'occupy', 'worry', 'concern', 'concern', 'concern', 'concern', 'concern', 'concern', 'concern', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'occupy', 'occupy', 'occupy', 'occupy', 'occupy', 'occupy', 'occupy', 'occupy', 'worry', 'worry', 'worry', 'worry', 'worry', 'worry', 'worry', 'worry', 'matter_to', 'interest', 'intrigue', 'fascinate', 'refer', 'pertain', 'relate', 'concern', 'come_to', 'bear_on', 'touch', 'touch_on', 'have-to_doe_with', 'matter_to', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'interest', 'uninteresting'], 'boring': ['drilling', 'creating_by_removal', 'drilling', 'drilling', 'drilling', 'oil_production', 'production', 'drilling', 'drilling', 'oil_production', 'bore', 'tire', 'interest', 'bore', 'bore', 'bore', 'bore', 'bore', 'bore', 'tire', 'tire', 'tire', 'tire', 'tire', 'bore', 'drill', 'counter-drill', 'spud', 'trepan', 'cut', 'bore', 'bore', 'bore', 'bore', 'bore', 'bore', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'deadening', 'dull', 'ho-hum', 'irksome', 'slow', 'tedious', 'tiresome', 'wearisome', 'deadening', 'deadening', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'ho-hum', 'irksome', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'tedious', 'tedious', 'tiresome', 'wearisome']}\n",
      "['drilling', 'creating_by_removal', 'drilling', 'drilling', 'drilling', 'oil_production', 'production', 'drilling', 'drilling', 'oil_production', 'bore', 'tire', 'interest', 'bore', 'bore', 'bore', 'bore', 'bore', 'bore', 'tire', 'tire', 'tire', 'tire', 'tire', 'bore', 'drill', 'counter-drill', 'spud', 'trepan', 'cut', 'bore', 'bore', 'bore', 'bore', 'bore', 'bore', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'drill', 'deadening', 'dull', 'ho-hum', 'irksome', 'slow', 'tedious', 'tiresome', 'wearisome', 'deadening', 'deadening', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'dull', 'ho-hum', 'irksome', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'slow', 'tedious', 'tedious', 'tiresome', 'wearisome']\n"
     ]
    }
   ],
   "source": [
    "#Define the target words (sentiment analysis task)\n",
    "\n",
    "target_words = ['good', 'bad', 'sad', 'happy', 'awesome', 'scary', 'interesting', 'boring']\n",
    "\n",
    "#Create the semantic lexicon dictionary by passing the target words as argument\n",
    "semantic_lexicon_dictionary = semantic_lexicon_dict(target_words)\n",
    "\n",
    "#prints the semantic lexicon dictionary\n",
    "print(semantic_lexicon_dictionary)\n",
    "\n",
    "#check the semantically related words for \"bad\"\n",
    "print(semantic_lexicon_dictionary.get(\"boring\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#function for creating a semantic lexicon matrix\n",
    "def semantic_lexicon_mat(semantic_lexicon_dictionary, target_words_list, NEW_ROWS, NEW_COLUMNS):\n",
    "    #a set (set because duplicates are removed) of unique semantic relations\n",
    "    #these relations will serve as columns in the matrix\n",
    "    semantic_relations = set()\n",
    "    for rel_word in semantic_lexicon_dictionary.values():\n",
    "        semantic_relations.update(rel_word)\n",
    "    \n",
    "    #Create a matrix filled with zeroes\n",
    "    #Each row represents a target word\n",
    "    #Each column represents a semantic relation\n",
    "    ROWS = target_words_list\n",
    "    COLUMNS = semantic_relations\n",
    "    semantic_lexicon_matrix = np.zeros((len(ROWS), len(COLUMNS)), dtype=int)\n",
    "    \n",
    "    #loop over the target_words_list and gets the word and its index\n",
    "    for i, target_word in enumerate(target_words_list):\n",
    "        #retrieve semantic related words for current target word from the semantic_lexicon_dict\n",
    "        related_words = semantic_lexicon_dictionary.get(target_word, [])\n",
    "        #loop over each related word for the current target word\n",
    "        for related_word in related_words:\n",
    "            #get the index of the related word from the semantic_relations\n",
    "            #converted to list so that index() can be used\n",
    "            j = list(semantic_relations).index(related_word)\n",
    "            #set the element (i, j) to 1 to indicate the relationship between\n",
    "            #the target word and the semantically related word\n",
    "            semantic_lexicon_matrix[i, j] = 1\n",
    "    \n",
    "    return semantic_lexicon_matrix\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNEW_ROWS = 100\\nNEW_COLUMNS = 150\\n\\n#create the semantic lexicon matrix by passing the semantic lexicon dictionary and target words as arguments\\nsemantic_lexicon_matrix = semantic_lexicon_mat(semantic_lexicon, target_words, NEW_ROWS, NEW_COLUMNS)\\n\\n#returns the semantic lexicon as a matrix\\n#with each row representing a target word\\n#and each column representing a semantically related word from WordNet \\nprint(semantic_lexicon_matrix)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#uses the same semantic lexicon dictionary created earlier\n",
    "#assigned to semantic_lexicon just for readability\n",
    "semantic_lexicon = semantic_lexicon_dictionary\n",
    "\n",
    "#adjust dimensionality of matrix\n",
    "'''\n",
    "'''dimensionality to be manipulated here to match word embeddings'''\n",
    "'''\n",
    "NEW_ROWS = 100\n",
    "NEW_COLUMNS = 150\n",
    "\n",
    "#create the semantic lexicon matrix by passing the semantic lexicon dictionary and target words as arguments\n",
    "semantic_lexicon_matrix = semantic_lexicon_mat(semantic_lexicon, target_words, NEW_ROWS, NEW_COLUMNS)\n",
    "\n",
    "#returns the semantic lexicon as a matrix\n",
    "#with each row representing a target word\n",
    "#and each column representing a semantically related word from WordNet \n",
    "print(semantic_lexicon_matrix)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#numpy.ndarray = dense matrix\\nprint(type(semantic_lexicon_matrix))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#numpy.ndarray = dense matrix\n",
    "print(type(semantic_lexicon_matrix))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
