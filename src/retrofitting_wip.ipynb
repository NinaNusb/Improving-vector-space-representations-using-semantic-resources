{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: keep?\n",
    "\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptu file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "  lexicon = {}\n",
    "  for line in open(filename, 'r'):\n",
    "    words = line.lower().strip().split()\n",
    "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "  return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify the input so that it doesn't take one word a the time but rather matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should we modify it to use np.arrays, tensors?\n",
    "\n",
    "# Hyperparameters\n",
    "ALPHA = 1 #coefficient minimizing the euclidean distance\n",
    "BETA = 1/ len(lexicon) # minimizes each vector distance to ?\n",
    "NB_ITER = 10\n",
    "\n",
    "''' Retrofit word vectors to a lexicon '''\n",
    "def retrofit(wordVecs, lexicon):\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for _ in range(NB_ITER):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the data estimate if the number of neighbours\n",
    "      newVec = ALPHA * wordVecs[word] + BETA * sum(newWordVecs[ppWord] for ppWord in wordNeighbours) \n",
    "      newWordVecs[word] = newVec/ (ALPHA + BETA * numNeighbours)\n",
    "  return newWordVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean \n"
     ]
    }
   ],
   "source": [
    "input_file = \"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\"\n",
    "lexicon_file = \"../data/English/lexicon/ws353_lexical_similarity.txt\"\n",
    "output_file = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "\n",
    "wordVecs = read_word_vecs(input_file)\n",
    "lexicon = read_lexicon(lexicon_file)\n",
    "outFileName = output_file\n",
    "\n",
    "# ''' Enrich the word vectors using ppdb and print the enriched vectors '''\n",
    "# print_word_vecs(retrofit(wordVecs, lexicon, numIter), outFileName)\n",
    "retrofittedVecs = retrofit(wordVecs, lexicon)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outputFile:\n",
    "    for word in retrofittedVecs.keys():\n",
    "        outputFile.write(word + ' ' + ' '.join(str(x) for x in retrofittedVecs[word]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check similarity before and after retroffit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec, GloVe: These models assign a vector to each word in a high-dimensional vector space based on the context in which the word appears. The similarity between two words can then be calculated as the cosine similarity between their corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available pre-trained models\n",
    "# gensim.downloader.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "# download the pre-trained word2vec model\n",
    "model_name = 'word2vec-google-news-300'\n",
    "path = api.load(model_name, return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"cat\", \"dog\", \"caramel\", \"cheese\", \"chocolate\", \"right\", \"left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between \"cat\" and \"dog\": 0.76\n",
      "Similarity between \"dog\" and \"caramel\": 0.16\n",
      "Similarity between \"caramel\" and \"cheese\": 0.48\n",
      "Similarity between \"cheese\" and \"chocolate\": 0.61\n",
      "Similarity between \"chocolate\" and \"right\": 0.04\n",
      "Similarity between \"right\" and \"left\": 0.49\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "for i in range(len(toy_corpus) -1):\n",
    "    word1, word2 = toy_corpus[i], toy_corpus[i+1]\n",
    "    similarity = model.similarity(word1, word2)\n",
    "    print(f'Similarity between \"{word1}\" and \"{word2}\": {similarity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert toy_corpus to a co-occurrence matrix '''\n",
    "from gensim import corpora, matutils\n",
    "\n",
    "# Create a dictionary from the corpus\n",
    "dictionary = corpora.Dictionary([toy_corpus])\n",
    "\n",
    "# Convert the corpus to a bag-of-words representation\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in [toy_corpus]]\n",
    "\n",
    "# Convert the corpus to a co-occurrence matrix\n",
    "cooccur_matrix = matutils.corpus2csc(bow_corpus).dot(matutils.corpus2csc(bow_corpus).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use the pre-trained Word2Vec model to create an embedding matrix '''\n",
    "embedding_matrix = {}\n",
    "for word in toy_corpus:\n",
    "    embedding = model[word]\n",
    "    embedding_matrix[dictionary.token2id[word]] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrofit the embedding matrix '''\n",
    "# from retrofitting import retrofit\n",
    "# package used here is not the same as the retrofitting algorithm proposed by Faruqui et al. (2015), \n",
    "# but rather an implementation of the algorithm proposed by Mrksic et al. (2017) which is faster and more scalable\n",
    "cooccur_matrix_dict = cooccur_matrix.todok()\n",
    "retrofitted_embeddings = retrofit(embedding_matrix, cooccur_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cat' and 'dog' before retrofitting: 0.83\n",
      "Similarity between 'cat' and 'dog' after retrofitting: 0.83\n",
      "\n",
      "Similarity between 'cat' and 'caramel' before retrofitting: 0.52\n",
      "Similarity between 'cat' and 'caramel' after retrofitting: 0.52\n",
      "\n",
      "Similarity between 'cat' and 'cheese' before retrofitting: 0.33\n",
      "Similarity between 'cat' and 'cheese' after retrofitting: 0.33\n",
      "\n",
      "Similarity between 'cat' and 'chocolate' before retrofitting: 0.84\n",
      "Similarity between 'cat' and 'chocolate' after retrofitting: 0.84\n",
      "\n",
      "Similarity between 'cat' and 'right' before retrofitting: 0.93\n",
      "Similarity between 'cat' and 'right' after retrofitting: 0.93\n",
      "\n",
      "Similarity between 'cat' and 'left' before retrofitting: 0.93\n",
      "Similarity between 'cat' and 'left' after retrofitting: 0.93\n",
      "\n",
      "Similarity between 'dog' and 'caramel' before retrofitting: 0.86\n",
      "Similarity between 'dog' and 'caramel' after retrofitting: 0.86\n",
      "\n",
      "Similarity between 'dog' and 'cheese' before retrofitting: 0.76\n",
      "Similarity between 'dog' and 'cheese' after retrofitting: 0.76\n",
      "\n",
      "Similarity between 'dog' and 'chocolate' before retrofitting: 0.24\n",
      "Similarity between 'dog' and 'chocolate' after retrofitting: 0.24\n",
      "\n",
      "Similarity between 'dog' and 'right' before retrofitting: 0.89\n",
      "Similarity between 'dog' and 'right' after retrofitting: 0.89\n",
      "\n",
      "Similarity between 'dog' and 'left' before retrofitting: 0.92\n",
      "Similarity between 'dog' and 'left' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'caramel' and 'cheese' before retrofitting: 0.39\n",
      "Similarity between 'caramel' and 'cheese' after retrofitting: 0.39\n",
      "\n",
      "Similarity between 'caramel' and 'chocolate' before retrofitting: 0.82\n",
      "Similarity between 'caramel' and 'chocolate' after retrofitting: 0.82\n",
      "\n",
      "Similarity between 'caramel' and 'right' before retrofitting: 0.92\n",
      "Similarity between 'caramel' and 'right' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'caramel' and 'left' before retrofitting: 0.92\n",
      "Similarity between 'caramel' and 'left' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'cheese' and 'chocolate' before retrofitting: 0.70\n",
      "Similarity between 'cheese' and 'chocolate' after retrofitting: 0.70\n",
      "\n",
      "Similarity between 'cheese' and 'right' before retrofitting: 0.95\n",
      "Similarity between 'cheese' and 'right' after retrofitting: 0.95\n",
      "\n",
      "Similarity between 'cheese' and 'left' before retrofitting: 0.96\n",
      "Similarity between 'cheese' and 'left' after retrofitting: 0.96\n",
      "\n",
      "Similarity between 'chocolate' and 'right' before retrofitting: 0.92\n",
      "Similarity between 'chocolate' and 'right' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'chocolate' and 'left' before retrofitting: 0.89\n",
      "Similarity between 'chocolate' and 'left' after retrofitting: 0.89\n",
      "\n",
      "Similarity between 'right' and 'left' before retrofitting: 0.51\n",
      "Similarity between 'right' and 'left' after retrofitting: 0.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Use the retrofitted embeddings to compute semantic similarity '''\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for i, word1 in enumerate(toy_corpus):\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:], i+1):\n",
    "        similarity_before = cosine(embedding_matrix[i], embedding_matrix[j])\n",
    "        similarity_after = cosine(retrofitted_embeddings[i], retrofitted_embeddings[j])\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' before retrofitting: {similarity_before:.2f}\")\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' after retrofitting: {similarity_after:.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
