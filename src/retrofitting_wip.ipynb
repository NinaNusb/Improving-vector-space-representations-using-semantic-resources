{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving vector space using retrofitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, matutils\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement the retrofitting algorithm proposed by Faruqui et al. on a lexicon of distributional vector representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "''' Getting the pretrained word embeddings '''\n",
    "print(gensim.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "# available pre-trained models\n",
    "# gensim.downloader.info()\n",
    "\n",
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# download the pre-trained word2vec model\n",
    "model_name = 'word2vec-google-news-300'\n",
    "path = api.load(model_name, return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initial functions (from Faruqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Faruqi\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()\n",
    "  \n",
    "\n",
    "  \n",
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "\n",
    "''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptut file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()\n",
    "\n",
    "\n",
    "  ''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "  lexicon = {}\n",
    "  for line in open(filename, 'r'):\n",
    "    words = line.lower().strip().split()\n",
    "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "  return lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean \n"
     ]
    }
   ],
   "source": [
    "input_file = \"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\"\n",
    "lexicon_file = \"../data/English/lexicon/ws353_lexical_similarity.txt\" \n",
    "output_file = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "\n",
    "wordVecs = read_word_vecs(input_file)\n",
    "lexicon = read_lexicon(lexicon_file)\n",
    "outFileName = output_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Retrofitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRetrofit word vectors to a lexical ontology.\\n\\nParameters:\\n-----------\\nwordVecs : numpy.ndarray\\n    Matrix containing word embeddings where each row represents a word vector.\\nlexicon : scipy.sparse.csr_matrix\\n    Sparse matrix where each row represents a node and its neighbors in the ontology.\\nalpha : float\\n    Scaling factor for the original embedding. Default is 1.0.\\nbeta : float\\n    Scaling factor for the ontology. Default is 1.0.\\nnum_iters : int\\n    Number of iterations to run the algorithm. Default is 10.\\n\\nReturns:\\n--------\\nnumpy.ndarray\\n    Matrix containing the retrofitted word embeddings where each row represents a word vector.\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add comments of the form of\n",
    "\"\"\"\n",
    "Retrofit word vectors to a lexical ontology.\n",
    "\n",
    "Parameters:\n",
    "-----------\n",
    "wordVecs : numpy.ndarray\n",
    "    Matrix containing word embeddings where each row represents a word vector.\n",
    "lexicon : scipy.sparse.csr_matrix\n",
    "    Sparse matrix where each row represents a node and its neighbors in the ontology.\n",
    "alpha : float\n",
    "    Scaling factor for the original embedding. Default is 1.0.\n",
    "beta : float\n",
    "    Scaling factor for the ontology. Default is 1.0.\n",
    "num_iters : int\n",
    "    Number of iterations to run the algorithm. Default is 10.\n",
    "\n",
    "Returns:\n",
    "--------\n",
    "numpy.ndarray\n",
    "    Matrix containing the retrofitted word embeddings where each row represents a word vector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Working version but no matrix involved'''\n",
    "# Hyperparameters\n",
    "ALPHA = 1 #coefficient minimizing the euclidean distance\n",
    "BETA = 1/ len(lexicon) # minimizes each vector distance to ? \n",
    "NB_ITER = 10\n",
    "\n",
    "''' Retrofit word vectors to a lexicon '''\n",
    "def retrofit(wordVecs, lexicon):\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for _ in range(NB_ITER):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the data estimate if the number of neighbours\n",
    "      newVec = ALPHA * wordVecs[word] + BETA * sum(newWordVecs[ppWord] for ppWord in wordNeighbours) \n",
    "      newWordVecs[word] = newVec/ (ALPHA + BETA * numNeighbours)\n",
    "  return newWordVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Version with modified beta = 1 over set of synonyms '''\n",
    "\n",
    "def retrofit_beta_updated(wordVecs, lexicon):\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    wvVocab = set(newWordVecs.keys())\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "    for _ in range(NB_ITER):\n",
    "        # loop through every node also in ontology (else just use data estimate)\n",
    "        for word in loopVocab:\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "            # no neighbours, pass - use data estimate\n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "            BETA = 1 / numNeighbours  # BETA as one over the number of neighbors\n",
    "            # the weight of the data estimate is the number of neighbours\n",
    "            newVec = ALPHA * wordVecs[word] + BETA * sum(newWordVecs[ppWord] for ppWord in wordNeighbours)\n",
    "            newWordVecs[word] = newVec / (ALPHA + BETA * numNeighbours)\n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrofit word vectors to a lexicon using matrix operations '''\n",
    "def retrofit_matrix(wordVecs, lexicon):\n",
    "    # Convert the word vectors dictionary to a matrix\n",
    "    wordVecMat = np.array(list(wordVecs.values()))\n",
    "    \n",
    "    # Create a set of vocabulary indices based on the shape of wordVecMat\n",
    "    wvVocab = set(range(wordVecMat.shape[0]))\n",
    "    \n",
    "    # Find the common vocabulary between wordVecMat and the lexicon\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "    \n",
    "    # Iterate over the specified number of iterations\n",
    "    for _ in range(NB_ITER):\n",
    "        for word in loopVocab:\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "            \n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "            \n",
    "            newVec = ALPHA * wordVecMat[word] + BETA * np.sum(wordVecMat[list(wordNeighbours)], axis=0)\n",
    "            newWordVec = newVec / (ALPHA + BETA * numNeighbours)\n",
    "            \n",
    "            wordVecMat[word] = newWordVec\n",
    "    \n",
    "    # Convert the matrix back to a dictionary of word vectors\n",
    "    newWordVecs = {word: vec for word, vec in zip(wordVecs.keys(), wordVecMat)}\n",
    "    \n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_word_vecs(retrofit(wordVecs, lexicon, numIter), outFileName)\n",
    "retrofittedVecs = retrofit_beta_updated(wordVecs, lexicon)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outputFile:\n",
    "    for word in retrofittedVecs.keys():\n",
    "        outputFile.write(word + ' ' + ' '.join(str(x) for x in retrofittedVecs[word]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Temp\\ipykernel_16620\\3965707036.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  wordVecMat = np.array(list(wordVecs.values()))\n"
     ]
    }
   ],
   "source": [
    "# Apply retrofitting using matrix operations\n",
    "retrofittedVecMat = retrofit_matrix(wordVecs, lexicon)\n",
    "\n",
    "# Convert the retrofitted matrix back to a dictionary of word vectors\n",
    "retrofittedVecs = {}\n",
    "for word, vec in zip(wordVecs.keys(), retrofittedVecMat):\n",
    "    retrofittedVecs[word] = vec\n",
    "\n",
    "# Save the retrofitted vectors to an output file\n",
    "with open(output_file, 'w', encoding='utf-8') as outputFile:\n",
    "    for word, vec in retrofittedVecs.items():\n",
    "        outputFile.write(word + ' ' + ' '.join(str(val) for val in vec) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use semantic resources (such as WOLF for French or PPDB/WordNet for English) to enhance the lexicon by incorporating knowledge from synonymy, hypernymy relations, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extraction of semantic relations in both languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Loading ppdb datafile '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Loading ppdb datafile '''\n",
    "# lexicon = {}\n",
    "# with open('ppdb-2.0-xl-all', 'r') as f:\n",
    "#     for line in f:\n",
    "#         fields = line.strip().split('\\t')\n",
    "#         if len(fields) == 2:\n",
    "#             lexicon[(fields[0], fields[1])] = 1.0\n",
    "#         elif len(fields) == 3:\n",
    "#             lexicon[(fields[0], fields[1])] = float(fields[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the performance of the retrofitting algorithm on two tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Lexical similarity task: Measure the improvement in capturing semantic relationships between words in the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Word2Vec, GloVe: These models assign a vector to each word in a high-dimensional vector space based on the context in which the word appears. The similarity between two words can then be calculated as the cosine similarity between their corresponding vectors.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: check similarity before and after retroffit\n",
    "# ''' Cosine similarity '''\n",
    "# for i, word1 in enumerate(output_file[:-1]):\n",
    "#     print(f'Similarities with \"{word1}\":')\n",
    "#     for j, word2 in enumerate(output_file[i+1:]):\n",
    "#         similarity = model.similarity(word1, word2)\n",
    "#         print(f'  - \"{word2}\": {similarity:.2f}')\n",
    "#     print()\n",
    "\n",
    "''' Word2Vec, GloVe: These models assign a vector to each word in a high-dimensional vector space based on the context in which the word appears. The similarity between two words can then be calculated as the cosine similarity between their corresponding vectors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"cat\", \"dog\", \"caramel\", \"cheese\", \"chocolate\", \"right\", \"left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"dog\": 0.76\n",
      "  - \"caramel\": 0.17\n",
      "  - \"cheese\": 0.14\n",
      "  - \"chocolate\": 0.24\n",
      "  - \"right\": 0.08\n",
      "  - \"left\": 0.11\n",
      "\n",
      "Similarities with \"dog\":\n",
      "  - \"caramel\": 0.16\n",
      "  - \"cheese\": 0.18\n",
      "  - \"chocolate\": 0.30\n",
      "  - \"right\": 0.11\n",
      "  - \"left\": 0.08\n",
      "\n",
      "Similarities with \"caramel\":\n",
      "  - \"cheese\": 0.48\n",
      "  - \"chocolate\": 0.67\n",
      "  - \"right\": 0.07\n",
      "  - \"left\": 0.07\n",
      "\n",
      "Similarities with \"cheese\":\n",
      "  - \"chocolate\": 0.61\n",
      "  - \"right\": 0.08\n",
      "  - \"left\": 0.08\n",
      "\n",
      "Similarities with \"chocolate\":\n",
      "  - \"right\": 0.04\n",
      "  - \"left\": 0.05\n",
      "\n",
      "Similarities with \"right\":\n",
      "  - \"left\": 0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Cosine similarity '''\n",
    "for i, word1 in enumerate(toy_corpus[:-1]):\n",
    "    print(f'Similarities with \"{word1}\":')\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:]):\n",
    "        similarity = model.similarity(word1, word2)\n",
    "        print(f'  - \"{word2}\": {similarity:.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert toy_corpus to a co-occurrence matrix '''\n",
    "# Create a dictionary from the corpus\n",
    "dictionary = corpora.Dictionary([toy_corpus])\n",
    "\n",
    "# Convert the corpus to a bag-of-words representation\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in [toy_corpus]]\n",
    "\n",
    "# Convert the corpus to a co-occurrence matrix\n",
    "cooccur_matrix = matutils.corpus2csc(bow_corpus).dot(matutils.corpus2csc(bow_corpus).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use the pre-trained Word2Vec model to create an embedding matrix '''\n",
    "embedding_matrix = {}\n",
    "for word in toy_corpus:\n",
    "    embedding = model[word]\n",
    "    embedding_matrix[dictionary.token2id[word]] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrofit the embedding matrix '''\n",
    "# Could also use the 'from retrofitting import retrofit' package\n",
    "# = an implementation of the algorithm proposed by Mrksic et al. (2017) which is faster and more scalable\n",
    "cooccur_matrix_dict = cooccur_matrix.todok()\n",
    "retrofitted_embeddings = retrofit(embedding_matrix, cooccur_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cat' and 'dog' before retrofitting: 0.83\n",
      "Similarity between 'cat' and 'dog' after retrofitting: 0.83\n",
      "\n",
      "Similarity between 'cat' and 'caramel' before retrofitting: 0.52\n",
      "Similarity between 'cat' and 'caramel' after retrofitting: 0.52\n",
      "\n",
      "Similarity between 'cat' and 'cheese' before retrofitting: 0.33\n",
      "Similarity between 'cat' and 'cheese' after retrofitting: 0.33\n",
      "\n",
      "Similarity between 'cat' and 'chocolate' before retrofitting: 0.84\n",
      "Similarity between 'cat' and 'chocolate' after retrofitting: 0.84\n",
      "\n",
      "Similarity between 'cat' and 'right' before retrofitting: 0.93\n",
      "Similarity between 'cat' and 'right' after retrofitting: 0.93\n",
      "\n",
      "Similarity between 'cat' and 'left' before retrofitting: 0.93\n",
      "Similarity between 'cat' and 'left' after retrofitting: 0.93\n",
      "\n",
      "Similarity between 'dog' and 'caramel' before retrofitting: 0.86\n",
      "Similarity between 'dog' and 'caramel' after retrofitting: 0.86\n",
      "\n",
      "Similarity between 'dog' and 'cheese' before retrofitting: 0.76\n",
      "Similarity between 'dog' and 'cheese' after retrofitting: 0.76\n",
      "\n",
      "Similarity between 'dog' and 'chocolate' before retrofitting: 0.24\n",
      "Similarity between 'dog' and 'chocolate' after retrofitting: 0.24\n",
      "\n",
      "Similarity between 'dog' and 'right' before retrofitting: 0.89\n",
      "Similarity between 'dog' and 'right' after retrofitting: 0.89\n",
      "\n",
      "Similarity between 'dog' and 'left' before retrofitting: 0.92\n",
      "Similarity between 'dog' and 'left' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'caramel' and 'cheese' before retrofitting: 0.39\n",
      "Similarity between 'caramel' and 'cheese' after retrofitting: 0.39\n",
      "\n",
      "Similarity between 'caramel' and 'chocolate' before retrofitting: 0.82\n",
      "Similarity between 'caramel' and 'chocolate' after retrofitting: 0.82\n",
      "\n",
      "Similarity between 'caramel' and 'right' before retrofitting: 0.92\n",
      "Similarity between 'caramel' and 'right' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'caramel' and 'left' before retrofitting: 0.92\n",
      "Similarity between 'caramel' and 'left' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'cheese' and 'chocolate' before retrofitting: 0.70\n",
      "Similarity between 'cheese' and 'chocolate' after retrofitting: 0.70\n",
      "\n",
      "Similarity between 'cheese' and 'right' before retrofitting: 0.95\n",
      "Similarity between 'cheese' and 'right' after retrofitting: 0.95\n",
      "\n",
      "Similarity between 'cheese' and 'left' before retrofitting: 0.96\n",
      "Similarity between 'cheese' and 'left' after retrofitting: 0.96\n",
      "\n",
      "Similarity between 'chocolate' and 'right' before retrofitting: 0.92\n",
      "Similarity between 'chocolate' and 'right' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'chocolate' and 'left' before retrofitting: 0.89\n",
      "Similarity between 'chocolate' and 'left' after retrofitting: 0.89\n",
      "\n",
      "Similarity between 'right' and 'left' before retrofitting: 0.51\n",
      "Similarity between 'right' and 'left' after retrofitting: 0.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Use the retrofitted embeddings to compute semantic similarity '''\n",
    "for i, word1 in enumerate(toy_corpus):\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:], i+1):\n",
    "        similarity_before = cosine(embedding_matrix[i], embedding_matrix[j])\n",
    "        similarity_after = cosine(retrofitted_embeddings[i], retrofitted_embeddings[j])\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' before retrofitting: {similarity_before:.2f}\")\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' after retrofitting: {similarity_after:.2f}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentiment analysis task: Apply the retrofitted word vectors to a corpus of film reviews and assess if they lead to better sentiment analysis performance compared to the original word vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
