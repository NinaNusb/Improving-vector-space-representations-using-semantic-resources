{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, matutils\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: keep?\n",
    "\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptut file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "  lexicon = {}\n",
    "  for line in open(filename, 'r'):\n",
    "    words = line.lower().strip().split()\n",
    "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "  return lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean \n"
     ]
    }
   ],
   "source": [
    "input_file = \"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\"\n",
    "lexicon_file = \"../data/English/lexicon/ws353_lexical_similarity.txt\"\n",
    "output_file = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "\n",
    "wordVecs = read_word_vecs(input_file)\n",
    "lexicon = read_lexicon(lexicon_file)\n",
    "outFileName = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify the input so that it doesn't take one word a the time but rather matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should we modify it to use np.arrays, tensors?\n",
    "\n",
    "# Hyperparameters\n",
    "ALPHA = 1 #coefficient minimizing the euclidean distance\n",
    "BETA = 1/ len(lexicon) # minimizes each vector distance to ? TODO: 1 over the number of neighbors?\n",
    "NB_ITER = 10\n",
    "\n",
    "''' Retrofit word vectors to a lexicon '''\n",
    "def retrofit(wordVecs, lexicon):\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for _ in range(NB_ITER):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the data estimate if the number of neighbours\n",
    "      newVec = ALPHA * wordVecs[word] + BETA * sum(newWordVecs[ppWord] for ppWord in wordNeighbours) \n",
    "      newWordVecs[word] = newVec/ (ALPHA + BETA * numNeighbours)\n",
    "  return newWordVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: work in progress, not effective yet\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "\n",
    "def retrofit_matrix(wordVecs, lexicon):\n",
    "    \"\"\"\n",
    "    Retrofit word vectors to a lexical ontology.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    wordVecs : numpy.ndarray\n",
    "        Matrix containing word embeddings where each row represents a word vector.\n",
    "    lexicon : scipy.sparse.csr_matrix\n",
    "        Sparse matrix where each row represents a node and its neighbors in the ontology.\n",
    "    alpha : float\n",
    "        Scaling factor for the original embedding. Default is 1.0.\n",
    "    beta : float\n",
    "        Scaling factor for the ontology. Default is 1.0.\n",
    "    num_iters : int\n",
    "        Number of iterations to run the algorithm. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Matrix containing the retrofitted word embeddings where each row represents a word vector.\n",
    "    \"\"\"\n",
    "   # Convert the word vectors dictionary to a matrix\n",
    "    wordVecMat = csr_matrix(np.array(list(wordVecs.values())))\n",
    "\n",
    "    # Create a copy of the word vectors matrix\n",
    "    newWordVecMat = wordVecMat.copy()\n",
    "\n",
    "    # Create a dictionary of word indices\n",
    "    wordIndices = {word: idx for idx, word in enumerate(wordVecs.keys())}\n",
    "\n",
    "    # Loop through the words in the lexicon\n",
    "    for _ in range(NB_ITER):\n",
    "        for word in lexicon.keys():\n",
    "            # Get the scores and neighbors for the current word\n",
    "            scores = np.array(lexicon[word]).ravel()\n",
    "            neighbors = np.array([wordVecs.get(w, np.zeros(wordVecMat.shape[1])) for w in lexicon[word].indices])\n",
    "\n",
    "            # Compute the neighbor matrix\n",
    "            neighborMat = csr_matrix(neighbors)\n",
    "\n",
    "            # Compute the weighted average of the neighbors\n",
    "            weightedNeighborVec = (neighborMat.transpose().dot(scores) / neighborMat.sum(axis=1)).ravel()\n",
    "\n",
    "            # Update the word vector for the current word\n",
    "            wordVec = wordVecMat[wordIndices[word], :]\n",
    "            newWordVec = ALPHA * wordVec + BETA * weightedNeighborVec\n",
    "            newWordVecMat[wordIndices[word], :] = newWordVec\n",
    "\n",
    "    # Convert the new word vectors matrix to a dictionary\n",
    "    newWordVecs = {word: newWordVecMat[wordIndices[word], :].toarray().ravel() for word in wordVecs.keys()}\n",
    "\n",
    "    # Return the new word vectors dictionary\n",
    "    return newWordVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Enrich the word vectors using ppdb and print the enriched vectors '''\n",
    "# print_word_vecs(retrofit(wordVecs, lexicon, numIter), outFileName)\n",
    "retrofittedVecs = retrofit(wordVecs, lexicon)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outputFile:\n",
    "    for word in retrofittedVecs.keys():\n",
    "        outputFile.write(word + ' ' + ' '.join(str(x) for x in retrofittedVecs[word]) + '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec, GloVe: These models assign a vector to each word in a high-dimensional vector space based on the context in which the word appears. The similarity between two words can then be calculated as the cosine similarity between their corresponding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check similarity before and after retroffit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available pre-trained models\n",
    "# gensim.downloader.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "# download the pre-trained word2vec model\n",
    "model_name = 'word2vec-google-news-300'\n",
    "path = api.load(model_name, return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"cat\", \"dog\", \"caramel\", \"cheese\", \"chocolate\", \"right\", \"left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"dog\": 0.76\n",
      "  - \"caramel\": 0.17\n",
      "  - \"cheese\": 0.14\n",
      "  - \"chocolate\": 0.24\n",
      "  - \"right\": 0.08\n",
      "  - \"left\": 0.11\n",
      "\n",
      "Similarities with \"dog\":\n",
      "  - \"caramel\": 0.16\n",
      "  - \"cheese\": 0.18\n",
      "  - \"chocolate\": 0.30\n",
      "  - \"right\": 0.11\n",
      "  - \"left\": 0.08\n",
      "\n",
      "Similarities with \"caramel\":\n",
      "  - \"cheese\": 0.48\n",
      "  - \"chocolate\": 0.67\n",
      "  - \"right\": 0.07\n",
      "  - \"left\": 0.07\n",
      "\n",
      "Similarities with \"cheese\":\n",
      "  - \"chocolate\": 0.61\n",
      "  - \"right\": 0.08\n",
      "  - \"left\": 0.08\n",
      "\n",
      "Similarities with \"chocolate\":\n",
      "  - \"right\": 0.04\n",
      "  - \"left\": 0.05\n",
      "\n",
      "Similarities with \"right\":\n",
      "  - \"left\": 0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Cosine similarity '''\n",
    "for i, word1 in enumerate(toy_corpus[:-1]):\n",
    "    print(f'Similarities with \"{word1}\":')\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:]):\n",
    "        similarity = model.similarity(word1, word2)\n",
    "        print(f'  - \"{word2}\": {similarity:.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert toy_corpus to a co-occurrence matrix '''\n",
    "# Create a dictionary from the corpus\n",
    "dictionary = corpora.Dictionary([toy_corpus])\n",
    "\n",
    "# Convert the corpus to a bag-of-words representation\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in [toy_corpus]]\n",
    "\n",
    "# Convert the corpus to a co-occurrence matrix\n",
    "cooccur_matrix = matutils.corpus2csc(bow_corpus).dot(matutils.corpus2csc(bow_corpus).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use the pre-trained Word2Vec model to create an embedding matrix '''\n",
    "embedding_matrix = {}\n",
    "for word in toy_corpus:\n",
    "    embedding = model[word]\n",
    "    embedding_matrix[dictionary.token2id[word]] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrofit the embedding matrix '''\n",
    "# Could also use the 'from retrofitting import retrofit' package\n",
    "# = an implementation of the algorithm proposed by Mrksic et al. (2017) which is faster and more scalable\n",
    "cooccur_matrix_dict = cooccur_matrix.todok()\n",
    "retrofitted_embeddings = retrofit(embedding_matrix, cooccur_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'cat' and 'dog' before retrofitting: 0.83\n",
      "Similarity between 'cat' and 'dog' after retrofitting: 0.83\n",
      "\n",
      "Similarity between 'cat' and 'caramel' before retrofitting: 0.52\n",
      "Similarity between 'cat' and 'caramel' after retrofitting: 0.52\n",
      "\n",
      "Similarity between 'cat' and 'cheese' before retrofitting: 0.33\n",
      "Similarity between 'cat' and 'cheese' after retrofitting: 0.33\n",
      "\n",
      "Similarity between 'cat' and 'chocolate' before retrofitting: 0.84\n",
      "Similarity between 'cat' and 'chocolate' after retrofitting: 0.84\n",
      "\n",
      "Similarity between 'cat' and 'right' before retrofitting: 0.93\n",
      "Similarity between 'cat' and 'right' after retrofitting: 0.93\n",
      "\n",
      "Similarity between 'cat' and 'left' before retrofitting: 0.93\n",
      "Similarity between 'cat' and 'left' after retrofitting: 0.93\n",
      "\n",
      "Similarity between 'dog' and 'caramel' before retrofitting: 0.86\n",
      "Similarity between 'dog' and 'caramel' after retrofitting: 0.86\n",
      "\n",
      "Similarity between 'dog' and 'cheese' before retrofitting: 0.76\n",
      "Similarity between 'dog' and 'cheese' after retrofitting: 0.76\n",
      "\n",
      "Similarity between 'dog' and 'chocolate' before retrofitting: 0.24\n",
      "Similarity between 'dog' and 'chocolate' after retrofitting: 0.24\n",
      "\n",
      "Similarity between 'dog' and 'right' before retrofitting: 0.89\n",
      "Similarity between 'dog' and 'right' after retrofitting: 0.89\n",
      "\n",
      "Similarity between 'dog' and 'left' before retrofitting: 0.92\n",
      "Similarity between 'dog' and 'left' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'caramel' and 'cheese' before retrofitting: 0.39\n",
      "Similarity between 'caramel' and 'cheese' after retrofitting: 0.39\n",
      "\n",
      "Similarity between 'caramel' and 'chocolate' before retrofitting: 0.82\n",
      "Similarity between 'caramel' and 'chocolate' after retrofitting: 0.82\n",
      "\n",
      "Similarity between 'caramel' and 'right' before retrofitting: 0.92\n",
      "Similarity between 'caramel' and 'right' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'caramel' and 'left' before retrofitting: 0.92\n",
      "Similarity between 'caramel' and 'left' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'cheese' and 'chocolate' before retrofitting: 0.70\n",
      "Similarity between 'cheese' and 'chocolate' after retrofitting: 0.70\n",
      "\n",
      "Similarity between 'cheese' and 'right' before retrofitting: 0.95\n",
      "Similarity between 'cheese' and 'right' after retrofitting: 0.95\n",
      "\n",
      "Similarity between 'cheese' and 'left' before retrofitting: 0.96\n",
      "Similarity between 'cheese' and 'left' after retrofitting: 0.96\n",
      "\n",
      "Similarity between 'chocolate' and 'right' before retrofitting: 0.92\n",
      "Similarity between 'chocolate' and 'right' after retrofitting: 0.92\n",
      "\n",
      "Similarity between 'chocolate' and 'left' before retrofitting: 0.89\n",
      "Similarity between 'chocolate' and 'left' after retrofitting: 0.89\n",
      "\n",
      "Similarity between 'right' and 'left' before retrofitting: 0.51\n",
      "Similarity between 'right' and 'left' after retrofitting: 0.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Use the retrofitted embeddings to compute semantic similarity '''\n",
    "for i, word1 in enumerate(toy_corpus):\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:], i+1):\n",
    "        similarity_before = cosine(embedding_matrix[i], embedding_matrix[j])\n",
    "        similarity_after = cosine(retrofitted_embeddings[i], retrofitted_embeddings[j])\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' before retrofitting: {similarity_before:.2f}\")\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' after retrofitting: {similarity_after:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
