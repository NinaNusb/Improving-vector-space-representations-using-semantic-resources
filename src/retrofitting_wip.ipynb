{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving vector space using retrofitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import urllib.request\n",
    "import io\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, matutils\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement the retrofitting algorithm proposed by Faruqui et al. on a lexicon of distributional vector representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Getting the pretrained word embeddings '''\n",
    "print(gensim.__file__)\n",
    "\n",
    "# available pre-trained models\n",
    "# gensim.downloader.info()\n",
    "\n",
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# download the pre-trained word2vec model\n",
    "# path = api.load('word2vec-google-news-300', return_path=True)\n",
    "# print(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Faruqi\n",
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()\n",
    "  \n",
    "\n",
    "  \n",
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptut file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()\n",
    "\n",
    "''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.lower().strip().split()\n",
    "            lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_matrix(wordVecs):\n",
    "    words = list(wordVecs.keys())\n",
    "    wordVecMat = np.array(list(wordVecs.values()))\n",
    "\n",
    "    return wordVecMat\n",
    "\n",
    "def convert_matrix_to_dict1(wordVecMat, wordVecs):\n",
    "    updatedWordVecs = {}\n",
    "\n",
    "    for i, word in enumerate(wordVecs.keys()):\n",
    "        updatedWordVecs[word] = wordVecMat[i]\n",
    "\n",
    "    return updatedWordVecs\n",
    "\n",
    "def convert_matrix_to_dict2(wordVecMat, wordVecs):\n",
    "    updated_wordVecs = {word: vec for word, vec in zip(wordVecs.keys(), wordVecMat)}\n",
    "\n",
    "    return updated_wordVecs\n",
    "\n",
    "def vectorize_list(corpus):\n",
    "    corpus_vecs = [model[word] for word in corpus]\n",
    "\n",
    "    return corpus_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"cat\", \"dog\", \"caramel\", \"cheese\", \"chocolate\", \"cacao\", \"right\", \"left\"]\n",
    "toy_dict_corpus = {\"random1\": [0.2, 0.4, 0.1], \"random2\": [0.5, 0.7, 0.3], \"random3\": [0.8, 0.6, 0.2], \"random3\": [0.3, 0.7, 0.2], \"random4\": [0.1, 0.3, 0.1], \"random5\": [0.4, 0.6, 0.2], \"random6\": [0.1, 0.3, 0.4], \"random7\": [0.6, 0.9, 0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dict_vecs= convert_dict_to_matrix(toy_dict_corpus)\n",
    "convert_matrix_to_dict2(toy_dict_vecs, toy_dict_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus_vecs = vectorize_list(toy_corpus)\n",
    "toy_corpus_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique words from the dictionary keys\n",
    "toy_dict_words = list(toy_dict_corpus.keys())\n",
    "\n",
    "# Determine the length of the word vectors\n",
    "toy_dict_vector_length = len(next(iter(toy_dict_corpus.values())))\n",
    "\n",
    "# Create an empty matrix with the shape (number of words, vector length)\n",
    "toy_corpus_vecs_matrix = np.zeros((len(toy_dict_words), toy_dict_vector_length))\n",
    "\n",
    "# Populate the word vectors matrix\n",
    "for idx, word in enumerate(toy_dict_words):\n",
    "    toy_corpus_vecs_matrix[idx] = toy_dict_corpus[word]\n",
    "\n",
    "print(toy_corpus_vecs_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrieving all the necessary data'''\n",
    "# word embeddings\n",
    "wordVecs = read_word_vecs(\"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\")\n",
    "# semantic relations\n",
    "lexical_similarity = read_lexicon(\"../data/English/lexicon/ws353_lexical_similarity.txt\")\n",
    "ppdb_lexicon = read_lexicon('../data/English/lexicon/ppdb-xl.txt')\n",
    "wordnet_lexicon = read_lexicon('../data/English/lexicon/wordnet-synonyms+.txt')\n",
    "# the file for the updated embeddings\n",
    "output_file = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "outFileName = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve semantic relations from WordNet\n",
    "def get_wordnet_for_specific_relation(word, relation_type):\n",
    "    relations = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    for synset in synsets:\n",
    "        if relation_type == \"synonyms\":\n",
    "            synonyms = synset.lemmas()\n",
    "            synonyms = [lemma.name() for lemma in synonyms]\n",
    "            relations.extend(synonyms)\n",
    "        elif relation_type == \"hypernyms\":\n",
    "            hypernyms = synset.hypernyms()\n",
    "            hypernyms = [hypernym.name().split('.')[0] for hypernym in hypernyms]\n",
    "            relations.extend(hypernyms)\n",
    "        elif relation_type == \"hyponyms\":\n",
    "            hyponyms = synset.hyponyms()\n",
    "            hyponyms = [hyponym.name().split('.')[0] for hyponym in hyponyms]\n",
    "            relations.extend(hyponyms)\n",
    "        # Add more relation types as needed (e.g., meronyms, holonyms, etc.)\n",
    "    return relations\n",
    "\n",
    "def get_wordnet_relations(word):\n",
    "    relations = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    for synset in synsets:\n",
    "        # Retrieve synonyms, hypernyms, hyponyms, or other related words based on your desired impact\n",
    "        relations.extend([lemma.name() for lemma in synset.lemmas()])\n",
    "        relations.extend([hypernym.lemma_names()[0] for hypernym in synset.hypernyms()])\n",
    "        relations.extend([hyponym.lemma_names()[0] for hyponym in synset.hyponyms()])\n",
    "    \n",
    "    return relations\n",
    "\n",
    "# Example usage to retrieve synonyms, hypernyms, and hyponyms for a word\n",
    "word = \"example\"\n",
    "synonyms = get_wordnet_for_specific_relation(word, \"synonyms\")\n",
    "hypernyms = get_wordnet_for_specific_relation(word, \"hypernyms\")\n",
    "hyponyms = get_wordnet_for_specific_relation(word, \"hyponyms\")\n",
    "\n",
    "wordnet_lexicon1 = {}\n",
    "for word in wordVecs:\n",
    "    wordnet_relations = get_wordnet_relations(word)\n",
    "    wordnet_lexicon[word] = wordnet_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs_matrix = convert_dict_to_matrix(wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Measure the cosine similarity before retrofitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get vectors cosine similarity before retrofitting '''\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm1 * norm2)\n",
    "    return similarity\n",
    "\n",
    "def calculate_cosine_similarity_matrix(vectors):\n",
    "    num_vectors = len(vectors)\n",
    "    similarity_matrix = np.zeros((num_vectors, num_vectors))\n",
    "\n",
    "    for i in range(num_vectors):\n",
    "        for j in range(num_vectors):\n",
    "            similarity = calculate_cosine_similarity(vectors[i], vectors[j])\n",
    "            similarity_matrix[i, j] = similarity\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# Cosine similarity from word2vec\n",
    "def print_similarities(corpus):\n",
    "    for i, word1 in enumerate(corpus):\n",
    "        print(f'Similarities with \"{word1}\":')\n",
    "        for j, word2 in enumerate(corpus):\n",
    "            if i == j:\n",
    "                continue\n",
    "            similarity = model.similarity(word1, word2) # TODO: comment Ã§a fonctionne?\n",
    "            print(f'  - \"{word2}\": {similarity:.4f}')\n",
    "        print()\n",
    "\n",
    "def print_vec_similarities(corpus, vectorized_corpus) :\n",
    "    for i, vec1 in enumerate(vectorized_corpus):\n",
    "        word1 = corpus[i]\n",
    "        print(f'Similarities with \"{word1}\":')\n",
    "        for j, vec2 in enumerate(vectorized_corpus):\n",
    "            if i == j:\n",
    "                continue\n",
    "            similarity = calculate_cosine_similarity(vec1, vec2)\n",
    "            print(f'  - \"{corpus[j]}\": {similarity:.4f}')\n",
    "        print()\n",
    "\n",
    "def print_matrix_similarities(similarity_matrix, word_index, words):\n",
    "    word = words[word_index]\n",
    "    similarities = similarity_matrix[word_index]\n",
    "    print(f\"Similarities with \\\"{word}\\\":\")\n",
    "    for idx, similarity in enumerate(similarities):\n",
    "        if idx != word_index:\n",
    "            print(f\"  - \\\"{words[idx]}\\\": {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similarities(toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_vec_similarities(toy_corpus, toy_corpus_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dict_vecs_matrix = np.array(list(toy_dict_corpus.values()))\n",
    "\n",
    "toy_dict_similarity_matrix = calculate_cosine_similarity_matrix(toy_dict_vecs_matrix)\n",
    "\n",
    "toy_dict_word_index = toy_dict_words.index(\"random1\")\n",
    "print_matrix_similarities(toy_dict_similarity_matrix, toy_dict_word_index, toy_dict_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Checking the similarity between word embeddings'''\n",
    "# 1) Sampling\n",
    "# Set the size of the subset you want to sample\n",
    "subset_size = 500\n",
    "\n",
    "# Randomly select a subset of vectors\n",
    "subset_words = random.sample(list(wordVecs.keys()), subset_size)\n",
    "subset_vectors = [wordVecs[word] for word in subset_words]\n",
    "\n",
    "# Calculate the pairwise cosine similarities for the subset\n",
    "subset_similarity_matrix = cosine_similarity(subset_vectors)\n",
    "\n",
    "# Create a word cloud for the subset\n",
    "subset_wordcloud = WordCloud().generate(' '.join(subset_words))\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.imshow(subset_wordcloud, interpolation='bilinear')\n",
    "plt.title('Subset Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the subset you want to sample\n",
    "subset_size = 500\n",
    "\n",
    "# Randomly select a subset of vectors\n",
    "subset_words = random.sample(list(wordVecs.keys()), subset_size)\n",
    "subset_vectors = [wordVecs[word] for word in subset_words]\n",
    "\n",
    "# Calculate the pairwise cosine similarities for the subset\n",
    "subset_similarity_matrix = calculate_cosine_similarity_matrix(subset_vectors)\n",
    "\n",
    "# Create a word cloud for the subset with similarity scores >= 0.8\n",
    "wordcloud_words = [word for i, word in enumerate(subset_words) if np.any(subset_similarity_matrix[i] >= 0.8)]\n",
    "wordcloud = WordCloud().generate(' '.join(wordcloud_words))\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud with Similarity Scores >= 0.8')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Clustering\n",
    "\n",
    "# Get the length of the first word vector\n",
    "vector_length = len(next(iter(wordVecs.values())))\n",
    "\n",
    "# Filter out word vectors with different lengths\n",
    "filtered_wordVecs = {word: vector for word, vector in wordVecs.items() if len(vector) == vector_length}\n",
    "\n",
    "# Check if the number of word vectors is sufficient for clustering\n",
    "num_vectors = len(filtered_wordVecs)\n",
    "num_clusters = 3  # Specify the desired number of clusters\n",
    "\n",
    "if num_vectors >= num_clusters:\n",
    "    # Perform clustering with K-means\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(list(filtered_wordVecs.values()))\n",
    "\n",
    "    # Get the cluster labels for each word\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Create a dictionary to store the cluster assignments\n",
    "    clusters = {}\n",
    "    for i, word in enumerate(filtered_wordVecs.keys()):\n",
    "        cluster_id = labels[i]\n",
    "        if cluster_id not in clusters:\n",
    "            clusters[cluster_id] = []\n",
    "        clusters[cluster_id].append(word)\n",
    "\n",
    "    # Print the words in each cluster\n",
    "    for cluster_id, words in clusters.items():\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "        print(words)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Insufficient number of word vectors for clustering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Word clouds\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the cosine similarity matrix and retrive a list of words\n",
    "words = list(wordVecs.keys())\n",
    "word_vecs_matrix = np.array(list(wordVecs.values()))\n",
    "vecs_similarity_matrix = calculate_cosine_similarity_matrix(word_vecs_matrix)\n",
    "\n",
    "# Set the similarity threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Create lists for high similarity words and low similarity words\n",
    "high_similarity_words = []\n",
    "low_similarity_words = []\n",
    "\n",
    "# Classify words based on similarity threshold\n",
    "for i in range(len(words)):\n",
    "    for j in range(i+1, len(words)):\n",
    "        similarity = vecs_similarity_matrix[i][j]\n",
    "        if similarity >= threshold:\n",
    "            high_similarity_words.append(words[i])\n",
    "            high_similarity_words.append(words[j])\n",
    "        else:\n",
    "            low_similarity_words.append(words[i])\n",
    "            low_similarity_words.append(words[j])\n",
    "\n",
    "# Create word clouds for high similarity words\n",
    "high_similarity_wordcloud = WordCloud().generate(' '.join(high_similarity_words))\n",
    "\n",
    "# Create word clouds for low similarity words\n",
    "low_similarity_wordcloud = WordCloud().generate(' '.join(low_similarity_words))\n",
    "\n",
    "# Plot the word clouds\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(high_similarity_wordcloud, interpolation='bilinear')\n",
    "axs[0].set_title('High Similarity Words')\n",
    "\n",
    "axs[1].imshow(low_similarity_wordcloud, interpolation='bilinear')\n",
    "axs[1].set_title('Low Similarity Words')\n",
    "\n",
    "# Remove axis labels\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Only considering the words in the lexicon similarity file\n",
    "# Retrive words from the lexical similarity file\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Create a list to store the words\n",
    "word_list = []\n",
    "\n",
    "# Iterate over the lines and extract the words\n",
    "for line in lines:\n",
    "    words = line.strip().split('\\t')\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    word_list.append((word1, word2))  # Store the words as a tuple\n",
    "\n",
    "# Determine the subset of words present in your wordVecs file while preserving the order\n",
    "subset = [word for word in word_list if word[0] in wordVecs and word[1] in wordVecs]\n",
    "\n",
    "# Create a dictionary to map words to indices\n",
    "w2i = {word: index for index, word in enumerate(wordVecs)}\n",
    "\n",
    "# Create an empty list to store the similarities\n",
    "similarities = []\n",
    "\n",
    "# Iterate over each tuple in the subset\n",
    "for word1, word2 in subset:\n",
    "    if word1 in wordVecs and word2 in wordVecs:\n",
    "        # Retrieve the embeddings for the words\n",
    "        embedding1 = wordVecs[word1]\n",
    "        embedding2 = wordVecs[word2]\n",
    "\n",
    "        # Calculate the similarity between the embeddings\n",
    "        similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "        # Append the similarity value to the list of similarities\n",
    "        similarities.append(similarity_score)\n",
    "\n",
    "# Print the similarities\n",
    "for i, similarity_score in enumerate(similarities):\n",
    "    print(f\"Similarity between {subset[i][0]} and {subset[i][1]}: {similarity_score[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get similaritiesof wordVecs before retrofitting\n",
    "# print(\"Similarities before retrofitting:\")\n",
    "# # Check the length of the word vectors\n",
    "# vector_length = len(next(iter(wordVecs.values())))\n",
    "\n",
    "# # Create a matrix to store the word vectors\n",
    "# word_vecs_matrix = np.zeros((len(wordVecs), vector_length))\n",
    "\n",
    "# # Populate the word vectors matrix\n",
    "# for idx, word in enumerate(wordVecs):\n",
    "#     word_vecs_matrix[idx] = wordVecs[word]\n",
    "\n",
    "# # Calculate the cosine similarities\n",
    "# similarities = cosine_similarity(word_vecs_matrix)\n",
    "\n",
    "# # Get the index of the word \"cat\" in the word embeddings\n",
    "# cat_index = list(wordVecs.keys()).index(\"cat\")\n",
    "\n",
    "# # Get the similarity scores of the word \"cat\" with other words\n",
    "# cat_similarities = similarities[cat_index]\n",
    "\n",
    "# # Print the similarity scores\n",
    "# for word, similarity in zip(wordVecs.keys(), cat_similarities):\n",
    "#     print(f\"Similarity between 'cat' and '{word}': {similarity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Retrofitting "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Update the retrofitting algorithm:\n",
    "        Consider the specific nature of the relations and the desired impact on the word embeddings. For example, synonyms may require similar adjustments, while hypernyms or hyponyms may require different weighting.\n",
    "\n",
    "    Enhance the word vectors:\n",
    "        Calculate the new vector for each word by incorporating the retrieved semantic relations into the retrofitting process.\n",
    "        Adjust the weights (ALPHA and BETA) accordingly to balance the influence of the original word vector and the retrieved relations.\n",
    "        Normalize the new vector to ensure it retains the desired characteristics of a word vector.\n",
    "\n",
    "    Convert the updated matrix back to a dictionary:\n",
    "        Convert the matrix of word vectors (wordVecMat) back into a dictionary format, similar to the initial word vectors (wordVecs).\n",
    "        Map each word to its corresponding updated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ALPHA = 1 #coefficient minimizing the euclidean distance\n",
    "NB_ITER = 10\n",
    "\n",
    "# TODO: reduce beta value like 0.1/nb_neighbors\n",
    "# TODO: define a stopping criterion based on changes in the Eucledean distance between adjacent vertices (eg: a threshold o less than 10^-2 as mentionned in the paper)\n",
    "# TODO: test algo with French word embeddings and WOLF, since the retrofitting algo performs significantly better with French resources. If the issue persists witht he  French data, further debugging may be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Working version but no matrix involved'''\n",
    "''' Retrofit word vectors to a lexicon '''\n",
    "def retrofit(wordVecs, lexicon):\n",
    "  newWordVecs = deepcopy(wordVecs)\n",
    "  wvVocab = set(newWordVecs.keys())\n",
    "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "  for _ in range(NB_ITER):\n",
    "    # loop through every node also in ontology (else just use data estimate)\n",
    "    for word in loopVocab:\n",
    "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "      numNeighbours = len(wordNeighbours)\n",
    "      #no neighbours, pass - use data estimate\n",
    "      if numNeighbours == 0:\n",
    "        continue\n",
    "      # the weight of the data estimate if the number of neighbours\n",
    "      newVec = ALPHA * wordVecs[word] + BETA * sum(newWordVecs[ppWord] for ppWord in wordNeighbours) \n",
    "      newWordVecs[word] = newVec/ (ALPHA + BETA * numNeighbours)\n",
    "  return newWordVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectors before retrofitting:\")\n",
    "print(toy_corpus_vecs)\n",
    "\n",
    "print(\"\\nVectors after retrofitting\")\n",
    "retrofit(toy_dict_corpus, ppdb_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrofit word vectors to a lexicon using matrix operations and the file lexical similarity'''\n",
    "def retrofit_matrix_lexical_similarity(wordVecs, lexicon):\n",
    "    # Convert the word vectors dictionary to a matrix\n",
    "    wordVecMat = np.array(list(wordVecs.values()))\n",
    "    \n",
    "    # Create a set of vocabulary indices based on the shape of wordVecMat\n",
    "    wvVocab = set(range(wordVecMat.shape[0]))\n",
    "    \n",
    "    # Find the common vocabulary between wordVecMat and the lexicon\n",
    "    loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
    "    \n",
    "    # Iterate over the specified number of iterations\n",
    "    for _ in range(NB_ITER):\n",
    "        for word in loopVocab:\n",
    "            wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "            \n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "            \n",
    "            newVec = ALPHA * wordVecMat[word] + BETA * np.sum(wordVecMat[list(wordNeighbours)], axis=0)\n",
    "            newWordVec = newVec / (ALPHA + BETA * numNeighbours)\n",
    "            \n",
    "            wordVecMat[word] = newWordVec\n",
    "    \n",
    "    # Convert the matrix back to a dictionary of word vectors\n",
    "    newWordVecs = {word: vec for word, vec in zip(wordVecs.keys(), wordVecMat)}\n",
    "    \n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply retrofitting using matrix operations\n",
    "retrofittedVecMat = retrofit_matrix_lexical_similarity(wordVecs, lexical_similarity)\n",
    "\n",
    "# Convert the retrofitted matrix back to a dictionary of word vectors\n",
    "retrofittedVecs = {}\n",
    "for word, vec in zip(wordVecs.keys(), retrofittedVecMat):\n",
    "    retrofittedVecs[word] = vec\n",
    "\n",
    "# Save the retrofitted vectors to an output file\n",
    "with open(output_file, 'w', encoding='utf-8') as outputFile:\n",
    "    for word, vec in retrofittedVecs.items():\n",
    "        outputFile.write(word + ' ' + ' '.join(str(val) for val in vec) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofit_embeddings_diverse_semantic_relations(wordVecs, ppdb_lexicon, wordnet_lexicon):\n",
    "    # Convert the word vectors dictionary to a matrix\n",
    "    wordVecMat = np.array(list(wordVecs.values()))\n",
    "\n",
    "    # Create a set of vocabulary indices based on the shape of wordVecMat\n",
    "    wvVocab = set(range(wordVecMat.shape[0]))\n",
    "\n",
    "    # Find the common vocabulary between wordVecMat and the lexicons\n",
    "    loopVocab = wvVocab.intersection(set(ppdb_lexicon.keys())).intersection(set(wordnet_lexicon.keys()))\n",
    "\n",
    "    # Iterate over the specified number of iterations\n",
    "    for _ in range(NB_ITER):\n",
    "        for word in loopVocab:\n",
    "            wordNeighbours = set(ppdb_lexicon[word]).union(set(wordnet_lexicon[word]))\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "\n",
    "            # Retrieve semantic relations from PPDB and WordNet\n",
    "            ppdb_relations = ppdb_lexicon.get(word, [])\n",
    "            wordnet_relations = wordnet_lexicon.get(word, [])\n",
    "\n",
    "            # Combine the relations with the existing lexicon\n",
    "            combined_relations = list(wordNeighbours) + ppdb_relations + wordnet_relations\n",
    "\n",
    "            BETA = 1/ len(combined_relations) # minimizes each vector distance to ? \n",
    "\n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "\n",
    "            newVec = ALPHA * wordVecMat[word] + BETA * np.sum(wordVecMat[list(combined_relations)], axis=0)\n",
    "            newWordVec = newVec / (ALPHA + BETA * numNeighbours)\n",
    "\n",
    "            wordVecMat[word] = newWordVec\n",
    "\n",
    "    # Convert the matrix back to a dictionary of word vectors\n",
    "    newWordVecs = {word: vec for word, vec in zip(wordVecs.keys(), wordVecMat)}\n",
    "\n",
    "        # # Normalize the new vector\n",
    "    #     norm = np.linalg.norm(newWordVecs[word])\n",
    "    #     if norm > 0:\n",
    "    #         newWordVecs[word] /= norm\n",
    "    \n",
    "    return newWordVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use matrix operations instead of for loops?\n",
    "def retrofit_embeddings_wordnet(wordVecs, wordnet_lexicon):\n",
    "    wordVecMat = np.array(list(wordVecs.values()))\n",
    "    wvVocab = set(range(wordVecMat.shape[0]))\n",
    "    loopVocab = wvVocab.intersection(set(wordnet_lexicon.keys()))\n",
    "\n",
    "    for _ in range(NB_ITER):\n",
    "        for word in loopVocab:\n",
    "            wordNeighbours = set(wordnet_lexicon[word])\n",
    "            numNeighbours = len(wordNeighbours)\n",
    "\n",
    "            wordnet_relations = wordnet_lexicon.get(word, [])\n",
    "\n",
    "            combined_relations = list(wordNeighbours) + wordnet_relations\n",
    "\n",
    "            if numNeighbours == 0:\n",
    "                continue\n",
    "\n",
    "            newVec = ALPHA * wordVecMat[word] + BETA * np.sum(wordVecMat[list(combined_relations)], axis=0)\n",
    "            newWordVec = newVec / (ALPHA + BETA * numNeighbours)\n",
    "\n",
    "            wordVecMat[word] = newWordVec\n",
    "\n",
    "    newWordVecs = {word: vec for word, vec in zip(wordVecs.keys(), wordVecMat)}\n",
    "\n",
    "    return newWordVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatedWordVecs = retrofit_embeddings_diverse_semantic_relations(toy_corpus_vecs, ppdb_lexicon, wordnet_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrofit the word embeddings\n",
    "updatedWordVecs = retrofit_embeddings_diverse_semantic_relations(wordVecs, ppdb_lexicon, wordnet_lexicon)\n",
    "\n",
    "# # Print the updated word embeddings\n",
    "# for word, vec in updatedWordVecs.items():\n",
    "#     print(f\"{word}: {vec}\")\n",
    "# Save the retrofitted vectors to an output file\n",
    "with open(output_file, 'w', encoding='utf-8') as outputFile:\n",
    "    for word, vec in updatedWordVecs.items():\n",
    "        outputFile.write(word + ' ' + ' '.join(str(val) for val in vec) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_cosine_similarity(word_vecs):\n",
    "    num_words = len(word_vecs)\n",
    "    total_similarity = 0.0\n",
    "    pair_count = 0\n",
    "\n",
    "    for i, (word1, vec1) in enumerate(word_vecs.items()):\n",
    "        for j, (word2, vec2) in enumerate(word_vecs.items()):\n",
    "            if j > i:\n",
    "                similarity = calculate_cosine_similarity(vec1, vec2)\n",
    "                total_similarity += similarity\n",
    "                pair_count += 1\n",
    "\n",
    "    if pair_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    avg_similarity = total_similarity / pair_count\n",
    "    return avg_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = len(wordVecs[next(iter(wordVecs))])\n",
    "wordVecs = {word: np.array(vec) for word, vec in wordVecs.items() if len(vec) == dimension}\n",
    "updatedWordVecs = {word: np.array(vec) for word, vec in updatedWordVecs.items() if len(vec) == dimension}\n",
    "\n",
    "# Calculate the average cosine similarity before and after retrofitting\n",
    "similarity_before = calculate_average_cosine_similarity(wordVecs)\n",
    "similarity_after = calculate_average_cosine_similarity(updatedWordVecs)\n",
    "\n",
    "print(f\"Average cosine similarity before retrofitting: {similarity_before}\")\n",
    "print(f\"Average cosine similarity after retrofitting: {similarity_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: store scores in pd.DataFrame\n",
    "# Calculate Spearman correlation between the human scores and the model scores\n",
    "# print(ws353_data[['score', 'original_score', 'retrofitted_score']].corr(method = 'spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the retrofitted vectors with the original word vectors\n",
    "data = []\n",
    "for word in retrofittedVecs.keys():\n",
    "    originalVec = wordVecs[word]\n",
    "    retrofittedVec = retrofittedVecs[word]\n",
    "    data.append([word, originalVec, retrofittedVec])\n",
    "    \n",
    "    # Compare the vectors and print the results\n",
    "    if np.array_equal(originalVec, retrofittedVec):\n",
    "        print(f\"The vector for word '{word}' has not been updated.\")\n",
    "    else:\n",
    "        print(f\"The vector for word '{word}' has been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame with appropriate column names\n",
    "df = pd.DataFrame(data, columns=[\"Word\", \"Original Vector\", \"Retrofitted Vector\"])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use semantic resources (such as WOLF for French or PPDB/WordNet for English) to enhance the lexicon by incorporating knowledge from synonymy, hypernymy relations, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extraction of semantic relations in both languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Loading ppdb datafile '''\n",
    "# lexicon = {}\n",
    "# with open('ppdb-2.0-xl-all', 'r') as f:\n",
    "#     for line in f:\n",
    "#         fields = line.strip().split('\\t')\n",
    "#         if len(fields) == 2:\n",
    "#             lexicon[(fields[0], fields[1])] = 1.0\n",
    "#         elif len(fields) == 3:\n",
    "#             lexicon[(fields[0], fields[1])] = float(fields[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the performance of the retrofitting algorithm on two tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Lexical similarity task: Measure the improvement in capturing semantic relationships between words in the lexicon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: Define a semantic relationship task\n",
    "    Analogy Completion:\n",
    "        In this task, you evaluate the model's ability to complete analogies by identifying the missing word in a given analogy. The analogy typically follows a pattern like \"A is to B as C is to ___.\" For example, \"man is to woman as king is to ___.\" The model should be able to generate the correct word \"queen\" to complete the analogy.\n",
    "\n",
    "    Word Categorization:\n",
    "        In this task, you assess the model's ability to categorize words into predefined semantic categories. You provide a set of words belonging to different categories and evaluate how well the model can assign new words to the correct categories. For example, given the words \"car,\" \"bicycle,\" and \"train,\" the model should be able to categorize them into the transportation category.\n",
    "\n",
    "    Word Sense Disambiguation:\n",
    "        Word sense disambiguation involves determining the correct meaning or sense of a word within a given context. The task evaluates the model's capability to correctly identify the sense of a word based on its context. For example, given the sentence \"I saw a bat in the zoo,\" the model should be able to identify whether the word \"bat\" refers to the animal or the sports equipment.\n",
    "- TODO collect data? \n",
    "- TODO: calculate similarity scores for the word pairs in lexicon\n",
    "- TODO: evaluate models: compare similarity scores obtained from model with reference scores (Pearson correlation, Spearman's rank correlation)\n",
    "- TODO: analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check similarity before and after retroffit\n",
    "# ''' Cosine similarity '''\n",
    "# for i, word1 in enumerate(output_file[:-1]):\n",
    "#     print(f'Similarities with \"{word1}\":')\n",
    "#     for j, word2 in enumerate(output_file[i+1:]):\n",
    "#         similarity = model.similarity(word1, word2)\n",
    "#         print(f'  - \"{word2}\": {similarity:.2f}')\n",
    "#     print()\n",
    "\n",
    "''' Word2Vec, GloVe: These models assign a vector to each word in a high-dimensional vector space based on the context in which the word appears. The similarity between two words can then be calculated as the cosine similarity between their corresponding vectors.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Cosine similarity '''\n",
    "for i, word1 in enumerate(toy_corpus[:-1]):\n",
    "    print(f'Similarities with \"{word1}\":')\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:]):\n",
    "        similarity = model.similarity(word1, word2)\n",
    "        print(f'  - \"{word2}\": {similarity:.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Convert toy_corpus to a co-occurrence matrix '''\n",
    "# Create a dictionary from the corpus\n",
    "dictionary = corpora.Dictionary([toy_corpus])\n",
    "\n",
    "# Convert the corpus to a bag-of-words representation\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in [toy_corpus]]\n",
    "\n",
    "# Convert the corpus to a co-occurrence matrix\n",
    "cooccur_matrix = matutils.corpus2csc(bow_corpus).dot(matutils.corpus2csc(bow_corpus).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use the pre-trained Word2Vec model to create an embedding matrix '''\n",
    "embedding_matrix = {}\n",
    "for word in toy_corpus:\n",
    "    embedding = model[word]\n",
    "    embedding_matrix[dictionary.token2id[word]] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrofit the embedding matrix '''\n",
    "# Could also use the 'from retrofitting import retrofit' package\n",
    "# = an implementation of the algorithm proposed by Mrksic et al. (2017) which is faster and more scalable\n",
    "cooccur_matrix_dict = cooccur_matrix.todok()\n",
    "retrofitted_embeddings = retrofit(embedding_matrix, cooccur_matrix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Use the retrofitted embeddings to compute semantic similarity '''\n",
    "for i, word1 in enumerate(toy_corpus):\n",
    "    for j, word2 in enumerate(toy_corpus[i+1:], i+1):\n",
    "        similarity_before = cosine(embedding_matrix[i], embedding_matrix[j])\n",
    "        similarity_after = cosine(retrofitted_embeddings[i], retrofitted_embeddings[j])\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' before retrofitting: {similarity_before:.2f}\")\n",
    "        print(f\"Similarity between '{word1}' and '{word2}' after retrofitting: {similarity_after:.2f}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentiment analysis task: Apply the retrofitted word vectors to a corpus of film reviews and assess if they lead to better sentiment analysis performance compared to the original word vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
