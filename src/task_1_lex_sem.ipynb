{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise dictionary that will contain the word embeddings\n",
    "#key=word\n",
    "#values=array of numerical representation of the corresponding word (vector)\n",
    "word_embeddings = {}\n",
    "#open pre-trained word embeddings stored in file\n",
    "with open(\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\", 'r') as file:\n",
    "    #for each line in the file\n",
    "    for line in file:\n",
    "        #split the line into word and vector\n",
    "        values = line.split()\n",
    "        #get the first element which is the word\n",
    "        word = values[0]\n",
    "        #get the remainder of the values which will form the vector represenation of the word in a matrix\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        #assign key=word and values=vector\n",
    "        word_embeddings[word] = vector\n",
    "\n",
    "#Access word vectors\n",
    "#print(word_embeddings['talk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of word pairs from the lexical similarity file\n",
    "word_pairs = []\n",
    "#list of human scores for each word pair\n",
    "human_scores = []\n",
    "#open lexical_similarity file\n",
    "with open (\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/English/evaluations/lexical similarity/ws353_lexical_similarity.txt\", 'r') as file:\n",
    "    #for every line in the file\n",
    "    for line in file:\n",
    "        #split the line by space\n",
    "        #one line contains the word pair and its score\n",
    "        w1, w2, score = line.split()\n",
    "        #add the words to word_pairs\n",
    "        word_pairs.append((w1, w2))\n",
    "        #add the ratings to human_score\n",
    "        human_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('love', 'sex')\n",
      "6.77\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs[0])\n",
    "print(human_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of cosine similarities\n",
    "scores = []\n",
    "\n",
    "#for every word pair\n",
    "for w1, w1 in word_pairs:\n",
    "    #check if the words exist in the word_embeddings\n",
    "    if w1 in word_embeddings and w2 in word_embeddings:\n",
    "        #retrieve word1's vector \n",
    "        vec1 = word_embeddings[w1]\n",
    "        #retreive word2's vector\n",
    "        vec2 = word_embeddings[w2]\n",
    "        #calculate cosine between the two vectors\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        #add the result to scores\n",
    "        scores.append(similarity)\n",
    "    #otherwise if words don't exist in the word embeddings\n",
    "    else:\n",
    "        #assign default value of 0.0\n",
    "        scores.append(0.00)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.77', '7.35', '10.00', '7.46', '7.62', '7.58', '5.77', '6.31', '7.50', '6.77', '7.42', '6.85', '6.19', '5.92', '7.00', '6.62', '6.81', '4.62', '5.81', '7.08', '8.08', '1.62', '1.31', '0.92', '1.81', '6.69', '3.73', '0.92', '7.46', '8.12', '7.73', '9.15', '0.31', '0.23', '8.58', '5.92', '6.69', '8.46', '7.65', '1.62', '9.44', '8.62', '9.03', '6.81', '6.63', '7.56', '6.73', '7.65', '2.50', '8.38', '7.38', '6.19', '6.73', '7.92', '8.12', '7.35', '4.88', '5.54', '8.46', '8.13', '3.04', '1.31', '5.96', '6.87', '7.85', '2.65', '8.94', '8.96', '9.29', '8.83', '9.10', '8.87', '9.02', '9.29', '8.79', '7.52', '7.10', '7.38', '6.46', '6.27', '2.69', '4.46', '5.85', '5.00', '2.08', '4.42', '4.38', '1.85', '3.08', '0.92', '3.15', '0.92', '0.54', '2.08', '0.54', '0.62', '8.42', '9.08', '9.04', '8.27', '7.57', '7.29', '8.50', '7.73', '6.88', '5.65', '3.31', '8.00', '8.00', '7.08', '6.85', '7.00', '4.77', '5.62', '5.87', '8.08', '7.00', '6.85', '7.42', '6.58', '6.42', '8.21', '7.69', '7.23', '6.71', '5.58', '7.48', '8.45', '8.06', '8.08', '8.02', '8.11', '7.92', '7.94', '5.85', '3.85', '2.81', '6.65', '2.50', '1.77', '6.04', '6.58', '6.85', '2.40', '2.92', '3.69', '2.15', '7.25', '5.00', '1.92', '5.90', '7.42', '7.27', '1.81', '5.06', '5.09', '6.78', '6.06', '6.94', '8.31', '4.59', '2.94', '5.63', '8.16', '7.53', '4.56', '6.34', '6.56', '2.38', '2.22', '8.66', '4.47', '5.34', '3.69', '3.00', '8.13', '6.31', '6.22', '6.50', '3.91', '2.56', '3.00', '5.63', '7.59', '3.16', '1.19', '3.31', '6.63', '4.75', '3.69', '4.25', '6.56', '4.25', '5.88', '5.94', '7.56', '2.75', '7.03', '5.47', '6.47', '7.91', '4.97', '5.00', '5.19', '7.03', '3.44', '2.31', '5.91', '7.38', '8.13', '4.63', '5.25', '5.03', '6.69', '7.88', '4.50', '4.75', '4.47', '3.25', '5.63', '3.69', '2.94', '5.28', '5.00', '6.44', '4.13', '4.75', '2.38', '4.94', '8.06', '5.31', '8.03', '5.94', '6.00', '5.41', '1.81', '8.97', '6.00', '6.72', '8.00', '4.81', '3.88', '5.16', '2.25', '6.44', '8.88', '6.88', '4.94', '2.56', '6.38', '7.81', '1.75', '4.25', '3.88', '2.88', '7.63', '6.48', '8.44', '7.50', '8.59', '6.34', '3.38', '6.00', '3.88', '7.63', '7.78', '9.22', '7.38', '6.09', '8.50', '8.31', '7.13', '5.91', '6.47', '3.38', '3.63', '7.13', '7.89', '5.97', '7.03', '7.69', '7.47', '6.19', '6.97', '3.56', '7.47', '8.34', '8.70', '7.81', '5.70', '6.22', '6.34', '4.06', '4.47', '5.97', '7.61', '8.36', '7.41', '2.69', '3.94', '7.16', '5.63', '7.53', '8.31', '8.81', '6.25', '8.30', '5.25', '8.53', '7.94', '6.88', '5.94', '4.06', '6.25', '7.72', '6.19', '2.97', '1.94', '3.75', '3.31', '3.69', '7.44', '6.41', '5.44', '6.25', '2.63', '0.88', '3.19', '4.69', '6.75', '5.31', '7.31', '5.75', '3.97', '3.47', '3.63', '5.56', '7.83', '3.88', '5.31', '6.81', '7.59', '7.19', '4.38', '6.53', '6.19', '7.69', '6.31', '6.03', '8.34', '6.25', '6.34', '3.78']\n",
      "[0.12286590039730072, 0.14685171842575073, 0.10362786054611206, 0.1489059180021286, 0.11427366733551025, 0.2051575928926468, 0.16196264326572418, 0.16196264326572418, 0.09789814054965973, 0.1844450682401657, 0.1844450682401657, 0.08277405798435211, 0.11397483199834824, 0.12728284299373627, 0.11135360598564148, 0.2137647271156311, 0.18800219893455505, 0.10223374515771866, 0.05961749330163002, 0.12448738515377045, 0.29308396577835083, 0.07042407989501953, 0.0, 0.09308329224586487, 0.10630564391613007, 0.10630564391613007, 0.15137329697608948, 0.29416683316230774, 0.1968262940645218, 0.11958006769418716, 0.14701680839061737, 0.07328396290540695, 0.06884782761335373, 0.07236698269844055, 0.21698139607906342, 0.06596391648054123, 0.17853273451328278, 0.0, 0.0, 0.12286590039730072, 0.12286590039730072, 0.1950647085905075, 0.1182718500494957, 0.14520733058452606, 0.15577903389930725, 0.06430214643478394, 0.19124479591846466, 0.12499654293060303, 0.0, 0.18977271020412445, 0.0764051228761673, 0.03649400919675827, 0.17907370626926422, 0.2153313308954239, 0.09165040403604507, 0.12424599379301071, 0.12424599379301071, 0.12424599379301071, 0.1565316617488861, 0.07848335802555084, 0.16196264326572418, 0.12841810286045074, 0.11158376932144165, 0.06333911418914795, 0.2074456512928009, 0.2074456512928009, 0.22595222294330597, 0.14023832976818085, 0.17683887481689453, 0.10425569117069244, 0.17684492468833923, 0.13086320459842682, 0.09107153117656708, 0.13987861573696136, 0.1730838268995285, 0.13113659620285034, 0.16487713158130646, 0.11642862856388092, 0.1064966544508934, 0.3386215567588806, 0.1064966544508934, 0.22263655066490173, 0.16196264326572418, 0.06059584766626358, 0.186715766787529, 0.1489267349243164, 0.23536819219589233, 0.2721285820007324, 0.186715766787529, 0.29448994994163513, 0.14701680839061737, 0.09107153117656708, 0.059570033103227615, 0.1907297968864441, 0.09281110763549805, 0.17683887481689453, 0.22327320277690887, 0.07328396290540695, 0.22120901942253113, 0.2430436909198761, 0.2350596785545349, 0.2601873576641083, 0.17581750452518463, 0.11173953860998154, 0.09661639481782913, 0.07457850128412247, 0.22500427067279816, 0.09308329224586487, 0.11615543812513351, 0.06808975338935852, 0.18634767830371857, 0.19933189451694489, 0.09618031978607178, 0.17986169457435608, 0.15850518643856049, 0.12478861957788467, 0.06526609510183334, 0.10473471134901047, 0.143249973654747, 0.12163636088371277, 0.2137647271156311, 0.0, 0.1516469419002533, 0.12734150886535645, 0.2266589105129242, 0.10991739481687546, 0.08903580158948898, 0.0764051228761673, 0.09068108350038528, 0.16817936301231384, 0.11202693730592728, 0.019686290994286537, 0.15804371237754822, 0.30125826597213745, 0.23802030086517334, 0.1043151393532753, 0.08903580158948898, 0.18625524640083313, 0.24458248913288116, 0.20598764717578888, 0.28787562251091003, 0.14124980568885803, 0.19189240038394928, 0.08605745434761047, 0.19133953750133514, 0.13958126306533813, 0.15680278837680817, 0.11147256940603256, 0.14327368140220642, 0.10306204855442047, 0.08313044160604477, 0.14685171842575073, 0.16196264326572418, 0.16423127055168152, 0.16830095648765564, 0.10438861697912216, 0.10438861697912216, 0.14611618220806122, 0.058005888015031815, 0.08910959959030151, 0.15185119211673737, 0.2180110663175583, 0.19416941702365875, 0.10744471848011017, 0.2953261137008667, 0.06131771206855774, 0.15804371237754822, 0.14505146443843842, 0.13646544516086578, 0.15080970525741577, 0.14439179003238678, 0.15080970525741577, 0.06131771206855774, 0.2953261137008667, 0.15328815579414368, 0.21729551255702972, 0.23570005595684052, 0.15169622004032135, 0.0, 0.32862868905067444, 0.1393069475889206, 0.16158221662044525, 0.07893414795398712, 0.30057215690612793, 0.21864303946495056, 0.14104874432086945, 0.15080970525741577, 0.18363909423351288, 0.15290096402168274, 0.19124479591846466, 0.19124479591846466, 0.27832114696502686, 0.11840294301509857, 0.09661639481782913, 0.27832114696502686, 0.15080970525741577, 0.15185119211673737, 0.1887643188238144, 0.22957396507263184, 0.075465627014637, 0.09002022445201874, 0.2768966257572174, 0.16741523146629333, 0.07530128210783005, 0.13665126264095306, 0.16441595554351807, 0.08522065728902817, 0.08629342168569565, 0.0842757448554039, 0.0, 0.137844055891037, 0.12958277761936188, 0.05590244010090828, 0.20277591049671173, 0.22957396507263184, 0.25958219170570374, 0.11906235665082932, 0.05042511597275734, 0.06883180886507034, 0.3219151794910431, 0.1583806425333023, 0.13665126264095306, 0.14439179003238678, 0.18434466421604156, 0.10195276141166687, 0.11368195712566376, 0.12715408205986023, 0.08199140429496765, 0.13532860577106476, 0.07101164013147354, 0.1043151393532753, 0.10429920256137848, 0.11958006769418716, 0.13192607462406158, 0.13531330227851868, 0.24843481183052063, 0.17760612070560455, 0.24843481183052063, 0.19005264341831207, 0.12100452929735184, 0.09731307625770569, 0.2073725014925003, 0.20788584649562836, 0.13222426176071167, 0.26539409160614014, 0.28613144159317017, 0.13913246989250183, 0.13619841635227203, 0.08106919378042221, 0.25493890047073364, 0.07042407989501953, 0.05165409296751022, 0.1615563929080963, 0.2039363980293274, 0.12191372364759445, 0.1243019625544548, 0.11749694496393204, 0.09921734035015106, 0.29308396577835083, 0.1790529489517212, 0.12448738515377045, 0.1893058717250824, 0.06124697998166084, 0.06124697998166084, 0.07738901674747467, 0.13455097377300262, 0.13034038245677948, 0.16579735279083252, 0.07738901674747467, 0.16615818440914154, 0.08740999549627304, 0.12557455897331238, 0.19376257061958313, 0.16196264326572418, 0.20900243520736694, 0.12191372364759445, 0.08805645257234573, 0.2180110663175583, 0.19871565699577332, 0.1877145767211914, 0.15877406299114227, 0.1630617380142212, 0.26794227957725525, 0.15617506206035614, 0.26794227957725525, 0.18384869396686554, 0.14327368140220642, 0.08333329856395721, 0.14327368140220642, 0.21254689991474152, 0.11223442852497101, 0.0537019781768322, 0.30057215690612793, 0.13456714153289795, 0.16047737002372742, 0.16047737002372742, 0.09328067302703857, 0.08997321873903275, 0.13665126264095306, 0.20349551737308502, 0.1811380386352539, 0.18222038447856903, 0.17216874659061432, 0.150686576962471, 0.08577119559049606, 0.1643548458814621, 0.22258903086185455, 0.18622148036956787, 0.08662590384483337, 0.21873314678668976, 0.10039527714252472, 0.1275426745414734, 0.29416683316230774, 0.07225985080003738, 0.25301793217658997, 0.2009565383195877, 0.19250226020812988, 0.11621283739805222, 0.21856854856014252, 0.29754355549812317, 0.2369232326745987, 0.0, 0.10577807575464249, 0.10893546044826508, 0.09612354636192322, 0.3005730211734772, 0.12959729135036469, 0.22724902629852295, 0.31197360157966614, 0.12108563631772995, 0.26581698656082153, 0.1368543952703476, 0.22556617856025696, 0.1821828931570053, 0.20413969457149506, 0.19136637449264526, 0.1987232267856598, 0.1987232267856598, 0.1299634575843811, 0.12766264379024506, 0.23045068979263306, 0.17134694755077362, 0.2153313308954239, 0.3019464612007141, 0.2562735378742218, 0.2562735378742218, -0.00043445464689284563, 0.13834476470947266, 0.18230073153972626, 0.07082236558198929, 0.26485970616340637, 0.1850692480802536, 1]\n"
     ]
    }
   ],
   "source": [
    "print(human_scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient:  -0.078\n"
     ]
    }
   ],
   "source": [
    "#calculate the spearman rank correlation\n",
    "spearman_correlation, _ = spearmanr(human_scores, scores)\n",
    "print(\"Correlation coefficient: \", round(spearman_correlation,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
