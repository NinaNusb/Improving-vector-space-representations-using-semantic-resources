{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load non-retrofitted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENGLISH\n",
    "\n",
    "#open pre_trained english embeddings\n",
    "file_non_ret_eng = open(\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\", 'r')\n",
    "#initialise dictionary that will contain the pre-trained word embeddings\n",
    "#key=word\n",
    "#values=array of numerical representation of the corresponding word (vector)\n",
    "non_ret_eng = {}\n",
    "#open pre-trained word embeddings stored in file\n",
    "#for each line in the file\n",
    "for line in file_non_ret_eng:\n",
    "    #split the line into word and vector\n",
    "    values = line.split()\n",
    "    #get the first element which is the word\n",
    "    word = values[0]\n",
    "    #get the remainder of the values which will form the vector represenation of the word in a matrix\n",
    "    vector = np.array(values[1:], dtype='float32')\n",
    "    #assign key=word and values=vector\n",
    "    non_ret_eng[word] = vector\n",
    "\n",
    "#Access word vectors\n",
    "#print(word_embeddings['talk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#open pre-trained french embeddings\n",
    "file_non_ret_fre_50 = open(\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/French/word_embeddings/vecs50-linear-frwiki\", 'r')\n",
    "\n",
    "#initialise dictionary that will contain the word embeddings\n",
    "#key=word\n",
    "#values=array of numerical representation of the corresponding word (vector)\n",
    "non_ret_fre_50 = {}\n",
    "#for each line in the file\n",
    "for line in file_non_ret_fre_50:\n",
    "    #split the line into word and vector\n",
    "    values = line.split()\n",
    "    #get the first element which is the word\n",
    "    word = values[0]\n",
    "    #get the remainder of the values which will form the vector represenation of the word in a matrix\n",
    "    vector = np.array(values[1:], dtype='float32')\n",
    "    #assign key=word and values=vector\n",
    "    non_ret_fre_50[word] = vector\n",
    "\n",
    "#Access word vectors\n",
    "#print(non_ret_fre['depuis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#open pre-trained french embeddings\n",
    "file_non_ret_fre_100 = open(\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/French/word_embeddings/vecs100-linear-frwiki\", 'r')\n",
    "\n",
    "#initialise dictionary that will contain the word embeddings\n",
    "#key=word\n",
    "#values=array of numerical representation of the corresponding word (vector)\n",
    "non_ret_fre_100 = {}\n",
    "#for each line in the file\n",
    "for line in file_non_ret_fre_100:\n",
    "    #split the line into word and vector\n",
    "    values = line.split()\n",
    "    #get the first element which is the word\n",
    "    word = values[0]\n",
    "    #get the remainder of the values which will form the vector represenation of the word in a matrix\n",
    "    vector = np.array(values[1:], dtype='float32')\n",
    "    #assign key=word and values=vector\n",
    "    non_ret_fre_100[word] = vector\n",
    "\n",
    "#Access word vectors\n",
    "#print(non_ret_fre['depuis'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load retrofitted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#open pre_trained english embeddings\\nfile_ret_eng = open(\"path\", \\'r\\')\\n#initialise dictionary that will contain the pre-trained word embeddings\\n#key=word\\n#values=array of numerical representation of the corresponding word (vector)\\nret_eng = {}\\n#open pre-trained word embeddings stored in file\\n#for each line in the file\\nfor line in file_ret_eng:\\n    #split the line into word and vector\\n    values = line.split()\\n    #get the first element which is the word\\n    word = values[0]\\n    #get the remainder of the values which will form the vector represenation of the word in a matrix\\n    vector = np.array(values[1:], dtype=\\'float32\\')\\n    #assign key=word and values=vector\\n    ret_eng[word] = vector\\n\\n#Access word vectors\\n#print(word_embeddings[\\'talk\\'])\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ENGLISH\n",
    "\n",
    "#open pre_trained english embeddings\n",
    "file_ret_eng = open(\"path\", 'r')\n",
    "#initialise dictionary that will contain the pre-trained word embeddings\n",
    "#key=word\n",
    "#values=array of numerical representation of the corresponding word (vector)\n",
    "ret_eng = {}\n",
    "#open pre-trained word embeddings stored in file\n",
    "#for each line in the file\n",
    "for line in file_ret_eng:\n",
    "    #split the line into word and vector\n",
    "    values = line.split()\n",
    "    #get the first element which is the word\n",
    "    word = values[0]\n",
    "    #get the remainder of the values which will form the vector represenation of the word in a matrix\n",
    "    vector = np.array(values[1:], dtype='float32')\n",
    "    #assign key=word and values=vector\n",
    "    ret_eng[word] = vector\n",
    "\n",
    "#Access word vectors\n",
    "#print(word_embeddings['talk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#open pre-trained french embeddings\n",
    "file_ret_fre = open(\"path\", 'r')\n",
    "\n",
    "#initialise dictionary that will contain the word embeddings\n",
    "#key=word\n",
    "#values=array of numerical representation of the corresponding word (vector)\n",
    "ret_fre = {}\n",
    "#for each line in the file\n",
    "for line in file_ret_fre:\n",
    "    #split the line into word and vector\n",
    "    values = line.split()\n",
    "    #get the first element which is the word\n",
    "    word = values[0]\n",
    "    #get the remainder of the values which will form the vector represenation of the word in a matrix\n",
    "    vector = np.array(values[1:], dtype='float32')\n",
    "    #assign key=word and values=vector\n",
    "    ret_fre[word] = vector\n",
    "\n",
    "#Access word vectors\n",
    "#print(non_ret_fre['depuis'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load lexical similarity files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENGLISH\n",
    "\n",
    "#open english lexical_similarity file\n",
    "lex_sim_eng = open (\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/English/evaluations/lexical similarity/ws353_lexical_similarity.txt\", 'r')\n",
    "#list of word pairs from the lexical similarity file\n",
    "word_pairs_eng = []\n",
    "#list of human scores for each word pair\n",
    "hum_score_eng = []\n",
    "\n",
    "#for every line in the file\n",
    "for line in lex_sim_eng:\n",
    "    #split the line by space\n",
    "    #one line contains the word pair and its score\n",
    "    w1, w2, score = line.split()\n",
    "    #add the words to word_pairs\n",
    "    word_pairs_eng.append((w1, w2))\n",
    "    #add the ratings to human_score\n",
    "    hum_score_eng.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('love', 'sex')\n",
      "6.77\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs_eng[0])\n",
    "print(hum_score_eng[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#open lexical_similarity file\n",
    "lex_sim_fre = open (\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/French/lexicon/rg65_french.txt\", 'r')\n",
    "#list of word pairs from the lexical similarity file\n",
    "word_pairs_fre = []\n",
    "#list of human scores for each word pair\n",
    "hum_score_fre = []\n",
    "#open lexical_similarity file\n",
    "\n",
    "#for every line in the file\n",
    "for line in lex_sim_fre:\n",
    "    #split the line by space\n",
    "    #one line contains the word pair and its score\n",
    "    w1, w2, score = line.split()\n",
    "    #add the words to word_pairs\n",
    "    word_pairs_fre.append((w1, w2))\n",
    "    #add the ratings to human_score\n",
    "    hum_score_fre.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('corde', 'sourire')\n",
      "0.00\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs_fre[0])\n",
    "print(hum_score_fre[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute cosine similarity for non-retrofitted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENGLISH\n",
    "\n",
    "#list of cosine similarities between non-retrofitted vectors\n",
    "cos_non_ret_eng = []\n",
    "\n",
    "#for every word pair\n",
    "for w1, w2 in word_pairs_eng:\n",
    "    #check if the words exist in the word_embeddings\n",
    "    if w1 in non_ret_eng and w2 in non_ret_eng:\n",
    "        #retrieve word1's vector \n",
    "        vec1 = non_ret_eng[w1]\n",
    "        #retreive word2's vector\n",
    "        vec2 = non_ret_eng[w2]\n",
    "        #calculate cosine between the two vectors\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        #add the result to scores\n",
    "        cos_non_ret_eng.append(round(similarity, 2))\n",
    "    #otherwise if words don't exist in the word embeddings\n",
    "    else:\n",
    "        #assign default value of 0.0\n",
    "        cos_non_ret_eng.append(0.00)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#list of cosine similarities between non-retrofitted vectors\n",
    "cos_non_ret_fre_50 = []\n",
    "\n",
    "#for every word pair\n",
    "for w1, w2 in word_pairs_fre:\n",
    "    #check if the words exist in the word_embeddings\n",
    "    if w1 in non_ret_fre_50 and w2 in non_ret_fre_50:\n",
    "        #retrieve word1's vector \n",
    "        vec1 = non_ret_fre_50[w1]\n",
    "        #retreive word2's vector\n",
    "        vec2 = non_ret_fre_50[w2]\n",
    "        #calculate cosine between the two vectors\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        #add the result to scores\n",
    "        cos_non_ret_fre_50.append(round(similarity, 2))\n",
    "    #otherwise if words don't exist in the word embeddings\n",
    "    else:\n",
    "        #assign default value of 0.0\n",
    "        cos_non_ret_fre_50.append(0.00)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#list of cosine similarities between non-retrofitted vectors\n",
    "cos_non_ret_fre_100 = []\n",
    "\n",
    "#for every word pair\n",
    "for w1, w2 in word_pairs_fre:\n",
    "    #check if the words exist in the word_embeddings\n",
    "    if w1 in non_ret_fre_100 and w2 in non_ret_fre_100:\n",
    "        #retrieve word1's vector \n",
    "        vec1 = non_ret_fre_100[w1]\n",
    "        #retreive word2's vector\n",
    "        vec2 = non_ret_fre_100[w2]\n",
    "        #calculate cosine between the two vectors\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        #add the result to scores\n",
    "        cos_non_ret_fre_100.append(round(similarity, 2))\n",
    "    #otherwise if words don't exist in the word embeddings\n",
    "    else:\n",
    "        #assign default value of 0.0\n",
    "        cos_non_ret_fre_100.append(0.00)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6.77', '7.35', '10.00', '7.46', '7.62', '7.58', '5.77', '6.31', '7.50', '6.77', '7.42', '6.85', '6.19', '5.92', '7.00', '6.62', '6.81', '4.62', '5.81', '7.08', '8.08', '1.62', '1.31', '0.92', '1.81', '6.69', '3.73', '0.92', '7.46', '8.12', '7.73', '9.15', '0.31', '0.23', '8.58', '5.92', '6.69', '8.46', '7.65', '1.62', '9.44', '8.62', '9.03', '6.81', '6.63', '7.56', '6.73', '7.65', '2.50', '8.38', '7.38', '6.19', '6.73', '7.92', '8.12', '7.35', '4.88', '5.54', '8.46', '8.13', '3.04', '1.31', '5.96', '6.87', '7.85', '2.65', '8.94', '8.96', '9.29', '8.83', '9.10', '8.87', '9.02', '9.29', '8.79', '7.52', '7.10', '7.38', '6.46', '6.27', '2.69', '4.46', '5.85', '5.00', '2.08', '4.42', '4.38', '1.85', '3.08', '0.92', '3.15', '0.92', '0.54', '2.08', '0.54', '0.62', '8.42', '9.08', '9.04', '8.27', '7.57', '7.29', '8.50', '7.73', '6.88', '5.65', '3.31', '8.00', '8.00', '7.08', '6.85', '7.00', '4.77', '5.62', '5.87', '8.08', '7.00', '6.85', '7.42', '6.58', '6.42', '8.21', '7.69', '7.23', '6.71', '5.58', '7.48', '8.45', '8.06', '8.08', '8.02', '8.11', '7.92', '7.94', '5.85', '3.85', '2.81', '6.65', '2.50', '1.77', '6.04', '6.58', '6.85', '2.40', '2.92', '3.69', '2.15', '7.25', '5.00', '1.92', '5.90', '7.42', '7.27', '1.81', '5.06', '5.09', '6.78', '6.06', '6.94', '8.31', '4.59', '2.94', '5.63', '8.16', '7.53', '4.56', '6.34', '6.56', '2.38', '2.22', '8.66', '4.47', '5.34', '3.69', '3.00', '8.13', '6.31', '6.22', '6.50', '3.91', '2.56', '3.00', '5.63', '7.59', '3.16', '1.19', '3.31', '6.63', '4.75', '3.69', '4.25', '6.56', '4.25', '5.88', '5.94', '7.56', '2.75', '7.03', '5.47', '6.47', '7.91', '4.97', '5.00', '5.19', '7.03', '3.44', '2.31', '5.91', '7.38', '8.13', '4.63', '5.25', '5.03', '6.69', '7.88', '4.50', '4.75', '4.47', '3.25', '5.63', '3.69', '2.94', '5.28', '5.00', '6.44', '4.13', '4.75', '2.38', '4.94', '8.06', '5.31', '8.03', '5.94', '6.00', '5.41', '1.81', '8.97', '6.00', '6.72', '8.00', '4.81', '3.88', '5.16', '2.25', '6.44', '8.88', '6.88', '4.94', '2.56', '6.38', '7.81', '1.75', '4.25', '3.88', '2.88', '7.63', '6.48', '8.44', '7.50', '8.59', '6.34', '3.38', '6.00', '3.88', '7.63', '7.78', '9.22', '7.38', '6.09', '8.50', '8.31', '7.13', '5.91', '6.47', '3.38', '3.63', '7.13', '7.89', '5.97', '7.03', '7.69', '7.47', '6.19', '6.97', '3.56', '7.47', '8.34', '8.70', '7.81', '5.70', '6.22', '6.34', '4.06', '4.47', '5.97', '7.61', '8.36', '7.41', '2.69', '3.94', '7.16', '5.63', '7.53', '8.31', '8.81', '6.25', '8.30', '5.25', '8.53', '7.94', '6.88', '5.94', '4.06', '6.25', '7.72', '6.19', '2.97', '1.94', '3.75', '3.31', '3.69', '7.44', '6.41', '5.44', '6.25', '2.63', '0.88', '3.19', '4.69', '6.75', '5.31', '7.31', '5.75', '3.97', '3.47', '3.63', '5.56', '7.83', '3.88', '5.31', '6.81', '7.59', '7.19', '4.38', '6.53', '6.19', '7.69', '6.31', '6.03', '8.34', '6.25', '6.34', '3.78']\n",
      "[0.32, 0.45, 1, 0.57, 0.49, 0.5, 0.41, 0.5, 0.37, 0.65, 0.32, 0.43, 0.81, 0.69, 0.57, 0.51, 0.46, 0.29, 0.4, 0.46, 0.54, 0.25, 0.0, 0.25, 0.17, 0.29, 0.19, 0.16, 0.38, 0.38, 0.45, 0.75, 0.1, 0.17, 0.69, 0.29, 0.44, 0.0, 0.0, 0.1, 0.26, 0.0, 0.73, 0.74, 0.5, 0.31, 0.0, 0.0, 0.0, 0.55, 0.39, 0.45, 0.33, 0.46, 0.37, 0.83, 0.23, 0.29, 0.57, 0.66, 0.35, 0.22, 0.3, 0.65, 0.61, 0.28, 0.65, 0.67, 0.51, 0.58, 0.6, 0.31, 0.47, 0.8, 0.67, 0.53, 0.47, 0.5, 0.44, 0.33, 0.14, 0.29, 0.24, 0.23, 0.4, 0.25, 0.31, 0.25, 0.38, 0.25, 0.4, 0.22, 0.24, 0.27, 0.12, 0.09, 0.47, 0.75, 0.46, 0.5, 0.36, 0.27, 0.38, 0.39, 0.18, 0.49, 0.13, 0.52, 0.4, 0.34, 0.31, 0.34, 0.17, 0.22, 0.45, 0.77, 0.4, 0.2, 0.26, 0.44, 0.43, 0.0, 0.36, 0.5, 0.67, 0.33, 0.68, 0.52, 0.5, 0.57, 0.46, 0.57, 0.41, 0.35, 0.35, 0.19, 0.08, 0.37, 0.14, 0.1, 0.29, 0.21, 0.17, 0.11, 0.11, 0.07, 0.14, 0.2, 0.08, 0.06, 0.16, 0.32, 0.47, 0.15, 0.51, 0.37, 0.42, 0.38, 0.0, 0.0, 0.32, 0.0, 0.0, 0.43, 0.31, 0.2, 0.23, 0.54, 0.17, 0.0, 0.72, 0.23, 0.17, 0.31, 0.32, 0.42, 0.3, 0.53, 0.0, 0.25, 0.31, 0.16, 0.51, 0.3, 0.22, 0.09, 0.15, 0.42, 0.28, 0.24, 0.2, 0.4, 0.36, 0.43, 0.25, 0.41, 0.29, 0.32, 0.23, 0.25, 0.44, 0.36, 0.23, 0.14, 0.58, 0.37, 0.14, 0.42, 0.4, 0.0, 0.23, 0.25, 0.34, 0.19, 0.39, 0.21, 0.31, 0.37, 0.14, 0.0, 0.22, 0.18, 0.3, 0.25, 0.3, 0.19, 0.44, 0.11, 0.31, 0.47, 0.32, 0.33, 0.32, 0.22, 0.38, 0.18, 0.52, 0.24, 0.51, 0.47, 0.37, 0.33, 0.25, 0.23, 0.28, 0.79, 0.33, 0.22, 0.12, 0.4, 0.43, 0.09, 0.58, 0.18, 0.26, 0.54, 0.35, 0.69, 0.45, 0.0, 0.32, 0.25, 0.26, 0.05, 0.2, 0.5, 0.36, 0.34, 0.12, 0.65, 0.32, 0.31, 0.24, 0.42, 0.44, 0.05, 0.18, 0.55, 0.41, 0.32, 0.36, 0.55, 0.42, 0.35, 0.42, 0.33, 0.7, 0.7, 0.48, 0.42, 0.41, 0.14, 0.34, 0.31, 0.29, 0.19, 0.74, 0.29, 0.14, 0.37, 0.26, 0.19, 0.38, 0.53, 0.58, 0.29, 0.67, 0.23, 0.71, 0.56, 0.35, 0.29, 0.37, 0.24, 0.57, 0.44, 0.2, 0.15, 0.23, 0.31, 0.39, 0.0, 0.59, 0.31, 0.15, 0.2, 0.1, 0.35, 0.36, 0.31, 0.2, 0.39, 0.16, 0.17, 0.27, 0.31, 0.48, 0.58, 0.32, 0.39, 0.49, 0.54, 0.47, 0.25, 0.16, 0.13, 0.42, 0.49, 0.25, 0.64, 0.26, 0.42, 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(hum_score_eng)\n",
    "print(cos_non_ret_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.06, 0.11, 0.0, 0.0, 0.06, 0.0, 0.0, 0.22, 0.22, 0.44, 0.06, 0.17, 0.17, 0.5, 0.22, 0.11, 0.33, 0.39, 0.29, 0.06, 0.17, 0.44, 0.17, 0.61, 0.11, 0.28, 0.06, 2.17, 0.56, 0.28, 0.44, 0.33, 0.22, 0.56, 0.56, 0.94, 2.0, 0.83, 1.28, 1.65, 2.41, 2.78, 2.89, 3.28, 2.78, 2.67, 2.94, 3.33, 3.39, 1.5, 1.89, 2.59, 3.56, 2.5, 3.72, 3.0, 4.0, 3.83, 3.0, 4.0, 3.94, 3.22, 2.17]\n",
      "[0.48, 0.42, 0.31, 0.28, 0.24, 0.19, 0.44, 0.36, 0.34, 0.53, 0.0, 0.59, 0.45, 0.63, 0.68, 0.67, 0.64, 0.51, 0.57, 0.63, 0.7, 0.36, 0.54, 0.59, 0.53, 0.34, 0.48, 0.31, 0.34, 0.71, 0.32, 0.5, 0.55, 0.2, 0.55, 0.77, 0.6, 0.41, 0.36, 0.75, 0.6, 0.49, 0.62, 0.52, 0.63, 0.63, 0.43, 0.92, 0.57, 0.87, 0.0, 0.73, 0.76, 0.9, 0.38, 0.76, 0.59, 0.74, 1, 0.77, 0.66, 1, 0.65, 0.81, 0.55]\n"
     ]
    }
   ],
   "source": [
    "print(hum_score_fre)\n",
    "print(cos_non_ret_fre_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.06, 0.11, 0.0, 0.0, 0.06, 0.0, 0.0, 0.22, 0.22, 0.44, 0.06, 0.17, 0.17, 0.5, 0.22, 0.11, 0.33, 0.39, 0.29, 0.06, 0.17, 0.44, 0.17, 0.61, 0.11, 0.28, 0.06, 2.17, 0.56, 0.28, 0.44, 0.33, 0.22, 0.56, 0.56, 0.94, 2.0, 0.83, 1.28, 1.65, 2.41, 2.78, 2.89, 3.28, 2.78, 2.67, 2.94, 3.33, 3.39, 1.5, 1.89, 2.59, 3.56, 2.5, 3.72, 3.0, 4.0, 3.83, 3.0, 4.0, 3.94, 3.22, 2.17]\n",
      "[0.4, 0.31, 0.22, 0.23, 0.2, 0.2, 0.37, 0.25, 0.12, 0.41, 0.0, 0.52, 0.39, 0.49, 0.47, 0.51, 0.52, 0.45, 0.45, 0.49, 0.63, 0.33, 0.36, 0.5, 0.46, 0.27, 0.42, 0.27, 0.25, 0.6, 0.21, 0.44, 0.42, 0.2, 0.41, 0.66, 0.48, 0.37, 0.31, 0.67, 0.57, 0.47, 0.55, 0.37, 0.48, 0.62, 0.36, 0.89, 0.49, 0.81, 0.0, 0.66, 0.65, 0.87, 0.47, 0.69, 0.54, 0.67, 1, 0.69, 0.6, 1, 0.57, 0.75, 0.46]\n"
     ]
    }
   ],
   "source": [
    "print(hum_score_fre)\n",
    "print(cos_non_ret_fre_100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute cosine similarity for retrofitted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#ENGLISH\\n\\n#list of cosine similarities between non-retrofitted vectors\\ncos_ret_eng = []\\n\\n#for every word pair\\nfor w1, w2 in word_pairs_eng:\\n    #check if the words exist in the word_embeddings\\n    if w1 in ret_eng and w2 in ret_eng:\\n        #retrieve word1's vector \\n        vec1 = ret_eng[w1]\\n        #retreive word2's vector\\n        vec2 = ret_eng[w2]\\n        #calculate cosine between the two vectors\\n        similarity = 1 - cosine(vec1, vec2)\\n        #add the result to scores\\n        cos_ret_eng.append(round(similarity, 2))\\n    #otherwise if words don't exist in the word embeddings\\n    else:\\n        #assign default value of 0.0\\n        cos_ret_eng.append(0.00)        \\n\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ENGLISH\n",
    "\n",
    "#list of cosine similarities between retrofitted vectors\n",
    "cos_ret_eng = []\n",
    "\n",
    "#for every word pair\n",
    "for w1, w2 in word_pairs_eng:\n",
    "    #check if the words exist in the word_embeddings\n",
    "    if w1 in ret_eng and w2 in ret_eng:\n",
    "        #retrieve word1's vector \n",
    "        vec1 = ret_eng[w1]\n",
    "        #retreive word2's vector\n",
    "        vec2 = ret_eng[w2]\n",
    "        #calculate cosine between the two vectors\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        #add the result to scores\n",
    "        cos_ret_eng.append(round(similarity, 2))\n",
    "    #otherwise if words don't exist in the word embeddings\n",
    "    else:\n",
    "        #assign default value of 0.0\n",
    "        cos_ret_eng.append(0.00)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FRENCH\n",
    "\n",
    "#list of cosine similarities between retrofitted vectors\n",
    "cos_ret_fre = []\n",
    "\n",
    "#for every word pair\n",
    "for w1, w2 in word_pairs_eng:\n",
    "    #check if the words exist in the word_embeddings\n",
    "    if w1 in ret_fre and w2 in ret_fre:\n",
    "        #retrieve word1's vector \n",
    "        vec1 = ret_fre[w1]\n",
    "        #retreive word2's vector\n",
    "        vec2 = ret_fre[w2]\n",
    "        #calculate cosine between the two vectors\n",
    "        similarity = 1 - cosine(vec1, vec2)\n",
    "        #add the result to scores\n",
    "        cos_ret_fre.append(round(similarity, 2))\n",
    "    #otherwise if words don't exist in the word embeddings\n",
    "    else:\n",
    "        #assign default value of 0.0\n",
    "        cos_ret_fre.append(0.00)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman\n",
    "- a statistical measure to see how 2 things are related to each other (used when the relation between to sets of data is not a straight line, e.g. relatio between height and weight). \n",
    "\n",
    "- range from [-1, 1]:\n",
    "    - if close to 1, then as one var goes up the other var tends to go up too\n",
    "    - if close to -1, then as one var goes up the other var tends to go down\n",
    "    - if close to 0, then there's not much of a relation between the two vars\n",
    "\n",
    "\n",
    "- affected by outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman correlation coefficient for english (using non-retrofitted vectors):  0.591\n",
      "\n",
      "The Spearman correlation coefficient for french (using non-retrofitted vectors (50)):  0.562\n",
      "\n",
      "The Spearman correlation coefficient for french (using non-retrofitted vectors (100)):  0.636\n"
     ]
    }
   ],
   "source": [
    "#calculate the spearman rank correlation between human ratings and non-retrofitted vectors\n",
    "spearman_non_ret_eng, _ = spearmanr(hum_score_eng, cos_non_ret_eng)\n",
    "print(\"The Spearman correlation coefficient for english (using non-retrofitted vectors): \", round(spearman_non_ret_eng,3))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "spearman_non_ret_fre_50, _ = spearmanr(hum_score_fre, cos_non_ret_fre_50)\n",
    "print(\"The Spearman correlation coefficient for french (using non-retrofitted vectors (50)): \", round(spearman_non_ret_fre_50,3))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "spearman_non_ret_fre_100, _ = spearmanr(hum_score_fre, cos_non_ret_fre_100)\n",
    "print(\"The Spearman correlation coefficient for french (using non-retrofitted vectors (100)): \", round(spearman_non_ret_fre_100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the spearman rank correlation between human ratings and retrofitted vectors\n",
    "spearman_ret_eng, _ = spearmanr(hum_score_eng, cos_ret_eng)\n",
    "print(\"The Spearman correlation coefficient for english (using retrofitted vectors): \", round(spearman_ret_eng,3))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "spearman_ret_fre, _ = spearmanr(hum_score_fre, cos_ret_fre)\n",
    "print(\"The Spearman correlation coefficient for french (using retrofitted vectors: \", round(spearman_ret_fre,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson\n",
    "\n",
    "- a statistical measure that quantifies the strength and direction of the linear relationshipo between two continuous variables.\n",
    "\n",
    "- range from [-1, 1]. Same interpretation as Spearman\n",
    "\n",
    "- p_value (point of reference, usually, O,05) determines how genuine the relation between the two vars are (if they are random or not)\n",
    "    - high p_value -> relation due to random chance\n",
    "    - low p_value -> evidence of a significant or real relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pearson correlation coefficient for english (using non-retrofitted vectors :  0.5606\n",
      "p_value:  0.0\n",
      "\n",
      "The Pearson correlation coefficient for french (using non-retrofitted vectors (50)):  0.4722\n",
      "p_value:  0.0001\n",
      "\n",
      "The Pearson correlation coefficient for french (using non-retrofitted vectors (100)):  0.5667\n",
      "p_value:  0.0\n"
     ]
    }
   ],
   "source": [
    "#convert values from string to float\n",
    "hum_score_eng = [float(value) for value in (hum_score_eng)]\n",
    "#compute the pearson correlation coefficient and the p_value between human ratings and non-retrofitted vectors\n",
    "pearson_non_ret_eng, p_value_non_ret_eng = pearsonr(hum_score_eng, cos_non_ret_eng)\n",
    "\n",
    "#print results\n",
    "print(\"The Pearson correlation coefficient for english (using non-retrofitted vectors : \", round(pearson_non_ret_eng, 4))\n",
    "print(\"p_value: \", round(p_value_non_ret_eng, 4))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "#convert values from string to float\n",
    "hum_score_fre = [float(value) for value in (hum_score_fre)]\n",
    "#compute the pearson correlation coefficient and the p_value\n",
    "pearson_non_ret_fre_50, p_value_non_ret_fre_50 = pearsonr(hum_score_fre, cos_non_ret_fre_50)\n",
    "pearson_non_ret_fre_100, p_value_non_ret_fre_100 = pearsonr(hum_score_fre, cos_non_ret_fre_100)\n",
    "\n",
    "#print results\n",
    "print(\"The Pearson correlation coefficient for french (using non-retrofitted vectors (50)): \", round(pearson_non_ret_fre_50, 4))\n",
    "print(\"p_value: \", round(p_value_non_ret_fre_50, 4))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"The Pearson correlation coefficient for french (using non-retrofitted vectors (100)): \", round(pearson_non_ret_fre_100, 4))\n",
    "print(\"p_value: \", round(p_value_non_ret_fre_100, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the pearson correlation coefficient and the p_value between human ratings and retrofitted vectors\n",
    "pearson_ret_eng, p_value_ret_eng = pearsonr(hum_score_eng, cos_ret_eng)\n",
    "\n",
    "#print results\n",
    "print(\"The Pearson correlation coefficient for english (using etrofitted vectors : \", round(pearson_ret_eng, 4))\n",
    "print(\"p_value: \", round(p_value_ret_eng, 4))\n",
    "\n",
    "pearson_ret_fre, p_value_ret_fre = pearsonr(hum_score_fre, cos_ret_fre)\n",
    "\n",
    "#print results\n",
    "print(\"The Pearson correlation coefficient for french (using retrofitted vectors: \", round(pearson_ret_fre, 4))\n",
    "print(\"p_value: \", round(p_value_ret_fre, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1: Lexical similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENG\n",
    "lex_sim_eng = open (\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/English/evaluations/lexical similarity/ws353_lexical_similarity.txt\", 'r')\n",
    "\n",
    "\n",
    "#FRE\n",
    "lex_sim_fre = open (\"/Users/deeksha/Desktop/Improving-vector-space-representations-using-semantic-resources/data/French/lexicon/rg65_french.txt\", 'r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENG\n",
    "file_non_ret_eng = file_non_ret_eng\n",
    "file_ret_eng = None #Nina\n",
    "\n",
    "#FRE\n",
    "file_non_ret_fre_50 = file_non_ret_fre_50\n",
    "file_non_ret_fre_100 = file_non_ret_fre_100\n",
    "file_ret_fre = None #Nina"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENG\n",
    "non_ret_eng = non_ret_eng\n",
    "ret_eng = None \n",
    "\n",
    "#FRE\n",
    "non_ret_fre_50 = non_ret_fre_50\n",
    "non_ret_fre_100 = non_ret_fre_100\n",
    "ret_fre = None "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENG\n",
    "hum_score_eng = hum_score_eng\n",
    "cos_non_ret_eng = cos_non_ret_eng\n",
    "cos_ret_eng = None\n",
    "\n",
    "#FRE\n",
    "hum_score_fre = hum_score_fre\n",
    "cos_non_ret_fre_50 = cos_non_ret_fre_50\n",
    "cos_non_ret_fre_100 = cos_non_ret_fre_100\n",
    "cos_ret_fre = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_non_ret_eng = (hum_score_eng, cos_non_ret_eng)\n",
    "spearman_ret_eng = (hum_score_eng, cos_ret_eng)\n",
    "\n",
    "spearman_non_ret_fre_50 = (hum_score_fre, cos_non_ret_fre_50)\n",
    "spearman_non_ret_fre_100 = (hum_score_fre, cos_non_ret_fre_100)\n",
    "spearman_ret_fre = (hum_score_fre, cos_ret_fre)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_non_ret_eng, p_value_non_ret_eng = (hum_score_fre, cos_non_ret_eng)\n",
    "pearson_ret_eng, p_value_ret_eng = (hum_score_fre, cos_ret_eng)\n",
    "\n",
    "pearson_non_ret_fre_50, p_value_non_ret_fre_50 = (hum_score_fre, cos_non_ret_fre_50)\n",
    "pearson_non_ret_fre_100, p_value_non_ret_fre_100 = (hum_score_fre, cos_non_ret_fre_100)\n",
    "pearson_ret_fre, p_value_ret_fre = (hum_score_fre, cos_ret_fre)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global function for task 1: lexical similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(lang):\n",
    "    pass\n",
    "\n",
    "#input = word_embedding file (ret/non_ret)\n",
    "#output = 1. human ratings (eng, fren)\n",
    "#         2. cosine similarities (eng, fre)\n",
    "#         3. spearman (eng, fre)\n",
    "#         4. pearson (eng, fre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
