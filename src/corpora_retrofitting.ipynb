{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\ninan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import urllib.request\n",
    "import io\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, matutils\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()\n",
    "  \n",
    "\n",
    "  \n",
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    first_line = True\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # Skip the first line\n",
    "      if first_line:\n",
    "        first_line =False\n",
    "        continue\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "  ''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptut file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()\n",
    "\n",
    "''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.lower().strip().split()\n",
    "            lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the same format for the toy corpus as for the provided word embeddings\n",
    "def convert_matrix_to_dict(wordVecMat, wordList):\n",
    "    wordVecs = {}\n",
    "\n",
    "    for i, word in enumerate(wordList):\n",
    "        wordVecs[word] = wordVecMat[i]\n",
    "\n",
    "    return wordVecs\n",
    "\n",
    "def convert_dict_to_matrix(wordVecs):\n",
    "    wordVecMat = np.stack(list(wordVecs.values()))\n",
    "    return wordVecMat\n",
    "\n",
    "def vectorize_list(corpus):\n",
    "    corpus_vecs = [model[word] for word in corpus]\n",
    "\n",
    "    return corpus_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same input format as the real corpus\n",
    "toy_corpus = [\"cat\", \"tiger\", \"computer\", \"keyboard\", \"plane\", \"car\", \"doctor\", \"nurse\", \"love\", \"sex\"]\n",
    "toy_corpus_list_vecs = vectorize_list(toy_corpus)\n",
    "toy_wordVecs = convert_matrix_to_dict(toy_corpus_list_vecs, toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    similarity = dot_product / norm_product\n",
    "    return similarity\n",
    "\n",
    "def generate_cosine_similarity_matrix(dict_vecs): \n",
    "    num_vectors = len(dict_vecs)\n",
    "    similarity_matrix = np.zeros((num_vectors, num_vectors))\n",
    "    for i, word1 in enumerate(dict_vecs):\n",
    "        for j, word2 in enumerate(dict_vecs):\n",
    "            similarity_matrix[i, j] = calculate_cosine_similarity(dict_vecs[word1], dict_vecs[word2])\n",
    "    return similarity_matrix\n",
    "\n",
    "def print_vec_similarities(wordList, similarity_matrix):\n",
    "    for word, vec in zip(wordList, similarity_matrix):\n",
    "        print(f'Similarities with \"{word}\":')\n",
    "        for i in range(len(vec)):\n",
    "            similarity = vec[i]\n",
    "            print(f'  - \"{wordList[i]}\": {similarity:.4f}')\n",
    "        print()\n",
    "\n",
    "def print_similarity_difference(similarity_matrix, retrofitted_similarity_matrix):\n",
    "    difference = np.abs(similarity_matrix - retrofitted_similarity_matrix)\n",
    "    print(\"Similarity Difference Matrix:\")\n",
    "    print(difference)\n",
    "\n",
    "def cosine_similarity_matrix(matrix1, matrix2):\n",
    "    dot_product = np.sum(matrix1 * matrix2)\n",
    "    norm_matrix1 = np.linalg.norm(matrix1)\n",
    "    norm_matrix2 = np.linalg.norm(matrix2)\n",
    "    cosine_similarity = dot_product / (norm_matrix1 * norm_matrix2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5173\n",
      "  - \"computer\": 0.1732\n",
      "  - \"keyboard\": 0.1834\n",
      "  - \"plane\": 0.1833\n",
      "  - \"car\": 0.2153\n",
      "  - \"doctor\": 0.1292\n",
      "  - \"nurse\": 0.1594\n",
      "  - \"love\": 0.1406\n",
      "  - \"sex\": 0.1368\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0677\n",
      "  - \"keyboard\": 0.0654\n",
      "  - \"plane\": 0.1660\n",
      "  - \"car\": 0.1672\n",
      "  - \"doctor\": 0.0835\n",
      "  - \"nurse\": 0.1111\n",
      "  - \"love\": 0.0871\n",
      "  - \"sex\": 0.2222\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732\n",
      "  - \"tiger\": 0.0677\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.3964\n",
      "  - \"plane\": 0.1909\n",
      "  - \"car\": 0.2461\n",
      "  - \"doctor\": 0.1628\n",
      "  - \"nurse\": 0.2178\n",
      "  - \"love\": 0.0573\n",
      "  - \"sex\": 0.1853\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834\n",
      "  - \"tiger\": 0.0654\n",
      "  - \"computer\": 0.3964\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1006\n",
      "  - \"car\": 0.1498\n",
      "  - \"doctor\": 0.0850\n",
      "  - \"nurse\": 0.1220\n",
      "  - \"love\": 0.1591\n",
      "  - \"sex\": 0.0943\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833\n",
      "  - \"tiger\": 0.1660\n",
      "  - \"computer\": 0.1909\n",
      "  - \"keyboard\": 0.1006\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.3780\n",
      "  - \"doctor\": 0.1879\n",
      "  - \"nurse\": 0.0978\n",
      "  - \"love\": 0.1080\n",
      "  - \"sex\": 0.0587\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153\n",
      "  - \"tiger\": 0.1672\n",
      "  - \"computer\": 0.2461\n",
      "  - \"keyboard\": 0.1498\n",
      "  - \"plane\": 0.3780\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1895\n",
      "  - \"nurse\": 0.1306\n",
      "  - \"love\": 0.0842\n",
      "  - \"sex\": 0.1169\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292\n",
      "  - \"tiger\": 0.0835\n",
      "  - \"computer\": 0.1628\n",
      "  - \"keyboard\": 0.0850\n",
      "  - \"plane\": 0.1879\n",
      "  - \"car\": 0.1895\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6320\n",
      "  - \"love\": 0.0831\n",
      "  - \"sex\": 0.1994\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594\n",
      "  - \"tiger\": 0.1111\n",
      "  - \"computer\": 0.2178\n",
      "  - \"keyboard\": 0.1220\n",
      "  - \"plane\": 0.0978\n",
      "  - \"car\": 0.1306\n",
      "  - \"doctor\": 0.6320\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.0631\n",
      "  - \"sex\": 0.1997\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406\n",
      "  - \"tiger\": 0.0871\n",
      "  - \"computer\": 0.0573\n",
      "  - \"keyboard\": 0.1591\n",
      "  - \"plane\": 0.1080\n",
      "  - \"car\": 0.0842\n",
      "  - \"doctor\": 0.0831\n",
      "  - \"nurse\": 0.0631\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.2639\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368\n",
      "  - \"tiger\": 0.2222\n",
      "  - \"computer\": 0.1853\n",
      "  - \"keyboard\": 0.0943\n",
      "  - \"plane\": 0.0587\n",
      "  - \"car\": 0.1169\n",
      "  - \"doctor\": 0.1994\n",
      "  - \"nurse\": 0.1997\n",
      "  - \"love\": 0.2639\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = generate_cosine_similarity_matrix(toy_wordVecs)\n",
    "print_vec_similarities(toy_corpus, similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_lexicon(target_words, relation_types):\n",
    "    lexicon = {}\n",
    "        \n",
    "    for word in target_words:\n",
    "        related_words = []\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Skip word if no synsets found\n",
    "        if not word_synsets:\n",
    "            continue\n",
    "\n",
    "        for syn in word_synsets:\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    if \"synonyms\" in relation_types:\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"antonyms\" in relation_types:\n",
    "                if syn.lemmas()[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            if \"hyponyms\" in relation_types:\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"hypernyms\" in relation_types:\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"meronyms\" in relation_types:\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"holonyms\" in relation_types:\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"homonyms\" in relation_types:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wordnet.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecMat = convert_dict_to_matrix(toy_wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(toy_corpus_list_vecs)) \n",
    "\n",
    "print(type(wordVecMat)) \n",
    "print(wordVecMat.shape)  \n",
    "print(wordVecMat.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 10)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(similarity_matrix)) \n",
    "print(similarity_matrix.shape)  \n",
    "print(similarity_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for the big corpus to retrive the word list from the keys\n",
    "def get_embeddings_words(wordVecs):\n",
    "    wordList = list(wordVecs.keys()) # TODO: or set?\n",
    "    return wordList\n",
    "\n",
    "wordList = get_embeddings_words(toy_wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighbors_embedding_matrix(wordList, relation_type):\n",
    "    # Retrieve synonyms for each word\n",
    "    neighbors_dict = get_wordnet_lexicon(wordList, relation_type)\n",
    "    \n",
    "    # Compute average embedding\n",
    "    average_embeddings = []\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        embeddings = [\n",
    "            model.get_vector(neighbor)\n",
    "            for neighbor in neighbors\n",
    "            if model.has_index_for(neighbor)\n",
    "        ]\n",
    "        if len(embeddings) > 0:\n",
    "            average_embedding = np.sum(embeddings, axis=0) / len(embeddings)\n",
    "        else:\n",
    "            # Handle the case where a word has no embeddings for its synonyms\n",
    "            average_embedding = np.zeros(model.vector_size)  # Use a zero vector\n",
    "        average_embeddings.append(average_embedding)\n",
    "    \n",
    "    # Create the word embedding matrix\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "\n",
    "    return neighbors_embedding_matrix\n",
    "\n",
    "   \n",
    "    \n",
    "neighbors_matrix = create_neighbors_embedding_matrix(wordList, \"synonyms\")\n",
    "\n",
    "# récupérer la liste des syn dans wordnet\n",
    "# vectorise chaque syn\n",
    "# BOW des synonymes (sum) pour n'avoir qu'un embedding \n",
    "# BOW_syn_cat\n",
    "# BOW_syn_dog= neighbors_matrix, shape (10, embedding_size) donc same size as wordVecs_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(neighbors_matrix))  # <class 'numpy.ndarray'>\n",
    "print(neighbors_matrix.shape)  # (m, n)\n",
    "print(neighbors_matrix.ndim)   # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat))  # <class 'numpy.ndarray'>\n",
    "print(wordVecMat.shape)  # (m, n)\n",
    "print(wordVecMat.ndim)   # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00676925  0.17245371 -0.32560504 -0.02995695  0.24918167  0.06591797\n",
      "  0.07754347  0.02062197  0.12026186 -0.24899179  0.11663705 -0.43598995\n",
      " -0.0165247  -0.43618888  0.05141873 -0.2046328   0.09973597  0.0461245\n",
      " -0.4420053   0.08787028  0.26338252 -0.1518453   0.11536521 -0.14642108\n",
      " -0.04931188  0.32405486 -0.15808557  0.31547716  0.40427201 -0.007934\n",
      " -0.00195312 -0.20903128 -0.03433369 -0.0849519   0.01340795  0.15198545\n",
      " -0.07754234  0.04350902  0.00148463  0.24564164  0.00093107  0.03174506\n",
      " -0.14347048  0.13828702  0.08990253 -0.03038646  0.37447442 -0.05503337\n",
      "  0.00611821  0.00164738 -0.15880896  0.13665545  0.34953252  0.14862174\n",
      " -0.0103189  -0.14141733  0.15832859 -0.30018898  0.35803392  0.18776052\n",
      "  0.28351056  0.18667716 -0.01274052  0.19207764  0.04131345  0.04772498\n",
      "  0.20808016  0.02882668 -0.10890771 -0.12867793  0.40634721 -0.03548855\n",
      " -0.15468343 -0.01318077 -0.11528298  0.38514766  0.11844098 -0.08239746\n",
      " -0.02005401 -0.06692618  0.08503554 -0.00346544  0.1995228  -0.07893711\n",
      " -0.34383251  0.04050474 -0.2636391  -0.14694101 -0.1783899  -0.0918257\n",
      " -0.0131429   0.04139088 -0.06987395 -0.25662345 -0.17748967 -0.27476671\n",
      " -0.01672363 -0.17666287 -0.33676034 -0.18375199 -0.24108435 -0.02301817\n",
      " -0.0048376   0.25613178  0.07612101 -0.20470513  0.0953064  -0.03844798\n",
      "  0.04892759 -0.11653646 -0.07707384  0.29826298  0.11636692  0.32834201\n",
      " -0.31238471 -0.00066913 -0.02505154 -0.23873901 -0.22574163 -0.04350577\n",
      " -0.03468753 -0.05264395 -0.30785455  0.02505323 -0.34844179  0.11226626\n",
      "  0.10000073  0.1189044   0.04702872 -0.14758527  0.13923419  0.04321967\n",
      " -0.15519799 -0.06542969 -0.02879789 -0.11844042 -0.02837584  0.09594444\n",
      "  0.10183038  0.10198523 -0.06081022  0.02417896 -0.15294054  0.04794516\n",
      "  0.06066442  0.15922716 -0.19703731  0.17048589 -0.12367983 -0.15320898\n",
      "  0.22272971  0.09056261 -0.31849727 -0.08702935 -0.13079947 -0.05045121\n",
      "  0.09113679  0.05342385  0.03645833  0.22512252 -0.10272612 -0.05701814\n",
      "  0.09566696 -0.13046604 -0.12130398  0.00603117 -0.04856138  0.21248373\n",
      "  0.17837637  0.02974899 -0.08555999 -0.03059218 -0.04345082  0.23660165\n",
      " -0.08013295  0.09815696  0.07034867 -0.12642416 -0.242515    0.27940539\n",
      " -0.16617132  0.01541816 -0.00099126  0.0524677  -0.03739194  0.07791251\n",
      "  0.20973601  0.3004286   0.12838844  0.06714884  0.02089154 -0.16434733\n",
      "  0.12680845  0.06278935 -0.01593244  0.184226    0.07969835 -0.18987359\n",
      " -0.11193918  0.18708067  0.06552409 -0.27893744 -0.0675354  -0.05583897\n",
      "  0.04787643  0.11910897 -0.01438636 -0.13045473  0.10323758 -0.26696325\n",
      " -0.40234262 -0.02973316 -0.0515894  -0.08254327 -0.11705977 -0.0447789\n",
      "  0.16213056 -0.31804127 -0.10855222  0.12037037  0.18612897  0.11705413\n",
      " -0.22371081 -0.27236599 -0.2145137  -0.07454851  0.07460587 -0.07304269\n",
      "  0.22288344  0.00952374  0.10791355  0.03955135 -0.03007846  0.04184525\n",
      " -0.28511386  0.05631058  0.27960488  0.02073415 -0.08864961  0.066971\n",
      "  0.23477738 -0.20298824  0.03927584  0.03217344  0.33202221  0.05400481\n",
      "  0.05493726 -0.036039    0.22249632 -0.13921215 -0.33608415  0.18813239\n",
      " -0.28481038  0.02863178 -0.04690326  0.27920871 -0.1286395   0.1268627\n",
      " -0.15716192  0.07503933 -0.08402846  0.04506429 -0.04430474 -0.1314799\n",
      "  0.06542573  0.24106775 -0.10216381  0.02992079 -0.27746017 -0.02798801\n",
      " -0.07106696  0.04219563 -0.15462466  0.13976994  0.05472141  0.09482377\n",
      "  0.1267994   0.16184228  0.17293295 -0.14490651  0.2610078   0.05841742\n",
      " -0.20719062  0.28466684  0.13873743 -0.08083654 -0.09813182  0.04846644\n",
      "  0.08657498  0.13033097 -0.25310149  0.11868851 -0.1590384   0.03733656\n",
      "  0.42320421 -0.01760412  0.18546778  0.49357605 -0.276461    0.01063255]\n"
     ]
    }
   ],
   "source": [
    "difference = toy_corpus_list_vecs[0] - neighbors_matrix[0]\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter):\n",
    "    # Create a deep copy of wordVecMat \n",
    "    newWordVecMat = np.copy(wordVecMat, order='K')\n",
    "    updates = []\n",
    "    \n",
    "    for _ in range(nb_iter):\n",
    "        # Calculate the number of neighbors for each word\n",
    "        # numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\n",
    "        \n",
    "        # Update the word embeddings using retrofitting formula\n",
    "        newWordVecMat = (alpha * newWordVecMat + beta * neighbors_mean_matrix) / (alpha + beta)\n",
    "\n",
    "        # Calculate the updates\n",
    "        update = newWordVecMat - wordVecMat\n",
    "        updates.append(update)\n",
    "\n",
    "        # Update the wordVecMat for the next iteration\n",
    "        wordVecMat = newWordVecMat\n",
    "        # TODO: calculer similarité après chaque itération\n",
    "        # Stoping criterion\n",
    "        if np.linalg.norm(updates) < 1e-2:\n",
    "            break # TODO: return the embedding\n",
    "\n",
    "    # Convert the matrix back to a dictionary of word vectors\n",
    "    # retrofitted_wordVecs = dict(zip(wordList, newWordVecMat))\n",
    "\n",
    "    return newWordVecMat, updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = get_wordnet_lexicon(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repair': ['doctor'],\n",
       " 'sexual_activity': ['sex'],\n",
       " 'airplane': ['plane'],\n",
       " 'physician': ['doctor'],\n",
       " 'sick': ['cat'],\n",
       " 'CT': ['cat'],\n",
       " 'have_it_away': ['love'],\n",
       " 'making_love': ['love'],\n",
       " 'machine': ['car'],\n",
       " 'bang': ['love'],\n",
       " 'be_intimate': ['love'],\n",
       " 'computed_axial_tomography': ['cat'],\n",
       " 'computing_device': ['computer'],\n",
       " 'be_sick': ['cat'],\n",
       " 'calculator': ['computer'],\n",
       " 'railway_car': ['car'],\n",
       " 'hombre': ['cat'],\n",
       " 'car': ['automobile',\n",
       "  'cable_car',\n",
       "  'machine',\n",
       "  'auto',\n",
       "  'motorcar',\n",
       "  'gondola',\n",
       "  'elevator_car',\n",
       "  'railway_car',\n",
       "  'railcar',\n",
       "  'railroad_car'],\n",
       " 'sex_activity': ['sex'],\n",
       " 'auto': ['car'],\n",
       " 'spue': ['cat'],\n",
       " 'CAT': ['cat'],\n",
       " 'elevator_car': ['car'],\n",
       " 'guy': ['cat'],\n",
       " 'tiger': ['Panthera_tigris'],\n",
       " 'breastfeed': ['nurse'],\n",
       " 'vomit': ['cat'],\n",
       " 'estimator': ['computer'],\n",
       " 'passion': ['love'],\n",
       " 'lovemaking': ['love'],\n",
       " 'computer': ['estimator',\n",
       "  'data_processor',\n",
       "  'reckoner',\n",
       "  'information_processing_system',\n",
       "  'figurer',\n",
       "  'computing_device',\n",
       "  'computing_machine',\n",
       "  'calculator',\n",
       "  'electronic_computer'],\n",
       " 'MD': ['doctor'],\n",
       " 'beloved': ['love'],\n",
       " 'furbish_up': ['doctor'],\n",
       " 'motorcar': ['car'],\n",
       " 'aeroplane': ['plane'],\n",
       " 'skim': ['plane'],\n",
       " 'have_sex': ['love'],\n",
       " 'electronic_computer': ['computer'],\n",
       " 'mend': ['doctor'],\n",
       " 'gondola': ['car'],\n",
       " 'bonk': ['love'],\n",
       " 'automobile': ['car'],\n",
       " 'woodworking_plane': ['plane'],\n",
       " 'bozo': ['cat'],\n",
       " 'keyboard': [],\n",
       " 'love': ['do_it',\n",
       "  'beloved',\n",
       "  'have_a_go_at_it',\n",
       "  'honey',\n",
       "  'make_love',\n",
       "  'sleep_with',\n",
       "  'have_it_away',\n",
       "  'making_love',\n",
       "  'bang',\n",
       "  'sleep_together',\n",
       "  'roll_in_the_hay',\n",
       "  'be_intimate',\n",
       "  'jazz',\n",
       "  'love_life',\n",
       "  'enjoy',\n",
       "  'have_sex',\n",
       "  'hump',\n",
       "  'dearest',\n",
       "  'bonk',\n",
       "  'get_laid',\n",
       "  'sexual_love',\n",
       "  'eff',\n",
       "  'make_out',\n",
       "  'have_intercourse',\n",
       "  'fuck',\n",
       "  'have_it_off',\n",
       "  'lie_with',\n",
       "  'dear',\n",
       "  'screw',\n",
       "  'bed',\n",
       "  'erotic_love',\n",
       "  'passion',\n",
       "  'lovemaking',\n",
       "  'get_it_on',\n",
       "  'know'],\n",
       " 'sexuality': ['sex'],\n",
       " 'doc': ['doctor'],\n",
       " 'regurgitate': ['cat'],\n",
       " 'sexual_practice': ['sex'],\n",
       " 'barf': ['cat'],\n",
       " 'dear': ['love'],\n",
       " 'cast': ['cat'],\n",
       " 'level': ['plane'],\n",
       " 'screw': ['love'],\n",
       " 'bed': ['love'],\n",
       " 'spew': ['cat'],\n",
       " 'computing_machine': ['computer'],\n",
       " 'lactate': ['nurse'],\n",
       " 'jazz': ['love'],\n",
       " 'computerized_tomography': ['cat'],\n",
       " 'railcar': ['car'],\n",
       " 'kat': ['cat'],\n",
       " 'throw_up': ['cat'],\n",
       " 'do_it': ['love'],\n",
       " 'give_suck': ['nurse'],\n",
       " 'plane': ['woodworking_plane',\n",
       "  'planer',\n",
       "  \"carpenter's_plane\",\n",
       "  'shave',\n",
       "  'airplane',\n",
       "  'level',\n",
       "  'aeroplane',\n",
       "  'skim',\n",
       "  'sheet',\n",
       "  'planing_machine',\n",
       "  'flat'],\n",
       " 'nurse': ['suckle',\n",
       "  'hold',\n",
       "  'give_suck',\n",
       "  'nanny',\n",
       "  'suck',\n",
       "  'harbour',\n",
       "  'entertain',\n",
       "  'nursemaid',\n",
       "  'lactate',\n",
       "  'wet-nurse',\n",
       "  'harbor',\n",
       "  'breastfeed'],\n",
       " 'sexual_urge': ['sex'],\n",
       " 'honey': ['love'],\n",
       " 'wind_up': ['sex'],\n",
       " 'suckle': ['nurse'],\n",
       " 'touch_on': ['doctor'],\n",
       " 'gender': ['sex'],\n",
       " 'purge': ['cat'],\n",
       " 'fix': ['doctor'],\n",
       " \"cat-o'-nine-tails\": ['cat'],\n",
       " 'retch': ['cat'],\n",
       " 'reckoner': ['computer'],\n",
       " 'harbour': ['nurse'],\n",
       " 'enjoy': ['love'],\n",
       " 'dearest': ['love'],\n",
       " 'doctor_up': ['doctor'],\n",
       " 'doctor': ['doctor_up',\n",
       "  'touch_on',\n",
       "  'repair',\n",
       "  'medico',\n",
       "  'fix',\n",
       "  'MD',\n",
       "  'mend',\n",
       "  'furbish_up',\n",
       "  'sophisticate',\n",
       "  'restore',\n",
       "  'physician',\n",
       "  'Doctor',\n",
       "  'doc',\n",
       "  'Dr.',\n",
       "  'Doctor_of_the_Church',\n",
       "  'bushel'],\n",
       " 'big_cat': ['cat'],\n",
       " 'cable_car': ['car'],\n",
       " 'chuck': ['cat'],\n",
       " 'figurer': ['computer'],\n",
       " 'sexual_love': ['love'],\n",
       " 'make_out': ['love'],\n",
       " 'fuck': ['love'],\n",
       " 'harbor': ['nurse'],\n",
       " 'qat': ['cat'],\n",
       " 'turn_on': ['sex'],\n",
       " 'khat': ['cat'],\n",
       " 'have_it_off': ['love'],\n",
       " 'lie_with': ['love'],\n",
       " 'restore': ['doctor'],\n",
       " 'quat': ['cat'],\n",
       " 'get_it_on': ['love'],\n",
       " 'honk': ['cat'],\n",
       " 'suck': ['nurse'],\n",
       " 'bushel': ['doctor'],\n",
       " 'computed_tomography': ['cat'],\n",
       " 'arouse': ['sex'],\n",
       " 'have_a_go_at_it': ['love'],\n",
       " \"carpenter's_plane\": ['plane'],\n",
       " 'shave': ['plane'],\n",
       " 'sophisticate': ['doctor'],\n",
       " 'entertain': ['nurse'],\n",
       " 'excite': ['sex'],\n",
       " 'nursemaid': ['nurse'],\n",
       " 'make_love': ['love'],\n",
       " 'sleep_with': ['love'],\n",
       " 'planing_machine': ['plane'],\n",
       " 'Dr.': ['doctor'],\n",
       " 'upchuck': ['cat'],\n",
       " 'computerized_axial_tomography': ['cat'],\n",
       " 'Panthera_tigris': ['tiger'],\n",
       " 'hold': ['nurse'],\n",
       " 'sleep_together': ['love'],\n",
       " 'planer': ['plane'],\n",
       " 'African_tea': ['cat'],\n",
       " 'roll_in_the_hay': ['love'],\n",
       " 'hump': ['love'],\n",
       " 'love_life': ['love'],\n",
       " 'Caterpillar': ['cat'],\n",
       " 'true_cat': ['cat'],\n",
       " 'Doctor_of_the_Church': ['doctor'],\n",
       " 'flat': ['plane'],\n",
       " 'railroad_car': ['car'],\n",
       " 'get_laid': ['love'],\n",
       " 'medico': ['doctor'],\n",
       " 'Arabian_tea': ['cat'],\n",
       " 'eff': ['love'],\n",
       " 'regorge': ['cat'],\n",
       " 'Doctor': ['doctor'],\n",
       " 'have_intercourse': ['love'],\n",
       " 'vomit_up': ['cat'],\n",
       " 'cat': ['computed_tomography',\n",
       "  'throw_up',\n",
       "  'sick',\n",
       "  'CT',\n",
       "  'upchuck',\n",
       "  'computerized_axial_tomography',\n",
       "  'purge',\n",
       "  \"cat-o'-nine-tails\",\n",
       "  'African_tea',\n",
       "  'retch',\n",
       "  'computed_axial_tomography',\n",
       "  'be_sick',\n",
       "  'Caterpillar',\n",
       "  'true_cat',\n",
       "  'hombre',\n",
       "  'big_cat',\n",
       "  'chuck',\n",
       "  'Arabian_tea',\n",
       "  'spue',\n",
       "  'CAT',\n",
       "  'regorge',\n",
       "  'guy',\n",
       "  'bozo',\n",
       "  'vomit_up',\n",
       "  'vomit',\n",
       "  'regurgitate',\n",
       "  'qat',\n",
       "  'khat',\n",
       "  'disgorge',\n",
       "  'barf',\n",
       "  'cast',\n",
       "  'quat',\n",
       "  'spew',\n",
       "  'puke',\n",
       "  'computerized_tomography',\n",
       "  'honk',\n",
       "  'kat'],\n",
       " 'disgorge': ['cat'],\n",
       " 'data_processor': ['computer'],\n",
       " 'nanny': ['nurse'],\n",
       " 'information_processing_system': ['computer'],\n",
       " 'sex': ['wind_up',\n",
       "  'sex_activity',\n",
       "  'sexual_practice',\n",
       "  'gender',\n",
       "  'arouse',\n",
       "  'sexual_activity',\n",
       "  'turn_on',\n",
       "  'sexual_urge',\n",
       "  'excite',\n",
       "  'sexuality'],\n",
       " 'erotic_love': ['love'],\n",
       " 'sheet': ['plane'],\n",
       " 'wet-nurse': ['nurse'],\n",
       " 'puke': ['cat'],\n",
       " 'know': ['love']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_graph_from_synonyms(synonyms_dict):\n",
    "    graph = {}\n",
    "    \n",
    "    # Create a set of all unique words in the dictionary\n",
    "    words = set(synonyms_dict.keys()).union(*synonyms_dict.values())\n",
    "    \n",
    "    # Initialize an empty adjacency dictionary for each word\n",
    "    for word in words:\n",
    "        graph[word] = set()\n",
    "    \n",
    "    # Iterate through the synonyms dictionary\n",
    "    for word, synonyms in synonyms_dict.items():\n",
    "        # Add synonyms to the adjacency set for the word\n",
    "        graph[word].update(synonyms)\n",
    "        \n",
    "        # Add the word as a synonym to each synonym's adjacency set\n",
    "        for synonym in synonyms:\n",
    "            graph[synonym].add(word)\n",
    "    \n",
    "    # Convert the adjacency sets to lists\n",
    "    graph = {word: list(adjacency_set) for word, adjacency_set in graph.items()}\n",
    "    \n",
    "    return graph\n",
    "\n",
    "graph = generate_graph_from_synonyms(lexicon)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs_article(Q, Q_hat, graph, alpha, beta, num_iterations=10):\n",
    "    num_words = Q.shape[0]\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        Q_new = np.zeros_like(Q)\n",
    "        for i in range(num_words):\n",
    "            neighbors = graph[i]\n",
    "            numerator = np.sum(beta[i, j] * Q[j] for j in neighbors) + alpha[i] * Q_hat[i]\n",
    "            denominator = np.sum(beta[i, j] for j in neighbors) + alpha[i]\n",
    "            Q_new[i] = numerator / denominator\n",
    "        Q = Q_new\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrofitted_toy_vecs, updates = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newVecs = retrofitting_wordVecs_article(wordVecMat, neighbors_matrix, graph, alpha=1, beta=1, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 6\n",
      "Number of edges: 5\n",
      "Neighbors of cat: ['kitten', 'animal', 'pet']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_to_keep = [\"cat\", \"kitten\", \"dog\", \"puppy\", \"animal\", \"pet\"]\n",
    "# Construct an empty graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(words_to_keep)\n",
    "\n",
    "# Add edges between related words\n",
    "graph.add_edges_from([('cat', 'kitten'), ('dog', 'puppy'), ('cat', 'animal'), ('dog', 'animal'), ('cat', 'pet')])\n",
    "\n",
    "# Print the graph information\n",
    "print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "print(\"Number of edges:\", graph.number_of_edges())\n",
    "\n",
    "# Access neighbors of a word\n",
    "word = 'cat'\n",
    "neighbors = graph.neighbors(word)\n",
    "print(\"Neighbors of\", word + \":\", list(neighbors))\n",
    "\n",
    "# Create the layout for the graph\n",
    "layout = nx.spring_layout(graph)\n",
    "\n",
    "# Draw the nodes\n",
    "nx.draw_networkx_nodes(graph, pos=layout, node_color='lightblue', node_size=500)\n",
    "\n",
    "# Draw the edges\n",
    "nx.draw_networkx_edges(graph, pos=layout, edge_color='gray')\n",
    "\n",
    "# Add labels to the nodes\n",
    "nx.draw_networkx_labels(graph, pos=layout, font_color='black')\n",
    "\n",
    "# Set plot properties\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00338463, -0.08622685,  0.16280252, ..., -0.24678802,\n",
       "          0.1382305 , -0.00531627],\n",
       "        [ 0.08007812, -0.06433105,  0.02783203, ..., -0.08123779,\n",
       "          0.03717041, -0.03466797],\n",
       "        [ 0.00255203,  0.02276611, -0.13401031, ..., -0.07601929,\n",
       "          0.05496216,  0.07133484],\n",
       "        ...,\n",
       "        [ 0.04763455,  0.11241319, -0.07042948, ..., -0.00129022,\n",
       "         -0.09945594, -0.13053046],\n",
       "        [-0.03367829,  0.04928064, -0.00994873, ...,  0.08418322,\n",
       "          0.06436348,  0.01234341],\n",
       "        [ 0.04439545, -0.02075195,  0.11517334, ..., -0.06329346,\n",
       "          0.06408691, -0.06994629]]),\n",
       " array([[-0.00169231, -0.04311343,  0.08140126, ..., -0.12339401,\n",
       "          0.06911525, -0.00265814],\n",
       "        [ 0.04003906, -0.03216553,  0.01391602, ..., -0.0406189 ,\n",
       "          0.01858521, -0.01733398],\n",
       "        [ 0.00127602,  0.01138306, -0.06700516, ..., -0.03800964,\n",
       "          0.02748108,  0.03566742],\n",
       "        ...,\n",
       "        [ 0.02381727,  0.0562066 , -0.03521474, ..., -0.00064511,\n",
       "         -0.04972797, -0.06526523],\n",
       "        [-0.01683915,  0.02464032, -0.00497437, ...,  0.04209161,\n",
       "          0.03218174,  0.0061717 ],\n",
       "        [ 0.02219772, -0.01037598,  0.05758667, ..., -0.03164673,\n",
       "          0.03204346, -0.03497314]]),\n",
       " array([[-0.00084616, -0.02155671,  0.04070063, ..., -0.06169701,\n",
       "          0.03455763, -0.00132907],\n",
       "        [ 0.02001953, -0.01608276,  0.00695801, ..., -0.02030945,\n",
       "          0.0092926 , -0.00866699],\n",
       "        [ 0.00063801,  0.00569153, -0.03350258, ..., -0.01900482,\n",
       "          0.01374054,  0.01783371],\n",
       "        ...,\n",
       "        [ 0.01190864,  0.0281033 , -0.01760737, ..., -0.00032255,\n",
       "         -0.02486398, -0.03263262],\n",
       "        [-0.00841957,  0.01232016, -0.00248718, ...,  0.0210458 ,\n",
       "          0.01609087,  0.00308585],\n",
       "        [ 0.01109886, -0.00518799,  0.02879333, ..., -0.01582336,\n",
       "          0.01602173, -0.01748657]]),\n",
       " array([[-0.00042308, -0.01077836,  0.02035031, ..., -0.0308485 ,\n",
       "          0.01727881, -0.00066453],\n",
       "        [ 0.01000977, -0.00804138,  0.003479  , ..., -0.01015472,\n",
       "          0.0046463 , -0.0043335 ],\n",
       "        [ 0.000319  ,  0.00284576, -0.01675129, ..., -0.00950241,\n",
       "          0.00687027,  0.00891685],\n",
       "        ...,\n",
       "        [ 0.00595432,  0.01405165, -0.00880369, ..., -0.00016128,\n",
       "         -0.01243199, -0.01631631],\n",
       "        [-0.00420979,  0.00616008, -0.00124359, ...,  0.0105229 ,\n",
       "          0.00804543,  0.00154293],\n",
       "        [ 0.00554943, -0.00259399,  0.01439667, ..., -0.00791168,\n",
       "          0.00801086, -0.00874329]]),\n",
       " array([[-2.11539096e-04, -5.38917829e-03,  1.01751575e-02, ...,\n",
       "         -1.54242516e-02,  8.63940627e-03, -3.32267140e-04],\n",
       "        [ 5.00488281e-03, -4.02069092e-03,  1.73950195e-03, ...,\n",
       "         -5.07736206e-03,  2.32315063e-03, -2.16674805e-03],\n",
       "        [ 1.59502029e-04,  1.42288208e-03, -8.37564468e-03, ...,\n",
       "         -4.75120544e-03,  3.43513489e-03,  4.45842743e-03],\n",
       "        ...,\n",
       "        [ 2.97715928e-03,  7.02582463e-03, -4.40184260e-03, ...,\n",
       "         -8.06384487e-05, -6.21599623e-03, -8.15815385e-03],\n",
       "        [-2.10489333e-03,  3.08004022e-03, -6.21795654e-04, ...,\n",
       "          5.26145101e-03,  4.02271748e-03,  7.71462917e-04],\n",
       "        [ 2.77471542e-03, -1.29699707e-03,  7.19833374e-03, ...,\n",
       "         -3.95584106e-03,  4.00543213e-03, -4.37164307e-03]]),\n",
       " array([[-1.05769548e-04, -2.69458914e-03,  5.08757873e-03, ...,\n",
       "         -7.71212578e-03,  4.31970314e-03, -1.66133570e-04],\n",
       "        [ 2.50244141e-03, -2.01034546e-03,  8.69750977e-04, ...,\n",
       "         -2.53868103e-03,  1.16157532e-03, -1.08337402e-03],\n",
       "        [ 7.97510147e-05,  7.11441040e-04, -4.18782234e-03, ...,\n",
       "         -2.37560272e-03,  1.71756744e-03,  2.22921371e-03],\n",
       "        ...,\n",
       "        [ 1.48857964e-03,  3.51291231e-03, -2.20092130e-03, ...,\n",
       "         -4.03192244e-05, -3.10799811e-03, -4.07907693e-03],\n",
       "        [-1.05244666e-03,  1.54002011e-03, -3.10897827e-04, ...,\n",
       "          2.63072550e-03,  2.01135874e-03,  3.85731459e-04],\n",
       "        [ 1.38735771e-03, -6.48498535e-04,  3.59916687e-03, ...,\n",
       "         -1.97792053e-03,  2.00271606e-03, -2.18582153e-03]]),\n",
       " array([[-5.28847740e-05, -1.34729457e-03,  2.54378936e-03, ...,\n",
       "         -3.85606289e-03,  2.15985157e-03, -8.30667850e-05],\n",
       "        [ 1.25122070e-03, -1.00517273e-03,  4.34875488e-04, ...,\n",
       "         -1.26934052e-03,  5.80787659e-04, -5.41687012e-04],\n",
       "        [ 3.98755074e-05,  3.55720520e-04, -2.09391117e-03, ...,\n",
       "         -1.18780136e-03,  8.58783722e-04,  1.11460686e-03],\n",
       "        ...,\n",
       "        [ 7.44289820e-04,  1.75645616e-03, -1.10046065e-03, ...,\n",
       "         -2.01596122e-05, -1.55399906e-03, -2.03953846e-03],\n",
       "        [-5.26223332e-04,  7.70010054e-04, -1.55448914e-04, ...,\n",
       "          1.31536275e-03,  1.00567937e-03,  1.92865729e-04],\n",
       "        [ 6.93678856e-04, -3.24249268e-04,  1.79958344e-03, ...,\n",
       "         -9.88960266e-04,  1.00135803e-03, -1.09291077e-03]]),\n",
       " array([[-2.64423870e-05, -6.73647286e-04,  1.27189468e-03, ...,\n",
       "         -1.92803144e-03,  1.07992578e-03, -4.15333925e-05],\n",
       "        [ 6.25610352e-04, -5.02586365e-04,  2.17437744e-04, ...,\n",
       "         -6.34670258e-04,  2.90393829e-04, -2.70843506e-04],\n",
       "        [ 1.99377537e-05,  1.77860260e-04, -1.04695559e-03, ...,\n",
       "         -5.93900681e-04,  4.29391861e-04,  5.57303429e-04],\n",
       "        ...,\n",
       "        [ 3.72144910e-04,  8.78228078e-04, -5.50230325e-04, ...,\n",
       "         -1.00798061e-05, -7.76999528e-04, -1.01976923e-03],\n",
       "        [-2.63111666e-04,  3.85005027e-04, -7.77244568e-05, ...,\n",
       "          6.57681376e-04,  5.02839684e-04,  9.64328647e-05],\n",
       "        [ 3.46839428e-04, -1.62124634e-04,  8.99791718e-04, ...,\n",
       "         -4.94480133e-04,  5.00679016e-04, -5.46455383e-04]]),\n",
       " array([[-1.32211935e-05, -3.36823643e-04,  6.35947341e-04, ...,\n",
       "         -9.64015722e-04,  5.39962892e-04, -2.07666963e-05],\n",
       "        [ 3.12805176e-04, -2.51293182e-04,  1.08718872e-04, ...,\n",
       "         -3.17335129e-04,  1.45196915e-04, -1.35421753e-04],\n",
       "        [ 9.96887684e-06,  8.89301300e-05, -5.23477793e-04, ...,\n",
       "         -2.96950340e-04,  2.14695930e-04,  2.78651714e-04],\n",
       "        ...,\n",
       "        [ 1.86072455e-04,  4.39114039e-04, -2.75115162e-04, ...,\n",
       "         -5.03990304e-06, -3.88499764e-04, -5.09884616e-04],\n",
       "        [-1.31555833e-04,  1.92502514e-04, -3.88622284e-05, ...,\n",
       "          3.28840688e-04,  2.51419842e-04,  4.82164323e-05],\n",
       "        [ 1.73419714e-04, -8.10623169e-05,  4.49895859e-04, ...,\n",
       "         -2.47240067e-04,  2.50339508e-04, -2.73227692e-04]]),\n",
       " array([[-6.61059676e-06, -1.68411822e-04,  3.17973670e-04, ...,\n",
       "         -4.82007861e-04,  2.69981446e-04, -1.03833481e-05],\n",
       "        [ 1.56402588e-04, -1.25646591e-04,  5.43594360e-05, ...,\n",
       "         -1.58667564e-04,  7.25984573e-05, -6.77108765e-05],\n",
       "        [ 4.98443842e-06,  4.44650650e-05, -2.61738896e-04, ...,\n",
       "         -1.48475170e-04,  1.07347965e-04,  1.39325857e-04],\n",
       "        ...,\n",
       "        [ 9.30362276e-05,  2.19557020e-04, -1.37557581e-04, ...,\n",
       "         -2.51995152e-06, -1.94249882e-04, -2.54942308e-04],\n",
       "        [-6.57779165e-05,  9.62512568e-05, -1.94311142e-05, ...,\n",
       "          1.64420344e-04,  1.25709921e-04,  2.41082162e-05],\n",
       "        [ 8.67098570e-05, -4.05311584e-05,  2.24947929e-04, ...,\n",
       "         -1.23620033e-04,  1.25169754e-04, -1.36613846e-04]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5173\n",
      "  - \"computer\": 0.1732\n",
      "  - \"keyboard\": 0.1834\n",
      "  - \"plane\": 0.1833\n",
      "  - \"car\": 0.2153\n",
      "  - \"doctor\": 0.1292\n",
      "  - \"nurse\": 0.1594\n",
      "  - \"love\": 0.1406\n",
      "  - \"sex\": 0.1368\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0677\n",
      "  - \"keyboard\": 0.0654\n",
      "  - \"plane\": 0.1660\n",
      "  - \"car\": 0.1672\n",
      "  - \"doctor\": 0.0835\n",
      "  - \"nurse\": 0.1111\n",
      "  - \"love\": 0.0871\n",
      "  - \"sex\": 0.2222\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732\n",
      "  - \"tiger\": 0.0677\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.3964\n",
      "  - \"plane\": 0.1909\n",
      "  - \"car\": 0.2461\n",
      "  - \"doctor\": 0.1628\n",
      "  - \"nurse\": 0.2178\n",
      "  - \"love\": 0.0573\n",
      "  - \"sex\": 0.1853\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834\n",
      "  - \"tiger\": 0.0654\n",
      "  - \"computer\": 0.3964\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1006\n",
      "  - \"car\": 0.1498\n",
      "  - \"doctor\": 0.0850\n",
      "  - \"nurse\": 0.1220\n",
      "  - \"love\": 0.1591\n",
      "  - \"sex\": 0.0943\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833\n",
      "  - \"tiger\": 0.1660\n",
      "  - \"computer\": 0.1909\n",
      "  - \"keyboard\": 0.1006\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.3780\n",
      "  - \"doctor\": 0.1879\n",
      "  - \"nurse\": 0.0978\n",
      "  - \"love\": 0.1080\n",
      "  - \"sex\": 0.0587\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153\n",
      "  - \"tiger\": 0.1672\n",
      "  - \"computer\": 0.2461\n",
      "  - \"keyboard\": 0.1498\n",
      "  - \"plane\": 0.3780\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1895\n",
      "  - \"nurse\": 0.1306\n",
      "  - \"love\": 0.0842\n",
      "  - \"sex\": 0.1169\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292\n",
      "  - \"tiger\": 0.0835\n",
      "  - \"computer\": 0.1628\n",
      "  - \"keyboard\": 0.0850\n",
      "  - \"plane\": 0.1879\n",
      "  - \"car\": 0.1895\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6320\n",
      "  - \"love\": 0.0831\n",
      "  - \"sex\": 0.1994\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594\n",
      "  - \"tiger\": 0.1111\n",
      "  - \"computer\": 0.2178\n",
      "  - \"keyboard\": 0.1220\n",
      "  - \"plane\": 0.0978\n",
      "  - \"car\": 0.1306\n",
      "  - \"doctor\": 0.6320\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.0631\n",
      "  - \"sex\": 0.1997\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406\n",
      "  - \"tiger\": 0.0871\n",
      "  - \"computer\": 0.0573\n",
      "  - \"keyboard\": 0.1591\n",
      "  - \"plane\": 0.1080\n",
      "  - \"car\": 0.0842\n",
      "  - \"doctor\": 0.0831\n",
      "  - \"nurse\": 0.0631\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.2639\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368\n",
      "  - \"tiger\": 0.2222\n",
      "  - \"computer\": 0.1853\n",
      "  - \"keyboard\": 0.0943\n",
      "  - \"plane\": 0.0587\n",
      "  - \"car\": 0.1169\n",
      "  - \"doctor\": 0.1994\n",
      "  - \"nurse\": 0.1997\n",
      "  - \"love\": 0.2639\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrofitted_toy_matrix = convert_dict_to_matrix(retrofitted_toy_vecs)\n",
    "retrofitted_similarity_matrix = generate_cosine_similarity_matrix(toy_wordVecs)\n",
    "print_vec_similarities(toy_corpus, retrofitted_similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172961950302124"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrofitted_similarity_matrix[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5173 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1732 -> 0.1732 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1834 -> 0.1834 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1833 -> 0.1833 (Difference: 0.0000)\n",
      "  - \"car\": 0.2153 -> 0.2153 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1292 -> 0.1292 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1594 -> 0.1594 (Difference: 0.0000)\n",
      "  - \"love\": 0.1406 -> 0.1406 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1368 -> 0.1368 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5173 (Difference: 0.0000)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0677 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0654 -> 0.0654 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1660 -> 0.1660 (Difference: 0.0000)\n",
      "  - \"car\": 0.1672 -> 0.1672 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0835 -> 0.0835 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1111 -> 0.1111 (Difference: 0.0000)\n",
      "  - \"love\": 0.0871 -> 0.0871 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2222 -> 0.2222 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.1732 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0677 -> 0.0677 (Difference: 0.0000)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.3964 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1909 -> 0.1909 (Difference: 0.0000)\n",
      "  - \"car\": 0.2461 -> 0.2461 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1628 -> 0.1628 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.2178 -> 0.2178 (Difference: 0.0000)\n",
      "  - \"love\": 0.0573 -> 0.0573 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1853 -> 0.1853 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.1834 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0654 -> 0.0654 (Difference: 0.0000)\n",
      "  - \"computer\": 0.3964 -> 0.3964 (Difference: 0.0000)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1006 (Difference: 0.0000)\n",
      "  - \"car\": 0.1498 -> 0.1498 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0850 -> 0.0850 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1220 -> 0.1220 (Difference: 0.0000)\n",
      "  - \"love\": 0.1591 -> 0.1591 (Difference: 0.0000)\n",
      "  - \"sex\": 0.0943 -> 0.0943 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.1833 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1660 -> 0.1660 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1909 -> 0.1909 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1006 -> 0.1006 (Difference: 0.0000)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"car\": 0.3780 -> 0.3780 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1879 -> 0.1879 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.0978 -> 0.0978 (Difference: 0.0000)\n",
      "  - \"love\": 0.1080 -> 0.1080 (Difference: 0.0000)\n",
      "  - \"sex\": 0.0587 -> 0.0587 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2153 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1672 -> 0.1672 (Difference: 0.0000)\n",
      "  - \"computer\": 0.2461 -> 0.2461 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1498 -> 0.1498 (Difference: 0.0000)\n",
      "  - \"plane\": 0.3780 -> 0.3780 (Difference: 0.0000)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1895 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1306 -> 0.1306 (Difference: 0.0000)\n",
      "  - \"love\": 0.0842 -> 0.0842 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1169 -> 0.1169 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.1292 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0835 -> 0.0835 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1628 -> 0.1628 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0850 -> 0.0850 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1879 -> 0.1879 (Difference: 0.0000)\n",
      "  - \"car\": 0.1895 -> 0.1895 (Difference: 0.0000)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6320 (Difference: 0.0000)\n",
      "  - \"love\": 0.0831 -> 0.0831 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1994 -> 0.1994 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.1594 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1111 -> 0.1111 (Difference: 0.0000)\n",
      "  - \"computer\": 0.2178 -> 0.2178 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1220 -> 0.1220 (Difference: 0.0000)\n",
      "  - \"plane\": 0.0978 -> 0.0978 (Difference: 0.0000)\n",
      "  - \"car\": 0.1306 -> 0.1306 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.6320 -> 0.6320 (Difference: 0.0000)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.0631 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1997 -> 0.1997 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.1406 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0871 -> 0.0871 (Difference: 0.0000)\n",
      "  - \"computer\": 0.0573 -> 0.0573 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1591 -> 0.1591 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1080 -> 0.1080 (Difference: 0.0000)\n",
      "  - \"car\": 0.0842 -> 0.0842 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0831 -> 0.0831 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.0631 -> 0.0631 (Difference: 0.0000)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.2639 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.1368 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.2222 -> 0.2222 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1853 -> 0.1853 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0943 -> 0.0943 (Difference: 0.0000)\n",
      "  - \"plane\": 0.0587 -> 0.0587 (Difference: 0.0000)\n",
      "  - \"car\": 0.1169 -> 0.1169 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1994 -> 0.1994 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1997 -> 0.1997 (Difference: 0.0000)\n",
      "  - \"love\": 0.2639 -> 0.2639 (Difference: 0.0000)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "def print_vec_difference(wordList, similarity_matrix1, similarity_matrix2):\n",
    "    for i, word in enumerate(wordList):\n",
    "        print(f\"\\nSimilarities with \\\"{word}\\\":\")\n",
    "        for j, neighbor in enumerate(wordList):\n",
    "            similarity1 = similarity_matrix1[i, j]\n",
    "            similarity2 = similarity_matrix2[i, j]\n",
    "            difference = similarity2 - similarity1  # Calculate the difference\n",
    "            print(f\"  - \\\"{neighbor}\\\": {similarity1:.4f} -> {similarity2:.4f} (Difference: {difference:.4f})\")\n",
    "\n",
    "print_vec_difference(toy_corpus, similarity_matrix, retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Difference Matrix:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print_similarity_difference(similarity_matrix, retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between wordVecMat and retrofitted_toy_vec\n",
    "# similarity_score = cosine_similarity_matrix(wordVecMat, retrofitted_toy_vecs)\n",
    "# print(\"Cosine Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average embedding update: 0.12658860989283166\n"
     ]
    }
   ],
   "source": [
    "def measure_embedding_updates(original_matrix, retrofitted_matrix):\n",
    "    absolute_diff = np.abs(original_matrix - retrofitted_matrix)\n",
    "    mean_absolute_diff = np.mean(absolute_diff)\n",
    "    return mean_absolute_diff\n",
    "\n",
    "# Example usage\n",
    "update_measure = measure_embedding_updates(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Average embedding update:\", update_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation score: 0.42055888661904023\n",
      "Pearson correlation score: 0.4232245375810048\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def spearman_measure_embedding_similarity(original_matrix, retrofitted_matrix):\n",
    "    original_flat = original_matrix.flatten()\n",
    "    retrofitted_flat = retrofitted_matrix.flatten()\n",
    "    correlation, _ = spearmanr(original_flat, retrofitted_flat)\n",
    "    return correlation\n",
    "\n",
    "def pearson_measure_embedding_similarity(original_matrix, retrofitted_matrix):\n",
    "    original_flat = original_matrix.flatten()\n",
    "    retrofitted_flat = retrofitted_matrix.flatten()\n",
    "    correlation, _ = pearsonr(original_flat, retrofitted_flat)\n",
    "    return correlation\n",
    "\n",
    "similarity_score = spearman_measure_embedding_similarity(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Spearman correlation score:\", similarity_score)\n",
    "similarity_score = pearson_measure_embedding_similarity(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Pearson correlation score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.1, 'beta': 0.1, 'nb_iter': 1}\n",
      "Best Spearman correlation score: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load human evaluation scores\n",
    "eval_file_path = r\"C:\\Users\\ninan\\OneDrive\\Bureau\\Université Paris Cité\\S2\\NLP project\\Improving-vector-space-representations-using-semantic-resources\\data\\English\\lexicon\\ws353_lexical_similarity.txt\"\n",
    "eval_scores = {}\n",
    "with open(eval_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        word1, word2, score = line.strip().split('\\t')\n",
    "        eval_scores[(word1, word2)] = float(score)\n",
    "\n",
    "# Find best values for hyperparameters\n",
    "best_similarity_score = -1  # Variable to store the best similarity score\n",
    "best_params = {}  # Dictionary to store the best hyperparameter values\n",
    "iteration_count = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1, 16):\n",
    "            retrofitted_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "            cosine_sim = cosine_similarity(wordVecMat, retrofitted_toy_vec)\n",
    "\n",
    "            # Calculate Spearman correlation against human evaluation scores\n",
    "            eval_scores_list = []\n",
    "            cosine_sim_list = []\n",
    "            for (word1, word2), score in eval_scores.items():\n",
    "                if word1 in wordList and word2 in wordList:\n",
    "                    word1_index = wordList.index(word1)\n",
    "                    word2_index = wordList.index(word2)\n",
    "                    eval_scores_list.append(score)\n",
    "                    cosine_sim_list.append(cosine_sim[word1_index, word2_index])\n",
    "\n",
    "            # Check if there are valid pairs for comparison\n",
    "            if len(eval_scores_list) > 0 and len(cosine_sim_list) > 0:\n",
    "                correlation, _ = spearmanr(eval_scores_list, cosine_sim_list)\n",
    "                # print(\"alpha =\", alpha, \"beta =\", beta, \"nb_iter =\", nb_iter, \"correlation =\", correlation)\n",
    "\n",
    "                # Update best similarity score and parameters if improved\n",
    "                if correlation > best_similarity_score:\n",
    "                    best_similarity_score = correlation\n",
    "                    best_params = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "            iteration_count += 1\n",
    "            if iteration_count >= 100:\n",
    "                break\n",
    "        if iteration_count >= 100:\n",
    "            break\n",
    "    if iteration_count >= 100:\n",
    "        break\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best Spearman correlation score:\", best_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.1, 'beta': 1.1000000000000003, 'nb_iter': 15}\n",
      "Best embedding update: 0.12671235242449622\n"
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update = -1  # Variable to store the best similarity score\n",
    "best_params = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "            embed_update = measure_embedding_updates(wordVecMat, retrofitted_toy_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update > best_embed_update:\n",
    "                best_embed_update = embed_update\n",
    "                best_params = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best embedding update:\", best_embed_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5134\n",
      "  - \"computer\": 0.2147\n",
      "  - \"keyboard\": 0.2270\n",
      "  - \"plane\": 0.2878\n",
      "  - \"car\": 0.2492\n",
      "  - \"doctor\": 0.2437\n",
      "  - \"nurse\": 0.3233\n",
      "  - \"love\": 0.3254\n",
      "  - \"sex\": 0.2202\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5134\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0783\n",
      "  - \"keyboard\": 0.1118\n",
      "  - \"plane\": 0.1769\n",
      "  - \"car\": 0.1518\n",
      "  - \"doctor\": 0.0866\n",
      "  - \"nurse\": 0.1714\n",
      "  - \"love\": 0.1518\n",
      "  - \"sex\": 0.2417\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.2147\n",
      "  - \"tiger\": 0.0783\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.4058\n",
      "  - \"plane\": 0.2883\n",
      "  - \"car\": 0.3039\n",
      "  - \"doctor\": 0.2093\n",
      "  - \"nurse\": 0.2180\n",
      "  - \"love\": 0.1358\n",
      "  - \"sex\": 0.1602\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.2270\n",
      "  - \"tiger\": 0.1118\n",
      "  - \"computer\": 0.4058\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1872\n",
      "  - \"car\": 0.1700\n",
      "  - \"doctor\": 0.1131\n",
      "  - \"nurse\": 0.1637\n",
      "  - \"love\": 0.2198\n",
      "  - \"sex\": 0.1140\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.2878\n",
      "  - \"tiger\": 0.1769\n",
      "  - \"computer\": 0.2883\n",
      "  - \"keyboard\": 0.1872\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.4366\n",
      "  - \"doctor\": 0.2202\n",
      "  - \"nurse\": 0.1756\n",
      "  - \"love\": 0.1935\n",
      "  - \"sex\": 0.0994\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2492\n",
      "  - \"tiger\": 0.1518\n",
      "  - \"computer\": 0.3039\n",
      "  - \"keyboard\": 0.1700\n",
      "  - \"plane\": 0.4366\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1865\n",
      "  - \"nurse\": 0.1453\n",
      "  - \"love\": 0.1766\n",
      "  - \"sex\": 0.1273\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.2437\n",
      "  - \"tiger\": 0.0866\n",
      "  - \"computer\": 0.2093\n",
      "  - \"keyboard\": 0.1131\n",
      "  - \"plane\": 0.2202\n",
      "  - \"car\": 0.1865\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6105\n",
      "  - \"love\": 0.1844\n",
      "  - \"sex\": 0.1923\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.3233\n",
      "  - \"tiger\": 0.1714\n",
      "  - \"computer\": 0.2180\n",
      "  - \"keyboard\": 0.1637\n",
      "  - \"plane\": 0.1756\n",
      "  - \"car\": 0.1453\n",
      "  - \"doctor\": 0.6105\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.2675\n",
      "  - \"sex\": 0.3084\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.3254\n",
      "  - \"tiger\": 0.1518\n",
      "  - \"computer\": 0.1358\n",
      "  - \"keyboard\": 0.2198\n",
      "  - \"plane\": 0.1935\n",
      "  - \"car\": 0.1766\n",
      "  - \"doctor\": 0.1844\n",
      "  - \"nurse\": 0.2675\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.3760\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.2202\n",
      "  - \"tiger\": 0.2417\n",
      "  - \"computer\": 0.1602\n",
      "  - \"keyboard\": 0.1140\n",
      "  - \"plane\": 0.0994\n",
      "  - \"car\": 0.1273\n",
      "  - \"doctor\": 0.1923\n",
      "  - \"nurse\": 0.3084\n",
      "  - \"love\": 0.3760\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_retrofitted_toy_matrix, new_updates = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha=0.1, beta=0.1, nb_iter=1)\n",
    "new_retrofitted_toy_dict = convert_matrix_to_dict(new_retrofitted_toy_matrix, wordList)\n",
    "new_retrofitted_similarity_matrix = generate_cosine_similarity_matrix(new_retrofitted_toy_dict)\n",
    "print_vec_similarities(toy_corpus, new_retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"computer\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"keyboard\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"plane\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"car\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"doctor\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"nurse\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"love\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"sex\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"keyboard\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"plane\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"car\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"doctor\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"nurse\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"love\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"sex\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"tiger\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"plane\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"car\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"doctor\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"nurse\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"love\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"sex\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"tiger\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"computer\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"car\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"doctor\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"nurse\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"love\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"sex\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"tiger\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"computer\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"keyboard\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"car\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"doctor\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"nurse\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"love\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"sex\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"tiger\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"computer\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"keyboard\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"plane\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"nurse\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"love\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"sex\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"tiger\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"computer\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"keyboard\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"plane\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"car\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"love\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"sex\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"tiger\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"computer\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"keyboard\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"plane\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"car\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"doctor\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"sex\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"tiger\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"computer\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"keyboard\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"plane\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"car\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"doctor\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"nurse\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "  - \"tiger\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "  - \"computer\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "  - \"keyboard\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "  - \"plane\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "  - \"car\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "  - \"doctor\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "  - \"nurse\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "  - \"love\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and after tuning hyperparam\n",
    "print_vec_difference(toy_corpus, similarity_matrix, new_retrofitted_similarity_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"computer\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"keyboard\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"plane\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"car\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"doctor\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"nurse\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"love\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"sex\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"keyboard\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"plane\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"car\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"doctor\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"nurse\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"love\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"sex\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"tiger\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"plane\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"car\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"doctor\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"nurse\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"love\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"sex\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"tiger\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"computer\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"car\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"doctor\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"nurse\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"love\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"sex\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"tiger\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"computer\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"keyboard\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"car\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"doctor\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"nurse\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"love\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"sex\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"tiger\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"computer\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"keyboard\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"plane\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"nurse\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"love\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"sex\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"tiger\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"computer\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"keyboard\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"plane\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"car\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"love\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"sex\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"tiger\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"computer\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"keyboard\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"plane\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"car\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"doctor\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"sex\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"tiger\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"computer\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"keyboard\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"plane\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"car\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"doctor\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"nurse\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "  - \"tiger\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "  - \"computer\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "  - \"keyboard\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "  - \"plane\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "  - \"car\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "  - \"doctor\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "  - \"nurse\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "  - \"love\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Difference between retrofitted embeddings and after tuning hyperaparams\n",
    "print_vec_difference(toy_corpus, retrofitted_similarity_matrix, new_retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>tiger</th>\n",
       "      <th>computer</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>plane</th>\n",
       "      <th>car</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "      <th>love</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>Before: 0.29245239862991185, After: 1.00000000...</td>\n",
       "      <td>Before: 0.35037322527996934, After: 0.20200897...</td>\n",
       "      <td>Before: 0.06136659928459301, After: 0.19719178...</td>\n",
       "      <td>Before: 0.18344955625364173, After: 0.20547455...</td>\n",
       "      <td>Before: 0.21134097025538556, After: 0.39445950...</td>\n",
       "      <td>Before: 0.136072027917602, After: 0.2634861035...</td>\n",
       "      <td>Before: 0.16547475026405636, After: 0.34338708...</td>\n",
       "      <td>Before: 0.24690899138830727, After: 0.54854860...</td>\n",
       "      <td>Before: 0.2989067535773432, After: 0.594053102...</td>\n",
       "      <td>Before: 0.1093022564011111, After: 0.259890845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>Before: 0.17347637872059773, After: 0.20200897...</td>\n",
       "      <td>Before: 0.48802072485209846, After: 1.00000000...</td>\n",
       "      <td>Before: 0.02942476361382193, After: 0.09025702...</td>\n",
       "      <td>Before: 0.06542581824273716, After: 0.18313975...</td>\n",
       "      <td>Before: 0.15090617196611955, After: 0.07307479...</td>\n",
       "      <td>Before: 0.16119598769364896, After: 0.10033040...</td>\n",
       "      <td>Before: 0.0774108002811773, After: 0.089083542...</td>\n",
       "      <td>Before: 0.1697249630498177, After: 0.202597136...</td>\n",
       "      <td>Before: 0.17516455047450735, After: 0.19102605...</td>\n",
       "      <td>Before: 0.14518818082220147, After: 0.18807845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>Before: 0.18984798734723213, After: 0.19719178...</td>\n",
       "      <td>Before: 0.05360514929979338, After: 0.09025702...</td>\n",
       "      <td>Before: 0.3047689400402037, After: 1.0</td>\n",
       "      <td>Before: 0.39639163439495995, After: 0.23934215...</td>\n",
       "      <td>Before: 0.28314903703275535, After: 0.40858220...</td>\n",
       "      <td>Before: 0.26826894486108244, After: 0.22732308...</td>\n",
       "      <td>Before: 0.18039329196329013, After: 0.20986951...</td>\n",
       "      <td>Before: 0.09297679298707379, After: 0.12068024...</td>\n",
       "      <td>Before: 0.1168711356729625, After: 0.234808759...</td>\n",
       "      <td>Before: 0.1158779845883439, After: 0.076953257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>Before: 0.20546589125510897, After: 0.20547455...</td>\n",
       "      <td>Before: 0.18314156317766062, After: 0.18313975...</td>\n",
       "      <td>Before: 0.2393287663091698, After: 0.239342157...</td>\n",
       "      <td>Before: 0.9999999999999996, After: 1.0</td>\n",
       "      <td>Before: 0.28971014677665063, After: 0.28970944...</td>\n",
       "      <td>Before: 0.15750632650840493, After: 0.15750846...</td>\n",
       "      <td>Before: 0.13804955762057758, After: 0.13804958...</td>\n",
       "      <td>Before: 0.16678031434047627, After: 0.16678347...</td>\n",
       "      <td>Before: 0.27407501845767246, After: 0.27407454...</td>\n",
       "      <td>Before: 0.10639718565916091, After: 0.10639914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>Before: 0.16761576142114898, After: 0.39445950...</td>\n",
       "      <td>Before: 0.06730278214710199, After: 0.07307479...</td>\n",
       "      <td>Before: 0.06606645196552532, After: 0.40858220...</td>\n",
       "      <td>Before: 0.10055138151211143, After: 0.28970944...</td>\n",
       "      <td>Before: 0.38405259310022044, After: 1.00000000...</td>\n",
       "      <td>Before: 0.3219357387657039, After: 0.354037032...</td>\n",
       "      <td>Before: 0.15146728859985834, After: 0.21833826...</td>\n",
       "      <td>Before: 0.10589313234551631, After: 0.28236372...</td>\n",
       "      <td>Before: 0.1936940498815496, After: 0.328525840...</td>\n",
       "      <td>Before: 0.05697047025775614, After: 0.11862340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>Before: 0.15859645192729996, After: 0.26348610...</td>\n",
       "      <td>Before: -0.049076379792710186, After: 0.100330...</td>\n",
       "      <td>Before: 0.13968323112249992, After: 0.22732308...</td>\n",
       "      <td>Before: 0.14983822223318854, After: 0.15750846...</td>\n",
       "      <td>Before: 0.2597415410728325, After: 0.354037032...</td>\n",
       "      <td>Before: 0.6158574248530795, After: 1.000000000...</td>\n",
       "      <td>Before: 0.13778205919388814, After: 0.19190468...</td>\n",
       "      <td>Before: 0.0865221800713605, After: 0.160778501...</td>\n",
       "      <td>Before: 0.22133439390681858, After: 0.24947266...</td>\n",
       "      <td>Before: 0.059270287407368415, After: 0.0662935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>Before: 0.25878907406446877, After: 0.34338708...</td>\n",
       "      <td>Before: 0.017639064485218965, After: 0.0890835...</td>\n",
       "      <td>Before: 0.0978054983995635, After: 0.209869514...</td>\n",
       "      <td>Before: 0.08500327165730943, After: 0.13804958...</td>\n",
       "      <td>Before: 0.13439994429915442, After: 0.21833826...</td>\n",
       "      <td>Before: 0.09431570687955, After: 0.19190468704...</td>\n",
       "      <td>Before: 0.6128107065755533, After: 1.0</td>\n",
       "      <td>Before: 0.27845097194129365, After: 0.34448646...</td>\n",
       "      <td>Before: 0.19325643013428553, After: 0.33078374...</td>\n",
       "      <td>Before: 0.0745189116843016, After: 0.175547118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>Before: 0.18217682480604822, After: 0.54854860...</td>\n",
       "      <td>Before: 0.07222178145580241, After: 0.20259713...</td>\n",
       "      <td>Before: 0.11697573887301807, After: 0.12068024...</td>\n",
       "      <td>Before: 0.12199094008346709, After: 0.16678347...</td>\n",
       "      <td>Before: 0.12790409339777273, After: 0.28236372...</td>\n",
       "      <td>Before: 0.07363428182232754, After: 0.16077850...</td>\n",
       "      <td>Before: 0.4308149649767604, After: 0.344486463...</td>\n",
       "      <td>Before: 0.378466332225932, After: 1.0000000000...</td>\n",
       "      <td>Before: 0.1708752362535986, After: 0.544714533...</td>\n",
       "      <td>Before: 0.1264203000118017, After: 0.409129275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>Before: 0.241072229246228, After: 0.5940531024...</td>\n",
       "      <td>Before: 0.10199943514005788, After: 0.19102605...</td>\n",
       "      <td>Before: 0.0730022470894344, After: 0.234808759...</td>\n",
       "      <td>Before: 0.15911448638969528, After: 0.27407454...</td>\n",
       "      <td>Before: 0.09505575751009387, After: 0.32852584...</td>\n",
       "      <td>Before: 0.11200247669649024, After: 0.24947266...</td>\n",
       "      <td>Before: 0.14771102908578468, After: 0.33078374...</td>\n",
       "      <td>Before: 0.3033847603340788, After: 0.544714533...</td>\n",
       "      <td>Before: 0.6110191309495465, After: 0.999999999...</td>\n",
       "      <td>Before: 0.30399420253175347, After: 0.36164712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>Before: 0.225917972105598, After: 0.2598908452...</td>\n",
       "      <td>Before: 0.1655264826766013, After: 0.188078452...</td>\n",
       "      <td>Before: 0.036750539503579295, After: 0.0769532...</td>\n",
       "      <td>Before: 0.09429740737651135, After: 0.10639914...</td>\n",
       "      <td>Before: 0.10212970436490482, After: 0.11862340...</td>\n",
       "      <td>Before: 0.13403080874265905, After: 0.06629354...</td>\n",
       "      <td>Before: 0.1436405909297276, After: 0.175547118...</td>\n",
       "      <td>Before: 0.26868181343357167, After: 0.40912927...</td>\n",
       "      <td>Before: 0.3049306009871644, After: 0.361647126...</td>\n",
       "      <td>Before: 0.4896246955281065, After: 1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        cat  \\\n",
       "cat       Before: 0.29245239862991185, After: 1.00000000...   \n",
       "tiger     Before: 0.17347637872059773, After: 0.20200897...   \n",
       "computer  Before: 0.18984798734723213, After: 0.19719178...   \n",
       "keyboard  Before: 0.20546589125510897, After: 0.20547455...   \n",
       "plane     Before: 0.16761576142114898, After: 0.39445950...   \n",
       "car       Before: 0.15859645192729996, After: 0.26348610...   \n",
       "doctor    Before: 0.25878907406446877, After: 0.34338708...   \n",
       "nurse     Before: 0.18217682480604822, After: 0.54854860...   \n",
       "love      Before: 0.241072229246228, After: 0.5940531024...   \n",
       "sex       Before: 0.225917972105598, After: 0.2598908452...   \n",
       "\n",
       "                                                      tiger  \\\n",
       "cat       Before: 0.35037322527996934, After: 0.20200897...   \n",
       "tiger     Before: 0.48802072485209846, After: 1.00000000...   \n",
       "computer  Before: 0.05360514929979338, After: 0.09025702...   \n",
       "keyboard  Before: 0.18314156317766062, After: 0.18313975...   \n",
       "plane     Before: 0.06730278214710199, After: 0.07307479...   \n",
       "car       Before: -0.049076379792710186, After: 0.100330...   \n",
       "doctor    Before: 0.017639064485218965, After: 0.0890835...   \n",
       "nurse     Before: 0.07222178145580241, After: 0.20259713...   \n",
       "love      Before: 0.10199943514005788, After: 0.19102605...   \n",
       "sex       Before: 0.1655264826766013, After: 0.188078452...   \n",
       "\n",
       "                                                   computer  \\\n",
       "cat       Before: 0.06136659928459301, After: 0.19719178...   \n",
       "tiger     Before: 0.02942476361382193, After: 0.09025702...   \n",
       "computer             Before: 0.3047689400402037, After: 1.0   \n",
       "keyboard  Before: 0.2393287663091698, After: 0.239342157...   \n",
       "plane     Before: 0.06606645196552532, After: 0.40858220...   \n",
       "car       Before: 0.13968323112249992, After: 0.22732308...   \n",
       "doctor    Before: 0.0978054983995635, After: 0.209869514...   \n",
       "nurse     Before: 0.11697573887301807, After: 0.12068024...   \n",
       "love      Before: 0.0730022470894344, After: 0.234808759...   \n",
       "sex       Before: 0.036750539503579295, After: 0.0769532...   \n",
       "\n",
       "                                                   keyboard  \\\n",
       "cat       Before: 0.18344955625364173, After: 0.20547455...   \n",
       "tiger     Before: 0.06542581824273716, After: 0.18313975...   \n",
       "computer  Before: 0.39639163439495995, After: 0.23934215...   \n",
       "keyboard             Before: 0.9999999999999996, After: 1.0   \n",
       "plane     Before: 0.10055138151211143, After: 0.28970944...   \n",
       "car       Before: 0.14983822223318854, After: 0.15750846...   \n",
       "doctor    Before: 0.08500327165730943, After: 0.13804958...   \n",
       "nurse     Before: 0.12199094008346709, After: 0.16678347...   \n",
       "love      Before: 0.15911448638969528, After: 0.27407454...   \n",
       "sex       Before: 0.09429740737651135, After: 0.10639914...   \n",
       "\n",
       "                                                      plane  \\\n",
       "cat       Before: 0.21134097025538556, After: 0.39445950...   \n",
       "tiger     Before: 0.15090617196611955, After: 0.07307479...   \n",
       "computer  Before: 0.28314903703275535, After: 0.40858220...   \n",
       "keyboard  Before: 0.28971014677665063, After: 0.28970944...   \n",
       "plane     Before: 0.38405259310022044, After: 1.00000000...   \n",
       "car       Before: 0.2597415410728325, After: 0.354037032...   \n",
       "doctor    Before: 0.13439994429915442, After: 0.21833826...   \n",
       "nurse     Before: 0.12790409339777273, After: 0.28236372...   \n",
       "love      Before: 0.09505575751009387, After: 0.32852584...   \n",
       "sex       Before: 0.10212970436490482, After: 0.11862340...   \n",
       "\n",
       "                                                        car  \\\n",
       "cat       Before: 0.136072027917602, After: 0.2634861035...   \n",
       "tiger     Before: 0.16119598769364896, After: 0.10033040...   \n",
       "computer  Before: 0.26826894486108244, After: 0.22732308...   \n",
       "keyboard  Before: 0.15750632650840493, After: 0.15750846...   \n",
       "plane     Before: 0.3219357387657039, After: 0.354037032...   \n",
       "car       Before: 0.6158574248530795, After: 1.000000000...   \n",
       "doctor    Before: 0.09431570687955, After: 0.19190468704...   \n",
       "nurse     Before: 0.07363428182232754, After: 0.16077850...   \n",
       "love      Before: 0.11200247669649024, After: 0.24947266...   \n",
       "sex       Before: 0.13403080874265905, After: 0.06629354...   \n",
       "\n",
       "                                                     doctor  \\\n",
       "cat       Before: 0.16547475026405636, After: 0.34338708...   \n",
       "tiger     Before: 0.0774108002811773, After: 0.089083542...   \n",
       "computer  Before: 0.18039329196329013, After: 0.20986951...   \n",
       "keyboard  Before: 0.13804955762057758, After: 0.13804958...   \n",
       "plane     Before: 0.15146728859985834, After: 0.21833826...   \n",
       "car       Before: 0.13778205919388814, After: 0.19190468...   \n",
       "doctor               Before: 0.6128107065755533, After: 1.0   \n",
       "nurse     Before: 0.4308149649767604, After: 0.344486463...   \n",
       "love      Before: 0.14771102908578468, After: 0.33078374...   \n",
       "sex       Before: 0.1436405909297276, After: 0.175547118...   \n",
       "\n",
       "                                                      nurse  \\\n",
       "cat       Before: 0.24690899138830727, After: 0.54854860...   \n",
       "tiger     Before: 0.1697249630498177, After: 0.202597136...   \n",
       "computer  Before: 0.09297679298707379, After: 0.12068024...   \n",
       "keyboard  Before: 0.16678031434047627, After: 0.16678347...   \n",
       "plane     Before: 0.10589313234551631, After: 0.28236372...   \n",
       "car       Before: 0.0865221800713605, After: 0.160778501...   \n",
       "doctor    Before: 0.27845097194129365, After: 0.34448646...   \n",
       "nurse     Before: 0.378466332225932, After: 1.0000000000...   \n",
       "love      Before: 0.3033847603340788, After: 0.544714533...   \n",
       "sex       Before: 0.26868181343357167, After: 0.40912927...   \n",
       "\n",
       "                                                       love  \\\n",
       "cat       Before: 0.2989067535773432, After: 0.594053102...   \n",
       "tiger     Before: 0.17516455047450735, After: 0.19102605...   \n",
       "computer  Before: 0.1168711356729625, After: 0.234808759...   \n",
       "keyboard  Before: 0.27407501845767246, After: 0.27407454...   \n",
       "plane     Before: 0.1936940498815496, After: 0.328525840...   \n",
       "car       Before: 0.22133439390681858, After: 0.24947266...   \n",
       "doctor    Before: 0.19325643013428553, After: 0.33078374...   \n",
       "nurse     Before: 0.1708752362535986, After: 0.544714533...   \n",
       "love      Before: 0.6110191309495465, After: 0.999999999...   \n",
       "sex       Before: 0.3049306009871644, After: 0.361647126...   \n",
       "\n",
       "                                                        sex  \n",
       "cat       Before: 0.1093022564011111, After: 0.259890845...  \n",
       "tiger     Before: 0.14518818082220147, After: 0.18807845...  \n",
       "computer  Before: 0.1158779845883439, After: 0.076953257...  \n",
       "keyboard  Before: 0.10639718565916091, After: 0.10639914...  \n",
       "plane     Before: 0.05697047025775614, After: 0.11862340...  \n",
       "car       Before: 0.059270287407368415, After: 0.0662935...  \n",
       "doctor    Before: 0.0745189116843016, After: 0.175547118...  \n",
       "nurse     Before: 0.1264203000118017, After: 0.409129275...  \n",
       "love      Before: 0.30399420253175347, After: 0.36164712...  \n",
       "sex                  Before: 0.4896246955281065, After: 1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(index=wordList, columns=wordList)\n",
    "\n",
    "# Loop over each word pair and calculate similarity scores\n",
    "for word1 in wordList:\n",
    "    for word2 in wordList:\n",
    "        word1_index = wordList.index(word1)\n",
    "        word2_index = wordList.index(word2)\n",
    "        \n",
    "        # Calculate similarity score before retrofitting\n",
    "        similarity_before = cosine_sim[word1_index, word2_index]\n",
    "        \n",
    "        # Calculate similarity score after retrofitting\n",
    "        retrofit_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "        word1_vec = retrofit_toy_vec[word1_index].reshape(1, -1)\n",
    "        word2_vec = retrofit_toy_vec[word2_index].reshape(1, -1)\n",
    "        similarity_after = cosine_similarity(word1_vec, word2_vec)[0, 0]\n",
    "        \n",
    "        # Store the scores in the DataFrame\n",
    "        results_df.loc[word1, word2] = f\"Before: {similarity_before}, After: {similarity_after}\"\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>tiger</th>\n",
       "      <th>computer</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>plane</th>\n",
       "      <th>car</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "      <th>love</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>Retrofitting: 0.20, Human: 0.73</td>\n",
       "      <td>Retrofitting: 1.00, Human: 1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.24, Human: 0.76</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.35, Human: 0.58</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.34, Human: 0.70</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.36, Human: 0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cat                            tiger  \\\n",
       "cat                                  None                             None   \n",
       "tiger     Retrofitting: 0.20, Human: 0.73  Retrofitting: 1.00, Human: 1.00   \n",
       "computer                             None                             None   \n",
       "keyboard                             None                             None   \n",
       "plane                                None                             None   \n",
       "car                                  None                             None   \n",
       "doctor                               None                             None   \n",
       "nurse                                None                             None   \n",
       "love                                 None                             None   \n",
       "sex                                  None                             None   \n",
       "\n",
       "         computer                         keyboard plane  \\\n",
       "cat          None                             None  None   \n",
       "tiger        None                             None  None   \n",
       "computer     None  Retrofitting: 0.24, Human: 0.76  None   \n",
       "keyboard     None                             None  None   \n",
       "plane        None                             None  None   \n",
       "car          None                             None  None   \n",
       "doctor       None                             None  None   \n",
       "nurse        None                             None  None   \n",
       "love         None                             None  None   \n",
       "sex          None                             None  None   \n",
       "\n",
       "                                      car doctor  \\\n",
       "cat                                  None   None   \n",
       "tiger                                None   None   \n",
       "computer                             None   None   \n",
       "keyboard                             None   None   \n",
       "plane     Retrofitting: 0.35, Human: 0.58   None   \n",
       "car                                  None   None   \n",
       "doctor                               None   None   \n",
       "nurse                                None   None   \n",
       "love                                 None   None   \n",
       "sex                                  None   None   \n",
       "\n",
       "                                    nurse  love  \\\n",
       "cat                                  None  None   \n",
       "tiger                                None  None   \n",
       "computer                             None  None   \n",
       "keyboard                             None  None   \n",
       "plane                                None  None   \n",
       "car                                  None  None   \n",
       "doctor    Retrofitting: 0.34, Human: 0.70  None   \n",
       "nurse                                None  None   \n",
       "love                                 None  None   \n",
       "sex                                  None  None   \n",
       "\n",
       "                                      sex  \n",
       "cat                                  None  \n",
       "tiger                                None  \n",
       "computer                             None  \n",
       "keyboard                             None  \n",
       "plane                                None  \n",
       "car                                  None  \n",
       "doctor                               None  \n",
       "nurse                                None  \n",
       "love      Retrofitting: 0.36, Human: 0.68  \n",
       "sex                                  None  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the evaluation scores from the file\n",
    "eval_scores = {}\n",
    "with open(eval_file_path, 'r') as eval_file:\n",
    "    for line in eval_file:\n",
    "        word1, word2, score = line.strip().split('\\t')\n",
    "        eval_scores[(word1, word2)] = float(score)\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(index=wordList, columns=wordList)\n",
    "\n",
    "# Loop over each word pair and calculate similarity scores\n",
    "for word1 in wordList:\n",
    "    for word2 in wordList:\n",
    "        word1_index = wordList.index(word1)\n",
    "        word2_index = wordList.index(word2)\n",
    "        \n",
    "        # Calculate similarity score after retrofitting\n",
    "        retrofit_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "        word1_vec = retrofit_toy_vec[word1_index].reshape(1, -1)\n",
    "        word2_vec = retrofit_toy_vec[word2_index].reshape(1, -1)\n",
    "        similarity_after = cosine_similarity(word1_vec, word2_vec)[0, 0]\n",
    "        \n",
    "        # Retrieve the evaluation score for the word pair\n",
    "        score = eval_scores.get((word1, word2))\n",
    "\n",
    "        # Scale the human score between 0 and 1\n",
    "        if score is not None:\n",
    "            scaled_score = score / 10.0\n",
    "        else:\n",
    "            scaled_score = None\n",
    "        \n",
    "        # Store the scores in the DataFrame\n",
    "        results_df.loc[word1, word2] = f\"Retrofitting: {similarity_after:.2f}, Human: {scaled_score:.2f}\" if scaled_score is not None else None\n",
    "\n",
    "# Print the results DataFrame\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean \n"
     ]
    }
   ],
   "source": [
    "wordVecs_gensim = read_word_vecs(\"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\")\n",
    "lexical_similarity = read_lexicon(\"../data/English/lexicon/ws353_lexical_similarity.txt\")\n",
    "output_file_gensim = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "outFileName_gensim = output_file_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_neighbors_embedding_matrix0(wordVecs, wordList, relation_type):\n",
    "    # Retrieve synonyms for each word\n",
    "    neighbors_dict = get_wordnet_lexicon(wordList, relation_type)\n",
    "\n",
    "    # Create a set of valid neighbors\n",
    "    valid_neighbors = set(neighbor for neighbors in neighbors_dict.values() for neighbor in neighbors) & set(wordList)\n",
    "    \n",
    "    # Get the embedding size\n",
    "    embedding_size = 250 #wordVecs[next(iter(wordVecs))].shape[0]\n",
    "    \n",
    "    # Compute average embedding\n",
    "    average_embeddings = []\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        if neighbors and any(neighbor in valid_neighbors for neighbor in neighbors):\n",
    "            embeddings = np.array([\n",
    "                wordVecs[wordList.index(neighbor)]\n",
    "                for neighbor in neighbors\n",
    "                if neighbor in valid_neighbors\n",
    "            ])\n",
    "            average_embedding = np.mean(embeddings, axis=0)\n",
    "            average_embeddings.append(average_embedding)\n",
    "    \n",
    "    # Create the word embedding matrix\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "\n",
    "    return neighbors_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_dict = get_wordnet_lexicon(wordList, \"synononys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_neighbors_embedding_matrix(wordVecMat, wordList, neighbors_dict):\n",
    "    valid_neighbors = set(neighbor for neighbors in neighbors_dict.values() for neighbor in neighbors) & set(wordList)\n",
    "\n",
    "    embedding_size = wordVecMat.shape[1]\n",
    "    average_embeddings = []\n",
    "\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        if neighbors and any(neighbor in valid_neighbors for neighbor in neighbors):\n",
    "            embeddings = np.array([\n",
    "                wordVecMat[wordList.index(neighbor)] if neighbor in wordList else np.zeros(embedding_size)\n",
    "                for neighbor in neighbors\n",
    "                if neighbor in valid_neighbors\n",
    "            ])\n",
    "            if embeddings.size > 0:\n",
    "                average_embedding = np.mean(embeddings, axis=0)\n",
    "                average_embeddings.append(average_embedding)\n",
    "        else:\n",
    "            average_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "    return neighbors_embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_gensim = get_embeddings_words(wordVecs_gensim)\n",
    "wordVecMat_gensim = convert_dict_to_matrix(wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix_gensim = retrieve_neighbors_embedding_matrix(wordVecMat_gensim, wordList_gensim, neighbors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create a small subset of wordVecs dictionary\n",
    "subset_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:100]}\n",
    "# subset_neighbors_dict = {word: neighbors_dict[word] for word in neighbors_dict[:100]}\n",
    "subset_wordVecMat = wordVecMat_gensim[:100] \n",
    "\n",
    "# Create a small subset of wordList\n",
    "subset_wordList = wordList_gensim[:100]\n",
    "\n",
    "# Test the function on the subset\n",
    "neighbors_matrix = retrieve_neighbors_embedding_matrix(subset_wordVecMat, subset_wordList, neighbors_dict)\n",
    "\n",
    "# Print the result\n",
    "print(neighbors_matrix)\n",
    "print(type(neighbors_matrix))  \n",
    "print(neighbors_matrix.shape)  \n",
    "print(neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(125776, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(neighbors_matrix_gensim))  \n",
    "print(neighbors_matrix_gensim.shape)  \n",
    "print(neighbors_matrix_gensim.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(125776, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat_gensim))  \n",
    "print(wordVecMat_gensim.shape) \n",
    "print(wordVecMat_gensim.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat))  \n",
    "print(wordVecMat.shape) \n",
    "print(wordVecMat.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_gensim = get_wordnet_lexicon(wordList_gensim, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix_gensim = generate_cosine_similarity_matrix(wordVecs_gensim)\n",
    "# retrofitted_similarity_matrix_gensim = generate_cosine_similarity_matrix(wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs_test(wordVecMat, neighbors_embedding_matrix, alpha=1, beta=1, nb_iter=10):\n",
    "    newWordVecMat = np.copy(wordVecMat)\n",
    "    for _ in range(nb_iter):\n",
    "        updates = alpha * neighbors_embedding_matrix + beta * newWordVecMat\n",
    "        newWordVecMat = updates / (alpha + beta)\n",
    "\n",
    "    return newWordVecMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_subset_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:50]}\n",
    "test_subset_wordVecMat = wordVecMat_gensim[:50] \n",
    "\n",
    "test_subset_wordList = wordList_gensim[:50]\n",
    "\n",
    "test_subset_neighbors_matrix= retrieve_neighbors_embedding_matrix(test_subset_wordVecMat, test_subset_wordList, neighbors_dict)\n",
    "\n",
    "print(test_subset_neighbors_matrix)\n",
    "print(type(test_subset_neighbors_matrix))  \n",
    "print(test_subset_neighbors_matrix.shape)  \n",
    "print(test_subset_neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \",\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"the\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \".\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"of\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"-\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"and\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"in\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"to\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"'\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"a\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \")\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"(\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"is\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"s\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"for\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"was\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"on\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"that\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"as\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"it\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"with\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"by\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"\"\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"at\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"he\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"from\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"be\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"this\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"i\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"an\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"his\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"are\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"not\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"has\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"have\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"but\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"or\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"utc\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"which\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"were\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"–\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"said\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"they\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"also\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"one\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"who\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"had\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"talk\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"new\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"their\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Temp\\ipykernel_2556\\4190515266.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / norm_product\n"
     ]
    }
   ],
   "source": [
    "test_before_retrofitted_gensim_dict = convert_matrix_to_dict(test_subset_neighbors_matrix, test_subset_wordList)\n",
    "test_before_retrofitted_gensim_similarity_matrix = generate_cosine_similarity_matrix(test_before_retrofitted_gensim_dict)\n",
    "print_vec_similarities(test_subset_wordList, test_before_retrofitted_gensim_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrofitted_wordVecs_gensim = retrofitting_wordVecs_test(wordVecMat_gensim, neighbors_matrix_gensim, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_cosine_similarity(X, Y, sample_size=10000):\n",
    "    np.random.seed(42)  # Set a random seed for reproducibility\n",
    "    sample_X = X[np.random.choice(X.shape[0], sample_size, replace=False)]\n",
    "    sample_Y = Y[np.random.choice(Y.shape[0], sample_size, replace=False)]\n",
    "    similarities = cosine_similarity(sample_X, sample_Y)\n",
    "    avg_cos_similarity = np.mean(similarities)\n",
    "    return avg_cos_similarity\n",
    "\n",
    "# Compute the average cosine similarity using a random sample\n",
    "avg_cos_similarity = calculate_average_cosine_similarity(wordVecMat_gensim, retrofitted_wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1406814758952421"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "subset_retrofitted_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:50]}\n",
    "subset_retrofitted_wordVecMat = retrofitted_wordVecs_gensim[:50] \n",
    "\n",
    "subset_retrofitted_wordList = wordList_gensim[:50]\n",
    "\n",
    "neighbors_matrix = retrieve_neighbors_embedding_matrix(subset_retrofitted_wordVecMat, subset_retrofitted_wordList, neighbors_dict)\n",
    "\n",
    "print(neighbors_matrix)\n",
    "print(type(neighbors_matrix))  \n",
    "print(neighbors_matrix.shape)  \n",
    "print(neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \",\":\n",
      "  - \",\": 1.0000\n",
      "  - \"the\": 0.5246\n",
      "  - \".\": 0.8301\n",
      "  - \"of\": 0.5214\n",
      "  - \"-\": 0.7543\n",
      "  - \"and\": 0.6395\n",
      "  - \"in\": 0.5861\n",
      "  - \"to\": 0.4614\n",
      "  - \"'\": 0.5109\n",
      "  - \"a\": 0.4850\n",
      "  - \")\": 0.6108\n",
      "  - \"(\": 0.5921\n",
      "  - \"is\": 0.4440\n",
      "  - \"s\": 0.4521\n",
      "  - \"for\": 0.4648\n",
      "  - \"was\": 0.4925\n",
      "  - \"on\": 0.4519\n",
      "  - \"that\": 0.4100\n",
      "  - \"as\": 0.4840\n",
      "  - \"it\": 0.4465\n",
      "  - \"with\": 0.4828\n",
      "  - \"by\": 0.4304\n",
      "  - \"\"\": 0.3507\n",
      "  - \"at\": 0.5117\n",
      "  - \"he\": 0.4664\n",
      "  - \"from\": 0.4830\n",
      "  - \"be\": 0.2936\n",
      "  - \"this\": 0.3865\n",
      "  - \"i\": 0.2934\n",
      "  - \"an\": 0.4290\n",
      "  - \"his\": 0.4085\n",
      "  - \"are\": 0.4065\n",
      "  - \"not\": 0.3309\n",
      "  - \"has\": 0.4605\n",
      "  - \"have\": 0.3552\n",
      "  - \"but\": 0.3084\n",
      "  - \"or\": 0.3191\n",
      "  - \"utc\": 0.3256\n",
      "  - \"which\": 0.3898\n",
      "  - \"were\": 0.3851\n",
      "  - \"–\": 0.4485\n",
      "  - \"said\": 0.3667\n",
      "  - \"they\": 0.3911\n",
      "  - \"also\": 0.5216\n",
      "  - \"one\": 0.4630\n",
      "  - \"who\": 0.3217\n",
      "  - \"had\": 0.3879\n",
      "  - \"talk\": 0.3371\n",
      "  - \"new\": 0.4277\n",
      "  - \"their\": 0.3509\n",
      "\n",
      "Similarities with \"the\":\n",
      "  - \",\": 0.5246\n",
      "  - \"the\": 1.0000\n",
      "  - \".\": 0.5650\n",
      "  - \"of\": 0.6847\n",
      "  - \"-\": 0.4697\n",
      "  - \"and\": 0.6368\n",
      "  - \"in\": 0.6208\n",
      "  - \"to\": 0.5324\n",
      "  - \"'\": 0.4664\n",
      "  - \"a\": 0.5986\n",
      "  - \")\": 0.3871\n",
      "  - \"(\": 0.3989\n",
      "  - \"is\": 0.5157\n",
      "  - \"s\": 0.5033\n",
      "  - \"for\": 0.4985\n",
      "  - \"was\": 0.5381\n",
      "  - \"on\": 0.5347\n",
      "  - \"that\": 0.5602\n",
      "  - \"as\": 0.4951\n",
      "  - \"it\": 0.5828\n",
      "  - \"with\": 0.5103\n",
      "  - \"by\": 0.5156\n",
      "  - \"\"\": 0.3547\n",
      "  - \"at\": 0.5098\n",
      "  - \"he\": 0.4513\n",
      "  - \"from\": 0.4914\n",
      "  - \"be\": 0.4029\n",
      "  - \"this\": 0.5624\n",
      "  - \"i\": 0.2798\n",
      "  - \"an\": 0.5235\n",
      "  - \"his\": 0.4642\n",
      "  - \"are\": 0.4323\n",
      "  - \"not\": 0.4128\n",
      "  - \"has\": 0.4344\n",
      "  - \"have\": 0.4200\n",
      "  - \"but\": 0.4758\n",
      "  - \"or\": 0.4006\n",
      "  - \"utc\": 0.2050\n",
      "  - \"which\": 0.6465\n",
      "  - \"were\": 0.4365\n",
      "  - \"–\": 0.3194\n",
      "  - \"said\": 0.3810\n",
      "  - \"they\": 0.4527\n",
      "  - \"also\": 0.5490\n",
      "  - \"one\": 0.6059\n",
      "  - \"who\": 0.3617\n",
      "  - \"had\": 0.4339\n",
      "  - \"talk\": 0.2662\n",
      "  - \"new\": 0.4359\n",
      "  - \"their\": 0.5333\n",
      "\n",
      "Similarities with \".\":\n",
      "  - \",\": 0.8301\n",
      "  - \"the\": 0.5650\n",
      "  - \".\": 1.0000\n",
      "  - \"of\": 0.5061\n",
      "  - \"-\": 0.7595\n",
      "  - \"and\": 0.5901\n",
      "  - \"in\": 0.5491\n",
      "  - \"to\": 0.4963\n",
      "  - \"'\": 0.5376\n",
      "  - \"a\": 0.4829\n",
      "  - \")\": 0.5742\n",
      "  - \"(\": 0.5698\n",
      "  - \"is\": 0.4403\n",
      "  - \"s\": 0.4721\n",
      "  - \"for\": 0.4884\n",
      "  - \"was\": 0.4554\n",
      "  - \"on\": 0.4954\n",
      "  - \"that\": 0.4804\n",
      "  - \"as\": 0.4601\n",
      "  - \"it\": 0.4876\n",
      "  - \"with\": 0.4638\n",
      "  - \"by\": 0.4499\n",
      "  - \"\"\": 0.3175\n",
      "  - \"at\": 0.5176\n",
      "  - \"he\": 0.3974\n",
      "  - \"from\": 0.4680\n",
      "  - \"be\": 0.3907\n",
      "  - \"this\": 0.4908\n",
      "  - \"i\": 0.3447\n",
      "  - \"an\": 0.4060\n",
      "  - \"his\": 0.3845\n",
      "  - \"are\": 0.4112\n",
      "  - \"not\": 0.4074\n",
      "  - \"has\": 0.4610\n",
      "  - \"have\": 0.4177\n",
      "  - \"but\": 0.4602\n",
      "  - \"or\": 0.3740\n",
      "  - \"utc\": 0.3789\n",
      "  - \"which\": 0.4946\n",
      "  - \"were\": 0.4073\n",
      "  - \"–\": 0.4081\n",
      "  - \"said\": 0.4129\n",
      "  - \"they\": 0.3677\n",
      "  - \"also\": 0.4873\n",
      "  - \"one\": 0.4869\n",
      "  - \"who\": 0.3527\n",
      "  - \"had\": 0.4085\n",
      "  - \"talk\": 0.4099\n",
      "  - \"new\": 0.4146\n",
      "  - \"their\": 0.3642\n",
      "\n",
      "Similarities with \"of\":\n",
      "  - \",\": 0.5214\n",
      "  - \"the\": 0.6847\n",
      "  - \".\": 0.5061\n",
      "  - \"of\": 1.0000\n",
      "  - \"-\": 0.4564\n",
      "  - \"and\": 0.6066\n",
      "  - \"in\": 0.5756\n",
      "  - \"to\": 0.4347\n",
      "  - \"'\": 0.4375\n",
      "  - \"a\": 0.4623\n",
      "  - \")\": 0.4465\n",
      "  - \"(\": 0.4424\n",
      "  - \"is\": 0.4895\n",
      "  - \"s\": 0.4212\n",
      "  - \"for\": 0.4764\n",
      "  - \"was\": 0.4460\n",
      "  - \"on\": 0.4650\n",
      "  - \"that\": 0.4399\n",
      "  - \"as\": 0.4333\n",
      "  - \"it\": 0.4193\n",
      "  - \"with\": 0.4502\n",
      "  - \"by\": 0.4370\n",
      "  - \"\"\": 0.3230\n",
      "  - \"at\": 0.4539\n",
      "  - \"he\": 0.4123\n",
      "  - \"from\": 0.5273\n",
      "  - \"be\": 0.2609\n",
      "  - \"this\": 0.3750\n",
      "  - \"i\": 0.2294\n",
      "  - \"an\": 0.4119\n",
      "  - \"his\": 0.3677\n",
      "  - \"are\": 0.4061\n",
      "  - \"not\": 0.3210\n",
      "  - \"has\": 0.3852\n",
      "  - \"have\": 0.3464\n",
      "  - \"but\": 0.3688\n",
      "  - \"or\": 0.3538\n",
      "  - \"utc\": 0.1789\n",
      "  - \"which\": 0.5006\n",
      "  - \"were\": 0.3664\n",
      "  - \"–\": 0.3121\n",
      "  - \"said\": 0.3073\n",
      "  - \"they\": 0.3076\n",
      "  - \"also\": 0.4869\n",
      "  - \"one\": 0.4692\n",
      "  - \"who\": 0.3380\n",
      "  - \"had\": 0.3860\n",
      "  - \"talk\": 0.2276\n",
      "  - \"new\": 0.3946\n",
      "  - \"their\": 0.3765\n",
      "\n",
      "Similarities with \"-\":\n",
      "  - \",\": 0.7543\n",
      "  - \"the\": 0.4697\n",
      "  - \".\": 0.7595\n",
      "  - \"of\": 0.4564\n",
      "  - \"-\": 1.0000\n",
      "  - \"and\": 0.5056\n",
      "  - \"in\": 0.4514\n",
      "  - \"to\": 0.4707\n",
      "  - \"'\": 0.4843\n",
      "  - \"a\": 0.4378\n",
      "  - \")\": 0.5834\n",
      "  - \"(\": 0.5638\n",
      "  - \"is\": 0.4143\n",
      "  - \"s\": 0.4076\n",
      "  - \"for\": 0.4293\n",
      "  - \"was\": 0.3893\n",
      "  - \"on\": 0.4379\n",
      "  - \"that\": 0.3770\n",
      "  - \"as\": 0.4253\n",
      "  - \"it\": 0.3762\n",
      "  - \"with\": 0.4224\n",
      "  - \"by\": 0.4078\n",
      "  - \"\"\": 0.2872\n",
      "  - \"at\": 0.4562\n",
      "  - \"he\": 0.3336\n",
      "  - \"from\": 0.4166\n",
      "  - \"be\": 0.2457\n",
      "  - \"this\": 0.4014\n",
      "  - \"i\": 0.3232\n",
      "  - \"an\": 0.3810\n",
      "  - \"his\": 0.3274\n",
      "  - \"are\": 0.3836\n",
      "  - \"not\": 0.3146\n",
      "  - \"has\": 0.3589\n",
      "  - \"have\": 0.3116\n",
      "  - \"but\": 0.3343\n",
      "  - \"or\": 0.3519\n",
      "  - \"utc\": 0.3464\n",
      "  - \"which\": 0.3681\n",
      "  - \"were\": 0.3254\n",
      "  - \"–\": 0.4731\n",
      "  - \"said\": 0.3348\n",
      "  - \"they\": 0.3213\n",
      "  - \"also\": 0.4237\n",
      "  - \"one\": 0.4412\n",
      "  - \"who\": 0.2722\n",
      "  - \"had\": 0.2965\n",
      "  - \"talk\": 0.3473\n",
      "  - \"new\": 0.3562\n",
      "  - \"their\": 0.2973\n",
      "\n",
      "Similarities with \"and\":\n",
      "  - \",\": 0.6395\n",
      "  - \"the\": 0.6368\n",
      "  - \".\": 0.5901\n",
      "  - \"of\": 0.6066\n",
      "  - \"-\": 0.5056\n",
      "  - \"and\": 1.0000\n",
      "  - \"in\": 0.5978\n",
      "  - \"to\": 0.5599\n",
      "  - \"'\": 0.4414\n",
      "  - \"a\": 0.5268\n",
      "  - \")\": 0.3939\n",
      "  - \"(\": 0.4213\n",
      "  - \"is\": 0.4632\n",
      "  - \"s\": 0.4357\n",
      "  - \"for\": 0.5425\n",
      "  - \"was\": 0.4871\n",
      "  - \"on\": 0.4980\n",
      "  - \"that\": 0.5361\n",
      "  - \"as\": 0.5523\n",
      "  - \"it\": 0.4787\n",
      "  - \"with\": 0.6079\n",
      "  - \"by\": 0.4967\n",
      "  - \"\"\": 0.3856\n",
      "  - \"at\": 0.4875\n",
      "  - \"he\": 0.5037\n",
      "  - \"from\": 0.5066\n",
      "  - \"be\": 0.3271\n",
      "  - \"this\": 0.3748\n",
      "  - \"i\": 0.2676\n",
      "  - \"an\": 0.4609\n",
      "  - \"his\": 0.4821\n",
      "  - \"are\": 0.4727\n",
      "  - \"not\": 0.2993\n",
      "  - \"has\": 0.4638\n",
      "  - \"have\": 0.4441\n",
      "  - \"but\": 0.5961\n",
      "  - \"or\": 0.4796\n",
      "  - \"utc\": 0.2434\n",
      "  - \"which\": 0.6658\n",
      "  - \"were\": 0.4625\n",
      "  - \"–\": 0.4187\n",
      "  - \"said\": 0.3821\n",
      "  - \"they\": 0.4807\n",
      "  - \"also\": 0.6205\n",
      "  - \"one\": 0.4881\n",
      "  - \"who\": 0.4885\n",
      "  - \"had\": 0.4304\n",
      "  - \"talk\": 0.2665\n",
      "  - \"new\": 0.4307\n",
      "  - \"their\": 0.5092\n",
      "\n",
      "Similarities with \"in\":\n",
      "  - \",\": 0.5861\n",
      "  - \"the\": 0.6208\n",
      "  - \".\": 0.5491\n",
      "  - \"of\": 0.5756\n",
      "  - \"-\": 0.4514\n",
      "  - \"and\": 0.5978\n",
      "  - \"in\": 1.0000\n",
      "  - \"to\": 0.4625\n",
      "  - \"'\": 0.4212\n",
      "  - \"a\": 0.5234\n",
      "  - \")\": 0.4413\n",
      "  - \"(\": 0.4197\n",
      "  - \"is\": 0.4672\n",
      "  - \"s\": 0.4379\n",
      "  - \"for\": 0.4522\n",
      "  - \"was\": 0.5058\n",
      "  - \"on\": 0.4987\n",
      "  - \"that\": 0.4011\n",
      "  - \"as\": 0.4232\n",
      "  - \"it\": 0.4701\n",
      "  - \"with\": 0.4696\n",
      "  - \"by\": 0.4625\n",
      "  - \"\"\": 0.3309\n",
      "  - \"at\": 0.5243\n",
      "  - \"he\": 0.4985\n",
      "  - \"from\": 0.4991\n",
      "  - \"be\": 0.3091\n",
      "  - \"this\": 0.3876\n",
      "  - \"i\": 0.2306\n",
      "  - \"an\": 0.4128\n",
      "  - \"his\": 0.4220\n",
      "  - \"are\": 0.3343\n",
      "  - \"not\": 0.2867\n",
      "  - \"has\": 0.4603\n",
      "  - \"have\": 0.3587\n",
      "  - \"but\": 0.3873\n",
      "  - \"or\": 0.2872\n",
      "  - \"utc\": 0.2020\n",
      "  - \"which\": 0.4883\n",
      "  - \"were\": 0.3627\n",
      "  - \"–\": 0.3037\n",
      "  - \"said\": 0.2904\n",
      "  - \"they\": 0.3937\n",
      "  - \"also\": 0.4976\n",
      "  - \"one\": 0.4364\n",
      "  - \"who\": 0.3571\n",
      "  - \"had\": 0.4096\n",
      "  - \"talk\": 0.2557\n",
      "  - \"new\": 0.4019\n",
      "  - \"their\": 0.3456\n",
      "\n",
      "Similarities with \"to\":\n",
      "  - \",\": 0.4614\n",
      "  - \"the\": 0.5324\n",
      "  - \".\": 0.4963\n",
      "  - \"of\": 0.4347\n",
      "  - \"-\": 0.4707\n",
      "  - \"and\": 0.5599\n",
      "  - \"in\": 0.4625\n",
      "  - \"to\": 1.0000\n",
      "  - \"'\": 0.3801\n",
      "  - \"a\": 0.4354\n",
      "  - \")\": 0.3440\n",
      "  - \"(\": 0.3160\n",
      "  - \"is\": 0.3580\n",
      "  - \"s\": 0.3915\n",
      "  - \"for\": 0.4566\n",
      "  - \"was\": 0.3952\n",
      "  - \"on\": 0.4377\n",
      "  - \"that\": 0.5090\n",
      "  - \"as\": 0.4013\n",
      "  - \"it\": 0.4368\n",
      "  - \"with\": 0.4628\n",
      "  - \"by\": 0.3695\n",
      "  - \"\"\": 0.2845\n",
      "  - \"at\": 0.4209\n",
      "  - \"he\": 0.3647\n",
      "  - \"from\": 0.4317\n",
      "  - \"be\": 0.3642\n",
      "  - \"this\": 0.4455\n",
      "  - \"i\": 0.2898\n",
      "  - \"an\": 0.3668\n",
      "  - \"his\": 0.3972\n",
      "  - \"are\": 0.3624\n",
      "  - \"not\": 0.3926\n",
      "  - \"has\": 0.3517\n",
      "  - \"have\": 0.3873\n",
      "  - \"but\": 0.5141\n",
      "  - \"or\": 0.3890\n",
      "  - \"utc\": 0.2090\n",
      "  - \"which\": 0.4803\n",
      "  - \"were\": 0.3728\n",
      "  - \"–\": 0.2479\n",
      "  - \"said\": 0.4054\n",
      "  - \"they\": 0.4824\n",
      "  - \"also\": 0.4587\n",
      "  - \"one\": 0.4380\n",
      "  - \"who\": 0.3530\n",
      "  - \"had\": 0.3821\n",
      "  - \"talk\": 0.2466\n",
      "  - \"new\": 0.3895\n",
      "  - \"their\": 0.4696\n",
      "\n",
      "Similarities with \"'\":\n",
      "  - \",\": 0.5109\n",
      "  - \"the\": 0.4664\n",
      "  - \".\": 0.5376\n",
      "  - \"of\": 0.4375\n",
      "  - \"-\": 0.4843\n",
      "  - \"and\": 0.4414\n",
      "  - \"in\": 0.4212\n",
      "  - \"to\": 0.3801\n",
      "  - \"'\": 1.0000\n",
      "  - \"a\": 0.4376\n",
      "  - \")\": 0.5649\n",
      "  - \"(\": 0.5630\n",
      "  - \"is\": 0.5231\n",
      "  - \"s\": 0.5292\n",
      "  - \"for\": 0.4026\n",
      "  - \"was\": 0.4206\n",
      "  - \"on\": 0.3763\n",
      "  - \"that\": 0.4177\n",
      "  - \"as\": 0.4279\n",
      "  - \"it\": 0.4801\n",
      "  - \"with\": 0.3314\n",
      "  - \"by\": 0.3617\n",
      "  - \"\"\": 0.2645\n",
      "  - \"at\": 0.3821\n",
      "  - \"he\": 0.3303\n",
      "  - \"from\": 0.3471\n",
      "  - \"be\": 0.2901\n",
      "  - \"this\": 0.3885\n",
      "  - \"i\": 0.3301\n",
      "  - \"an\": 0.4047\n",
      "  - \"his\": 0.2922\n",
      "  - \"are\": 0.3215\n",
      "  - \"not\": 0.3124\n",
      "  - \"has\": 0.3341\n",
      "  - \"have\": 0.3281\n",
      "  - \"but\": 0.3604\n",
      "  - \"or\": 0.3643\n",
      "  - \"utc\": 0.2767\n",
      "  - \"which\": 0.3376\n",
      "  - \"were\": 0.2683\n",
      "  - \"–\": 0.4101\n",
      "  - \"said\": 0.3478\n",
      "  - \"they\": 0.3221\n",
      "  - \"also\": 0.4129\n",
      "  - \"one\": 0.3987\n",
      "  - \"who\": 0.3351\n",
      "  - \"had\": 0.2804\n",
      "  - \"talk\": 0.3356\n",
      "  - \"new\": 0.3129\n",
      "  - \"their\": 0.2767\n",
      "\n",
      "Similarities with \"a\":\n",
      "  - \",\": 0.4850\n",
      "  - \"the\": 0.5986\n",
      "  - \".\": 0.4829\n",
      "  - \"of\": 0.4623\n",
      "  - \"-\": 0.4378\n",
      "  - \"and\": 0.5268\n",
      "  - \"in\": 0.5234\n",
      "  - \"to\": 0.4354\n",
      "  - \"'\": 0.4376\n",
      "  - \"a\": 1.0000\n",
      "  - \")\": 0.4036\n",
      "  - \"(\": 0.3928\n",
      "  - \"is\": 0.5659\n",
      "  - \"s\": 0.4352\n",
      "  - \"for\": 0.4318\n",
      "  - \"was\": 0.4865\n",
      "  - \"on\": 0.4286\n",
      "  - \"that\": 0.4742\n",
      "  - \"as\": 0.4129\n",
      "  - \"it\": 0.4853\n",
      "  - \"with\": 0.4635\n",
      "  - \"by\": 0.3777\n",
      "  - \"\"\": 0.3399\n",
      "  - \"at\": 0.4151\n",
      "  - \"he\": 0.4150\n",
      "  - \"from\": 0.4113\n",
      "  - \"be\": 0.3347\n",
      "  - \"this\": 0.4678\n",
      "  - \"i\": 0.2837\n",
      "  - \"an\": 0.5197\n",
      "  - \"his\": 0.3884\n",
      "  - \"are\": 0.3544\n",
      "  - \"not\": 0.3220\n",
      "  - \"has\": 0.4008\n",
      "  - \"have\": 0.3516\n",
      "  - \"but\": 0.4337\n",
      "  - \"or\": 0.4007\n",
      "  - \"utc\": 0.1852\n",
      "  - \"which\": 0.5183\n",
      "  - \"were\": 0.2746\n",
      "  - \"–\": 0.2607\n",
      "  - \"said\": 0.3443\n",
      "  - \"they\": 0.3485\n",
      "  - \"also\": 0.4644\n",
      "  - \"one\": 0.5032\n",
      "  - \"who\": 0.3611\n",
      "  - \"had\": 0.3762\n",
      "  - \"talk\": 0.2649\n",
      "  - \"new\": 0.3297\n",
      "  - \"their\": 0.3504\n",
      "\n",
      "Similarities with \")\":\n",
      "  - \",\": 0.6108\n",
      "  - \"the\": 0.3871\n",
      "  - \".\": 0.5742\n",
      "  - \"of\": 0.4465\n",
      "  - \"-\": 0.5834\n",
      "  - \"and\": 0.3939\n",
      "  - \"in\": 0.4413\n",
      "  - \"to\": 0.3440\n",
      "  - \"'\": 0.5649\n",
      "  - \"a\": 0.4036\n",
      "  - \")\": 1.0000\n",
      "  - \"(\": 0.8799\n",
      "  - \"is\": 0.4146\n",
      "  - \"s\": 0.3749\n",
      "  - \"for\": 0.4241\n",
      "  - \"was\": 0.3337\n",
      "  - \"on\": 0.3424\n",
      "  - \"that\": 0.2885\n",
      "  - \"as\": 0.4192\n",
      "  - \"it\": 0.3392\n",
      "  - \"with\": 0.3389\n",
      "  - \"by\": 0.3582\n",
      "  - \"\"\": 0.2565\n",
      "  - \"at\": 0.4194\n",
      "  - \"he\": 0.3026\n",
      "  - \"from\": 0.3885\n",
      "  - \"be\": 0.2113\n",
      "  - \"this\": 0.3964\n",
      "  - \"i\": 0.3227\n",
      "  - \"an\": 0.3219\n",
      "  - \"his\": 0.1946\n",
      "  - \"are\": 0.3034\n",
      "  - \"not\": 0.2591\n",
      "  - \"has\": 0.3670\n",
      "  - \"have\": 0.2831\n",
      "  - \"but\": 0.2025\n",
      "  - \"or\": 0.3415\n",
      "  - \"utc\": 0.4105\n",
      "  - \"which\": 0.3048\n",
      "  - \"were\": 0.2244\n",
      "  - \"–\": 0.5241\n",
      "  - \"said\": 0.2439\n",
      "  - \"they\": 0.2250\n",
      "  - \"also\": 0.3228\n",
      "  - \"one\": 0.3461\n",
      "  - \"who\": 0.3488\n",
      "  - \"had\": 0.2286\n",
      "  - \"talk\": 0.4670\n",
      "  - \"new\": 0.2816\n",
      "  - \"their\": 0.1785\n",
      "\n",
      "Similarities with \"(\":\n",
      "  - \",\": 0.5921\n",
      "  - \"the\": 0.3989\n",
      "  - \".\": 0.5698\n",
      "  - \"of\": 0.4424\n",
      "  - \"-\": 0.5638\n",
      "  - \"and\": 0.4213\n",
      "  - \"in\": 0.4197\n",
      "  - \"to\": 0.3160\n",
      "  - \"'\": 0.5630\n",
      "  - \"a\": 0.3928\n",
      "  - \")\": 0.8799\n",
      "  - \"(\": 1.0000\n",
      "  - \"is\": 0.3862\n",
      "  - \"s\": 0.3835\n",
      "  - \"for\": 0.3903\n",
      "  - \"was\": 0.3416\n",
      "  - \"on\": 0.3386\n",
      "  - \"that\": 0.2523\n",
      "  - \"as\": 0.3877\n",
      "  - \"it\": 0.3327\n",
      "  - \"with\": 0.3059\n",
      "  - \"by\": 0.3693\n",
      "  - \"\"\": 0.2682\n",
      "  - \"at\": 0.3914\n",
      "  - \"he\": 0.2888\n",
      "  - \"from\": 0.3929\n",
      "  - \"be\": 0.2730\n",
      "  - \"this\": 0.3956\n",
      "  - \"i\": 0.3317\n",
      "  - \"an\": 0.3330\n",
      "  - \"his\": 0.2103\n",
      "  - \"are\": 0.3167\n",
      "  - \"not\": 0.2794\n",
      "  - \"has\": 0.3531\n",
      "  - \"have\": 0.2989\n",
      "  - \"but\": 0.2536\n",
      "  - \"or\": 0.3597\n",
      "  - \"utc\": 0.4313\n",
      "  - \"which\": 0.2855\n",
      "  - \"were\": 0.2620\n",
      "  - \"–\": 0.5239\n",
      "  - \"said\": 0.2436\n",
      "  - \"they\": 0.2134\n",
      "  - \"also\": 0.2875\n",
      "  - \"one\": 0.3300\n",
      "  - \"who\": 0.3307\n",
      "  - \"had\": 0.2659\n",
      "  - \"talk\": 0.4832\n",
      "  - \"new\": 0.3012\n",
      "  - \"their\": 0.1947\n",
      "\n",
      "Similarities with \"is\":\n",
      "  - \",\": 0.4440\n",
      "  - \"the\": 0.5157\n",
      "  - \".\": 0.4403\n",
      "  - \"of\": 0.4895\n",
      "  - \"-\": 0.4143\n",
      "  - \"and\": 0.4632\n",
      "  - \"in\": 0.4672\n",
      "  - \"to\": 0.3580\n",
      "  - \"'\": 0.5231\n",
      "  - \"a\": 0.5659\n",
      "  - \")\": 0.4146\n",
      "  - \"(\": 0.3862\n",
      "  - \"is\": 1.0000\n",
      "  - \"s\": 0.3921\n",
      "  - \"for\": 0.3808\n",
      "  - \"was\": 0.5807\n",
      "  - \"on\": 0.4007\n",
      "  - \"that\": 0.4018\n",
      "  - \"as\": 0.3866\n",
      "  - \"it\": 0.5116\n",
      "  - \"with\": 0.3777\n",
      "  - \"by\": 0.3716\n",
      "  - \"\"\": 0.2743\n",
      "  - \"at\": 0.3633\n",
      "  - \"he\": 0.3148\n",
      "  - \"from\": 0.3619\n",
      "  - \"be\": 0.3733\n",
      "  - \"this\": 0.3421\n",
      "  - \"i\": 0.2777\n",
      "  - \"an\": 0.4810\n",
      "  - \"his\": 0.2667\n",
      "  - \"are\": 0.5279\n",
      "  - \"not\": 0.3399\n",
      "  - \"has\": 0.5250\n",
      "  - \"have\": 0.3340\n",
      "  - \"but\": 0.3657\n",
      "  - \"or\": 0.3357\n",
      "  - \"utc\": 0.2189\n",
      "  - \"which\": 0.4054\n",
      "  - \"were\": 0.2729\n",
      "  - \"–\": 0.2517\n",
      "  - \"said\": 0.3072\n",
      "  - \"they\": 0.2942\n",
      "  - \"also\": 0.4076\n",
      "  - \"one\": 0.4067\n",
      "  - \"who\": 0.3073\n",
      "  - \"had\": 0.2889\n",
      "  - \"talk\": 0.2649\n",
      "  - \"new\": 0.2806\n",
      "  - \"their\": 0.2145\n",
      "\n",
      "Similarities with \"s\":\n",
      "  - \",\": 0.4521\n",
      "  - \"the\": 0.5033\n",
      "  - \".\": 0.4721\n",
      "  - \"of\": 0.4212\n",
      "  - \"-\": 0.4076\n",
      "  - \"and\": 0.4357\n",
      "  - \"in\": 0.4379\n",
      "  - \"to\": 0.3915\n",
      "  - \"'\": 0.5292\n",
      "  - \"a\": 0.4352\n",
      "  - \")\": 0.3749\n",
      "  - \"(\": 0.3835\n",
      "  - \"is\": 0.3921\n",
      "  - \"s\": 1.0000\n",
      "  - \"for\": 0.3969\n",
      "  - \"was\": 0.4264\n",
      "  - \"on\": 0.3712\n",
      "  - \"that\": 0.4193\n",
      "  - \"as\": 0.4151\n",
      "  - \"it\": 0.3119\n",
      "  - \"with\": 0.4033\n",
      "  - \"by\": 0.3606\n",
      "  - \"\"\": 0.2454\n",
      "  - \"at\": 0.4059\n",
      "  - \"he\": 0.3482\n",
      "  - \"from\": 0.3441\n",
      "  - \"be\": 0.3054\n",
      "  - \"this\": 0.3517\n",
      "  - \"i\": 0.2905\n",
      "  - \"an\": 0.3569\n",
      "  - \"his\": 0.4312\n",
      "  - \"are\": 0.2989\n",
      "  - \"not\": 0.2731\n",
      "  - \"has\": 0.3621\n",
      "  - \"have\": 0.3159\n",
      "  - \"but\": 0.3293\n",
      "  - \"or\": 0.3015\n",
      "  - \"utc\": 0.1715\n",
      "  - \"which\": 0.3497\n",
      "  - \"were\": 0.3258\n",
      "  - \"–\": 0.2846\n",
      "  - \"said\": 0.3839\n",
      "  - \"they\": 0.3007\n",
      "  - \"also\": 0.3943\n",
      "  - \"one\": 0.3819\n",
      "  - \"who\": 0.2977\n",
      "  - \"had\": 0.3927\n",
      "  - \"talk\": 0.1701\n",
      "  - \"new\": 0.3038\n",
      "  - \"their\": 0.3601\n",
      "\n",
      "Similarities with \"for\":\n",
      "  - \",\": 0.4648\n",
      "  - \"the\": 0.4985\n",
      "  - \".\": 0.4884\n",
      "  - \"of\": 0.4764\n",
      "  - \"-\": 0.4293\n",
      "  - \"and\": 0.5425\n",
      "  - \"in\": 0.4522\n",
      "  - \"to\": 0.4566\n",
      "  - \"'\": 0.4026\n",
      "  - \"a\": 0.4318\n",
      "  - \")\": 0.4241\n",
      "  - \"(\": 0.3903\n",
      "  - \"is\": 0.3808\n",
      "  - \"s\": 0.3969\n",
      "  - \"for\": 1.0000\n",
      "  - \"was\": 0.3969\n",
      "  - \"on\": 0.4290\n",
      "  - \"that\": 0.4263\n",
      "  - \"as\": 0.5151\n",
      "  - \"it\": 0.3472\n",
      "  - \"with\": 0.4430\n",
      "  - \"by\": 0.3603\n",
      "  - \"\"\": 0.3066\n",
      "  - \"at\": 0.4654\n",
      "  - \"he\": 0.4103\n",
      "  - \"from\": 0.4067\n",
      "  - \"be\": 0.3158\n",
      "  - \"this\": 0.3820\n",
      "  - \"i\": 0.2645\n",
      "  - \"an\": 0.3802\n",
      "  - \"his\": 0.3596\n",
      "  - \"are\": 0.3719\n",
      "  - \"not\": 0.3433\n",
      "  - \"has\": 0.3980\n",
      "  - \"have\": 0.3770\n",
      "  - \"but\": 0.3847\n",
      "  - \"or\": 0.3300\n",
      "  - \"utc\": 0.2402\n",
      "  - \"which\": 0.4294\n",
      "  - \"were\": 0.3669\n",
      "  - \"–\": 0.3485\n",
      "  - \"said\": 0.3013\n",
      "  - \"they\": 0.3271\n",
      "  - \"also\": 0.5157\n",
      "  - \"one\": 0.4635\n",
      "  - \"who\": 0.3697\n",
      "  - \"had\": 0.3609\n",
      "  - \"talk\": 0.2700\n",
      "  - \"new\": 0.3393\n",
      "  - \"their\": 0.3851\n",
      "\n",
      "Similarities with \"was\":\n",
      "  - \",\": 0.4925\n",
      "  - \"the\": 0.5381\n",
      "  - \".\": 0.4554\n",
      "  - \"of\": 0.4460\n",
      "  - \"-\": 0.3893\n",
      "  - \"and\": 0.4871\n",
      "  - \"in\": 0.5058\n",
      "  - \"to\": 0.3952\n",
      "  - \"'\": 0.4206\n",
      "  - \"a\": 0.4865\n",
      "  - \")\": 0.3337\n",
      "  - \"(\": 0.3416\n",
      "  - \"is\": 0.5807\n",
      "  - \"s\": 0.4264\n",
      "  - \"for\": 0.3969\n",
      "  - \"was\": 1.0000\n",
      "  - \"on\": 0.4076\n",
      "  - \"that\": 0.3655\n",
      "  - \"as\": 0.3961\n",
      "  - \"it\": 0.4383\n",
      "  - \"with\": 0.3761\n",
      "  - \"by\": 0.4769\n",
      "  - \"\"\": 0.2904\n",
      "  - \"at\": 0.4334\n",
      "  - \"he\": 0.4941\n",
      "  - \"from\": 0.4239\n",
      "  - \"be\": 0.3801\n",
      "  - \"this\": 0.3021\n",
      "  - \"i\": 0.2542\n",
      "  - \"an\": 0.4060\n",
      "  - \"his\": 0.4279\n",
      "  - \"are\": 0.3241\n",
      "  - \"not\": 0.2393\n",
      "  - \"has\": 0.4701\n",
      "  - \"have\": 0.3196\n",
      "  - \"but\": 0.3688\n",
      "  - \"or\": 0.2025\n",
      "  - \"utc\": 0.1760\n",
      "  - \"which\": 0.3848\n",
      "  - \"were\": 0.5998\n",
      "  - \"–\": 0.3205\n",
      "  - \"said\": 0.3526\n",
      "  - \"they\": 0.3394\n",
      "  - \"also\": 0.4157\n",
      "  - \"one\": 0.3957\n",
      "  - \"who\": 0.3467\n",
      "  - \"had\": 0.6005\n",
      "  - \"talk\": 0.1835\n",
      "  - \"new\": 0.3516\n",
      "  - \"their\": 0.2838\n",
      "\n",
      "Similarities with \"on\":\n",
      "  - \",\": 0.4519\n",
      "  - \"the\": 0.5347\n",
      "  - \".\": 0.4954\n",
      "  - \"of\": 0.4650\n",
      "  - \"-\": 0.4379\n",
      "  - \"and\": 0.4980\n",
      "  - \"in\": 0.4987\n",
      "  - \"to\": 0.4377\n",
      "  - \"'\": 0.3763\n",
      "  - \"a\": 0.4286\n",
      "  - \")\": 0.3424\n",
      "  - \"(\": 0.3386\n",
      "  - \"is\": 0.4007\n",
      "  - \"s\": 0.3712\n",
      "  - \"for\": 0.4290\n",
      "  - \"was\": 0.4076\n",
      "  - \"on\": 1.0000\n",
      "  - \"that\": 0.4199\n",
      "  - \"as\": 0.3966\n",
      "  - \"it\": 0.4193\n",
      "  - \"with\": 0.4506\n",
      "  - \"by\": 0.4545\n",
      "  - \"\"\": 0.2704\n",
      "  - \"at\": 0.4705\n",
      "  - \"he\": 0.3232\n",
      "  - \"from\": 0.4096\n",
      "  - \"be\": 0.3192\n",
      "  - \"this\": 0.3671\n",
      "  - \"i\": 0.2002\n",
      "  - \"an\": 0.3580\n",
      "  - \"his\": 0.3316\n",
      "  - \"are\": 0.3436\n",
      "  - \"not\": 0.3073\n",
      "  - \"has\": 0.3558\n",
      "  - \"have\": 0.3485\n",
      "  - \"but\": 0.3520\n",
      "  - \"or\": 0.2902\n",
      "  - \"utc\": 0.1983\n",
      "  - \"which\": 0.4530\n",
      "  - \"were\": 0.3637\n",
      "  - \"–\": 0.2296\n",
      "  - \"said\": 0.3099\n",
      "  - \"they\": 0.3236\n",
      "  - \"also\": 0.4097\n",
      "  - \"one\": 0.4198\n",
      "  - \"who\": 0.2817\n",
      "  - \"had\": 0.3610\n",
      "  - \"talk\": 0.2147\n",
      "  - \"new\": 0.3277\n",
      "  - \"their\": 0.3559\n",
      "\n",
      "Similarities with \"that\":\n",
      "  - \",\": 0.4100\n",
      "  - \"the\": 0.5602\n",
      "  - \".\": 0.4804\n",
      "  - \"of\": 0.4399\n",
      "  - \"-\": 0.3770\n",
      "  - \"and\": 0.5361\n",
      "  - \"in\": 0.4011\n",
      "  - \"to\": 0.5090\n",
      "  - \"'\": 0.4177\n",
      "  - \"a\": 0.4742\n",
      "  - \")\": 0.2885\n",
      "  - \"(\": 0.2523\n",
      "  - \"is\": 0.4018\n",
      "  - \"s\": 0.4193\n",
      "  - \"for\": 0.4263\n",
      "  - \"was\": 0.3655\n",
      "  - \"on\": 0.4199\n",
      "  - \"that\": 1.0000\n",
      "  - \"as\": 0.4455\n",
      "  - \"it\": 0.5371\n",
      "  - \"with\": 0.4780\n",
      "  - \"by\": 0.4118\n",
      "  - \"\"\": 0.3851\n",
      "  - \"at\": 0.3396\n",
      "  - \"he\": 0.3551\n",
      "  - \"from\": 0.3905\n",
      "  - \"be\": 0.4753\n",
      "  - \"this\": 0.5993\n",
      "  - \"i\": 0.3989\n",
      "  - \"an\": 0.4072\n",
      "  - \"his\": 0.3790\n",
      "  - \"are\": 0.3965\n",
      "  - \"not\": 0.5074\n",
      "  - \"has\": 0.4058\n",
      "  - \"have\": 0.5365\n",
      "  - \"but\": 0.6532\n",
      "  - \"or\": 0.4106\n",
      "  - \"utc\": 0.2412\n",
      "  - \"which\": 0.7366\n",
      "  - \"were\": 0.3538\n",
      "  - \"–\": 0.1914\n",
      "  - \"said\": 0.5332\n",
      "  - \"they\": 0.5066\n",
      "  - \"also\": 0.5833\n",
      "  - \"one\": 0.4954\n",
      "  - \"who\": 0.4302\n",
      "  - \"had\": 0.3999\n",
      "  - \"talk\": 0.2984\n",
      "  - \"new\": 0.3534\n",
      "  - \"their\": 0.4321\n",
      "\n",
      "Similarities with \"as\":\n",
      "  - \",\": 0.4840\n",
      "  - \"the\": 0.4951\n",
      "  - \".\": 0.4601\n",
      "  - \"of\": 0.4333\n",
      "  - \"-\": 0.4253\n",
      "  - \"and\": 0.5523\n",
      "  - \"in\": 0.4232\n",
      "  - \"to\": 0.4013\n",
      "  - \"'\": 0.4279\n",
      "  - \"a\": 0.4129\n",
      "  - \")\": 0.4192\n",
      "  - \"(\": 0.3877\n",
      "  - \"is\": 0.3866\n",
      "  - \"s\": 0.4151\n",
      "  - \"for\": 0.5151\n",
      "  - \"was\": 0.3961\n",
      "  - \"on\": 0.3966\n",
      "  - \"that\": 0.4455\n",
      "  - \"as\": 1.0000\n",
      "  - \"it\": 0.3648\n",
      "  - \"with\": 0.4241\n",
      "  - \"by\": 0.3730\n",
      "  - \"\"\": 0.3048\n",
      "  - \"at\": 0.3423\n",
      "  - \"he\": 0.3357\n",
      "  - \"from\": 0.3637\n",
      "  - \"be\": 0.3606\n",
      "  - \"this\": 0.3511\n",
      "  - \"i\": 0.2199\n",
      "  - \"an\": 0.3673\n",
      "  - \"his\": 0.3112\n",
      "  - \"are\": 0.3464\n",
      "  - \"not\": 0.3524\n",
      "  - \"has\": 0.3553\n",
      "  - \"have\": 0.3407\n",
      "  - \"but\": 0.3820\n",
      "  - \"or\": 0.3621\n",
      "  - \"utc\": 0.2208\n",
      "  - \"which\": 0.4127\n",
      "  - \"were\": 0.3336\n",
      "  - \"–\": 0.2881\n",
      "  - \"said\": 0.3537\n",
      "  - \"they\": 0.3107\n",
      "  - \"also\": 0.4897\n",
      "  - \"one\": 0.4024\n",
      "  - \"who\": 0.3590\n",
      "  - \"had\": 0.3713\n",
      "  - \"talk\": 0.2969\n",
      "  - \"new\": 0.2462\n",
      "  - \"their\": 0.3354\n",
      "\n",
      "Similarities with \"it\":\n",
      "  - \",\": 0.4465\n",
      "  - \"the\": 0.5828\n",
      "  - \".\": 0.4876\n",
      "  - \"of\": 0.4193\n",
      "  - \"-\": 0.3762\n",
      "  - \"and\": 0.4787\n",
      "  - \"in\": 0.4701\n",
      "  - \"to\": 0.4368\n",
      "  - \"'\": 0.4801\n",
      "  - \"a\": 0.4853\n",
      "  - \")\": 0.3392\n",
      "  - \"(\": 0.3327\n",
      "  - \"is\": 0.5116\n",
      "  - \"s\": 0.3119\n",
      "  - \"for\": 0.3472\n",
      "  - \"was\": 0.4383\n",
      "  - \"on\": 0.4193\n",
      "  - \"that\": 0.5371\n",
      "  - \"as\": 0.3648\n",
      "  - \"it\": 1.0000\n",
      "  - \"with\": 0.3541\n",
      "  - \"by\": 0.4179\n",
      "  - \"\"\": 0.2548\n",
      "  - \"at\": 0.3499\n",
      "  - \"he\": 0.4189\n",
      "  - \"from\": 0.3687\n",
      "  - \"be\": 0.4483\n",
      "  - \"this\": 0.6052\n",
      "  - \"i\": 0.3469\n",
      "  - \"an\": 0.4183\n",
      "  - \"his\": 0.2834\n",
      "  - \"are\": 0.3888\n",
      "  - \"not\": 0.4246\n",
      "  - \"has\": 0.3599\n",
      "  - \"have\": 0.4107\n",
      "  - \"but\": 0.4901\n",
      "  - \"or\": 0.3481\n",
      "  - \"utc\": 0.2442\n",
      "  - \"which\": 0.5677\n",
      "  - \"were\": 0.3291\n",
      "  - \"–\": 0.1565\n",
      "  - \"said\": 0.3132\n",
      "  - \"they\": 0.4417\n",
      "  - \"also\": 0.3991\n",
      "  - \"one\": 0.4653\n",
      "  - \"who\": 0.2633\n",
      "  - \"had\": 0.3036\n",
      "  - \"talk\": 0.3040\n",
      "  - \"new\": 0.3383\n",
      "  - \"their\": 0.3008\n",
      "\n",
      "Similarities with \"with\":\n",
      "  - \",\": 0.4828\n",
      "  - \"the\": 0.5103\n",
      "  - \".\": 0.4638\n",
      "  - \"of\": 0.4502\n",
      "  - \"-\": 0.4224\n",
      "  - \"and\": 0.6079\n",
      "  - \"in\": 0.4696\n",
      "  - \"to\": 0.4628\n",
      "  - \"'\": 0.3314\n",
      "  - \"a\": 0.4635\n",
      "  - \")\": 0.3389\n",
      "  - \"(\": 0.3059\n",
      "  - \"is\": 0.3777\n",
      "  - \"s\": 0.4033\n",
      "  - \"for\": 0.4430\n",
      "  - \"was\": 0.3761\n",
      "  - \"on\": 0.4506\n",
      "  - \"that\": 0.4780\n",
      "  - \"as\": 0.4241\n",
      "  - \"it\": 0.3541\n",
      "  - \"with\": 1.0000\n",
      "  - \"by\": 0.4156\n",
      "  - \"\"\": 0.2737\n",
      "  - \"at\": 0.4052\n",
      "  - \"he\": 0.3290\n",
      "  - \"from\": 0.4548\n",
      "  - \"be\": 0.2806\n",
      "  - \"this\": 0.3701\n",
      "  - \"i\": 0.2078\n",
      "  - \"an\": 0.3966\n",
      "  - \"his\": 0.3652\n",
      "  - \"are\": 0.4395\n",
      "  - \"not\": 0.2955\n",
      "  - \"has\": 0.4468\n",
      "  - \"have\": 0.4165\n",
      "  - \"but\": 0.4127\n",
      "  - \"or\": 0.3554\n",
      "  - \"utc\": 0.1654\n",
      "  - \"which\": 0.5273\n",
      "  - \"were\": 0.3935\n",
      "  - \"–\": 0.2693\n",
      "  - \"said\": 0.3487\n",
      "  - \"they\": 0.3914\n",
      "  - \"also\": 0.4630\n",
      "  - \"one\": 0.4398\n",
      "  - \"who\": 0.3918\n",
      "  - \"had\": 0.4619\n",
      "  - \"talk\": 0.1993\n",
      "  - \"new\": 0.3093\n",
      "  - \"their\": 0.4425\n",
      "\n",
      "Similarities with \"by\":\n",
      "  - \",\": 0.4304\n",
      "  - \"the\": 0.5156\n",
      "  - \".\": 0.4499\n",
      "  - \"of\": 0.4370\n",
      "  - \"-\": 0.4078\n",
      "  - \"and\": 0.4967\n",
      "  - \"in\": 0.4625\n",
      "  - \"to\": 0.3695\n",
      "  - \"'\": 0.3617\n",
      "  - \"a\": 0.3777\n",
      "  - \")\": 0.3582\n",
      "  - \"(\": 0.3693\n",
      "  - \"is\": 0.3716\n",
      "  - \"s\": 0.3606\n",
      "  - \"for\": 0.3603\n",
      "  - \"was\": 0.4769\n",
      "  - \"on\": 0.4545\n",
      "  - \"that\": 0.4118\n",
      "  - \"as\": 0.3730\n",
      "  - \"it\": 0.4179\n",
      "  - \"with\": 0.4156\n",
      "  - \"by\": 1.0000\n",
      "  - \"\"\": 0.2663\n",
      "  - \"at\": 0.3432\n",
      "  - \"he\": 0.2603\n",
      "  - \"from\": 0.3981\n",
      "  - \"be\": 0.2866\n",
      "  - \"this\": 0.3266\n",
      "  - \"i\": 0.1992\n",
      "  - \"an\": 0.3496\n",
      "  - \"his\": 0.2840\n",
      "  - \"are\": 0.3314\n",
      "  - \"not\": 0.2776\n",
      "  - \"has\": 0.3790\n",
      "  - \"have\": 0.3369\n",
      "  - \"but\": 0.2726\n",
      "  - \"or\": 0.2382\n",
      "  - \"utc\": 0.1878\n",
      "  - \"which\": 0.4564\n",
      "  - \"were\": 0.4172\n",
      "  - \"–\": 0.2592\n",
      "  - \"said\": 0.3107\n",
      "  - \"they\": 0.2832\n",
      "  - \"also\": 0.4029\n",
      "  - \"one\": 0.4003\n",
      "  - \"who\": 0.3256\n",
      "  - \"had\": 0.3470\n",
      "  - \"talk\": 0.2320\n",
      "  - \"new\": 0.3231\n",
      "  - \"their\": 0.3336\n",
      "\n",
      "Similarities with \"\"\":\n",
      "  - \",\": 0.3507\n",
      "  - \"the\": 0.3547\n",
      "  - \".\": 0.3175\n",
      "  - \"of\": 0.3230\n",
      "  - \"-\": 0.2872\n",
      "  - \"and\": 0.3856\n",
      "  - \"in\": 0.3309\n",
      "  - \"to\": 0.2845\n",
      "  - \"'\": 0.2645\n",
      "  - \"a\": 0.3399\n",
      "  - \")\": 0.2565\n",
      "  - \"(\": 0.2682\n",
      "  - \"is\": 0.2743\n",
      "  - \"s\": 0.2454\n",
      "  - \"for\": 0.3066\n",
      "  - \"was\": 0.2904\n",
      "  - \"on\": 0.2704\n",
      "  - \"that\": 0.3851\n",
      "  - \"as\": 0.3048\n",
      "  - \"it\": 0.2548\n",
      "  - \"with\": 0.2737\n",
      "  - \"by\": 0.2663\n",
      "  - \"\"\": 1.0000\n",
      "  - \"at\": 0.2113\n",
      "  - \"he\": 0.2767\n",
      "  - \"from\": 0.2612\n",
      "  - \"be\": 0.2389\n",
      "  - \"this\": 0.2880\n",
      "  - \"i\": 0.2783\n",
      "  - \"an\": 0.2753\n",
      "  - \"his\": 0.3284\n",
      "  - \"are\": 0.2901\n",
      "  - \"not\": 0.1880\n",
      "  - \"has\": 0.2632\n",
      "  - \"have\": 0.2679\n",
      "  - \"but\": 0.2651\n",
      "  - \"or\": 0.2733\n",
      "  - \"utc\": 0.2090\n",
      "  - \"which\": 0.3948\n",
      "  - \"were\": 0.2392\n",
      "  - \"–\": 0.2880\n",
      "  - \"said\": 0.4398\n",
      "  - \"they\": 0.2696\n",
      "  - \"also\": 0.3325\n",
      "  - \"one\": 0.3030\n",
      "  - \"who\": 0.2298\n",
      "  - \"had\": 0.2418\n",
      "  - \"talk\": 0.1886\n",
      "  - \"new\": 0.3005\n",
      "  - \"their\": 0.3309\n",
      "\n",
      "Similarities with \"at\":\n",
      "  - \",\": 0.5117\n",
      "  - \"the\": 0.5098\n",
      "  - \".\": 0.5176\n",
      "  - \"of\": 0.4539\n",
      "  - \"-\": 0.4562\n",
      "  - \"and\": 0.4875\n",
      "  - \"in\": 0.5243\n",
      "  - \"to\": 0.4209\n",
      "  - \"'\": 0.3821\n",
      "  - \"a\": 0.4151\n",
      "  - \")\": 0.4194\n",
      "  - \"(\": 0.3914\n",
      "  - \"is\": 0.3633\n",
      "  - \"s\": 0.4059\n",
      "  - \"for\": 0.4654\n",
      "  - \"was\": 0.4334\n",
      "  - \"on\": 0.4705\n",
      "  - \"that\": 0.3396\n",
      "  - \"as\": 0.3423\n",
      "  - \"it\": 0.3499\n",
      "  - \"with\": 0.4052\n",
      "  - \"by\": 0.3432\n",
      "  - \"\"\": 0.2113\n",
      "  - \"at\": 1.0000\n",
      "  - \"he\": 0.4541\n",
      "  - \"from\": 0.4975\n",
      "  - \"be\": 0.2245\n",
      "  - \"this\": 0.3444\n",
      "  - \"i\": 0.2473\n",
      "  - \"an\": 0.3460\n",
      "  - \"his\": 0.3473\n",
      "  - \"are\": 0.3162\n",
      "  - \"not\": 0.2273\n",
      "  - \"has\": 0.3817\n",
      "  - \"have\": 0.3224\n",
      "  - \"but\": 0.3411\n",
      "  - \"or\": 0.2134\n",
      "  - \"utc\": 0.2127\n",
      "  - \"which\": 0.3978\n",
      "  - \"were\": 0.3232\n",
      "  - \"–\": 0.3489\n",
      "  - \"said\": 0.2784\n",
      "  - \"they\": 0.3260\n",
      "  - \"also\": 0.4312\n",
      "  - \"one\": 0.4159\n",
      "  - \"who\": 0.2987\n",
      "  - \"had\": 0.3547\n",
      "  - \"talk\": 0.2474\n",
      "  - \"new\": 0.3295\n",
      "  - \"their\": 0.2814\n",
      "\n",
      "Similarities with \"he\":\n",
      "  - \",\": 0.4664\n",
      "  - \"the\": 0.4513\n",
      "  - \".\": 0.3974\n",
      "  - \"of\": 0.4123\n",
      "  - \"-\": 0.3336\n",
      "  - \"and\": 0.5037\n",
      "  - \"in\": 0.4985\n",
      "  - \"to\": 0.3647\n",
      "  - \"'\": 0.3303\n",
      "  - \"a\": 0.4150\n",
      "  - \")\": 0.3026\n",
      "  - \"(\": 0.2888\n",
      "  - \"is\": 0.3148\n",
      "  - \"s\": 0.3482\n",
      "  - \"for\": 0.4103\n",
      "  - \"was\": 0.4941\n",
      "  - \"on\": 0.3232\n",
      "  - \"that\": 0.3551\n",
      "  - \"as\": 0.3357\n",
      "  - \"it\": 0.4189\n",
      "  - \"with\": 0.3290\n",
      "  - \"by\": 0.2603\n",
      "  - \"\"\": 0.2767\n",
      "  - \"at\": 0.4541\n",
      "  - \"he\": 1.0000\n",
      "  - \"from\": 0.4136\n",
      "  - \"be\": 0.2387\n",
      "  - \"this\": 0.2918\n",
      "  - \"i\": 0.3372\n",
      "  - \"an\": 0.3305\n",
      "  - \"his\": 0.6718\n",
      "  - \"are\": 0.2225\n",
      "  - \"not\": 0.1892\n",
      "  - \"has\": 0.3476\n",
      "  - \"have\": 0.2858\n",
      "  - \"but\": 0.3972\n",
      "  - \"or\": 0.1501\n",
      "  - \"utc\": 0.1672\n",
      "  - \"which\": 0.3633\n",
      "  - \"were\": 0.3242\n",
      "  - \"–\": 0.2829\n",
      "  - \"said\": 0.2856\n",
      "  - \"they\": 0.4326\n",
      "  - \"also\": 0.4187\n",
      "  - \"one\": 0.3874\n",
      "  - \"who\": 0.5640\n",
      "  - \"had\": 0.3924\n",
      "  - \"talk\": 0.1667\n",
      "  - \"new\": 0.2828\n",
      "  - \"their\": 0.3040\n",
      "\n",
      "Similarities with \"from\":\n",
      "  - \",\": 0.4830\n",
      "  - \"the\": 0.4914\n",
      "  - \".\": 0.4680\n",
      "  - \"of\": 0.5273\n",
      "  - \"-\": 0.4166\n",
      "  - \"and\": 0.5066\n",
      "  - \"in\": 0.4991\n",
      "  - \"to\": 0.4317\n",
      "  - \"'\": 0.3471\n",
      "  - \"a\": 0.4113\n",
      "  - \")\": 0.3885\n",
      "  - \"(\": 0.3929\n",
      "  - \"is\": 0.3619\n",
      "  - \"s\": 0.3441\n",
      "  - \"for\": 0.4067\n",
      "  - \"was\": 0.4239\n",
      "  - \"on\": 0.4096\n",
      "  - \"that\": 0.3905\n",
      "  - \"as\": 0.3637\n",
      "  - \"it\": 0.3687\n",
      "  - \"with\": 0.4548\n",
      "  - \"by\": 0.3981\n",
      "  - \"\"\": 0.2612\n",
      "  - \"at\": 0.4975\n",
      "  - \"he\": 0.4136\n",
      "  - \"from\": 1.0000\n",
      "  - \"be\": 0.2271\n",
      "  - \"this\": 0.3277\n",
      "  - \"i\": 0.2379\n",
      "  - \"an\": 0.3671\n",
      "  - \"his\": 0.3798\n",
      "  - \"are\": 0.3398\n",
      "  - \"not\": 0.2040\n",
      "  - \"has\": 0.3753\n",
      "  - \"have\": 0.3511\n",
      "  - \"but\": 0.3707\n",
      "  - \"or\": 0.2654\n",
      "  - \"utc\": 0.1741\n",
      "  - \"which\": 0.4547\n",
      "  - \"were\": 0.3559\n",
      "  - \"–\": 0.3096\n",
      "  - \"said\": 0.3181\n",
      "  - \"they\": 0.3252\n",
      "  - \"also\": 0.4160\n",
      "  - \"one\": 0.3848\n",
      "  - \"who\": 0.3789\n",
      "  - \"had\": 0.3669\n",
      "  - \"talk\": 0.1980\n",
      "  - \"new\": 0.3682\n",
      "  - \"their\": 0.3191\n",
      "\n",
      "Similarities with \"be\":\n",
      "  - \",\": 0.2936\n",
      "  - \"the\": 0.4029\n",
      "  - \".\": 0.3907\n",
      "  - \"of\": 0.2609\n",
      "  - \"-\": 0.2457\n",
      "  - \"and\": 0.3271\n",
      "  - \"in\": 0.3091\n",
      "  - \"to\": 0.3642\n",
      "  - \"'\": 0.2901\n",
      "  - \"a\": 0.3347\n",
      "  - \")\": 0.2113\n",
      "  - \"(\": 0.2730\n",
      "  - \"is\": 0.3733\n",
      "  - \"s\": 0.3054\n",
      "  - \"for\": 0.3158\n",
      "  - \"was\": 0.3801\n",
      "  - \"on\": 0.3192\n",
      "  - \"that\": 0.4753\n",
      "  - \"as\": 0.3606\n",
      "  - \"it\": 0.4483\n",
      "  - \"with\": 0.2806\n",
      "  - \"by\": 0.2866\n",
      "  - \"\"\": 0.2389\n",
      "  - \"at\": 0.2245\n",
      "  - \"he\": 0.2387\n",
      "  - \"from\": 0.2271\n",
      "  - \"be\": 1.0000\n",
      "  - \"this\": 0.4486\n",
      "  - \"i\": 0.3345\n",
      "  - \"an\": 0.2608\n",
      "  - \"his\": 0.2261\n",
      "  - \"are\": 0.4102\n",
      "  - \"not\": 0.4354\n",
      "  - \"has\": 0.3045\n",
      "  - \"have\": 0.5131\n",
      "  - \"but\": 0.4302\n",
      "  - \"or\": 0.4143\n",
      "  - \"utc\": 0.2560\n",
      "  - \"which\": 0.4102\n",
      "  - \"were\": 0.4211\n",
      "  - \"–\": 0.1246\n",
      "  - \"said\": 0.2991\n",
      "  - \"they\": 0.3966\n",
      "  - \"also\": 0.3620\n",
      "  - \"one\": 0.3513\n",
      "  - \"who\": 0.2026\n",
      "  - \"had\": 0.2515\n",
      "  - \"talk\": 0.3275\n",
      "  - \"new\": 0.2219\n",
      "  - \"their\": 0.3468\n",
      "\n",
      "Similarities with \"this\":\n",
      "  - \",\": 0.3865\n",
      "  - \"the\": 0.5624\n",
      "  - \".\": 0.4908\n",
      "  - \"of\": 0.3750\n",
      "  - \"-\": 0.4014\n",
      "  - \"and\": 0.3748\n",
      "  - \"in\": 0.3876\n",
      "  - \"to\": 0.4455\n",
      "  - \"'\": 0.3885\n",
      "  - \"a\": 0.4678\n",
      "  - \")\": 0.3964\n",
      "  - \"(\": 0.3956\n",
      "  - \"is\": 0.3421\n",
      "  - \"s\": 0.3517\n",
      "  - \"for\": 0.3820\n",
      "  - \"was\": 0.3021\n",
      "  - \"on\": 0.3671\n",
      "  - \"that\": 0.5993\n",
      "  - \"as\": 0.3511\n",
      "  - \"it\": 0.6052\n",
      "  - \"with\": 0.3701\n",
      "  - \"by\": 0.3266\n",
      "  - \"\"\": 0.2880\n",
      "  - \"at\": 0.3444\n",
      "  - \"he\": 0.2918\n",
      "  - \"from\": 0.3277\n",
      "  - \"be\": 0.4486\n",
      "  - \"this\": 1.0000\n",
      "  - \"i\": 0.4033\n",
      "  - \"an\": 0.3515\n",
      "  - \"his\": 0.3377\n",
      "  - \"are\": 0.3710\n",
      "  - \"not\": 0.4615\n",
      "  - \"has\": 0.4014\n",
      "  - \"have\": 0.4623\n",
      "  - \"but\": 0.4521\n",
      "  - \"or\": 0.4209\n",
      "  - \"utc\": 0.4021\n",
      "  - \"which\": 0.5679\n",
      "  - \"were\": 0.2871\n",
      "  - \"–\": 0.1735\n",
      "  - \"said\": 0.3700\n",
      "  - \"they\": 0.3588\n",
      "  - \"also\": 0.4470\n",
      "  - \"one\": 0.4543\n",
      "  - \"who\": 0.2629\n",
      "  - \"had\": 0.3135\n",
      "  - \"talk\": 0.4553\n",
      "  - \"new\": 0.3319\n",
      "  - \"their\": 0.3742\n",
      "\n",
      "Similarities with \"i\":\n",
      "  - \",\": 0.2934\n",
      "  - \"the\": 0.2798\n",
      "  - \".\": 0.3447\n",
      "  - \"of\": 0.2294\n",
      "  - \"-\": 0.3232\n",
      "  - \"and\": 0.2676\n",
      "  - \"in\": 0.2306\n",
      "  - \"to\": 0.2898\n",
      "  - \"'\": 0.3301\n",
      "  - \"a\": 0.2837\n",
      "  - \")\": 0.3227\n",
      "  - \"(\": 0.3317\n",
      "  - \"is\": 0.2777\n",
      "  - \"s\": 0.2905\n",
      "  - \"for\": 0.2645\n",
      "  - \"was\": 0.2542\n",
      "  - \"on\": 0.2002\n",
      "  - \"that\": 0.3989\n",
      "  - \"as\": 0.2199\n",
      "  - \"it\": 0.3469\n",
      "  - \"with\": 0.2078\n",
      "  - \"by\": 0.1992\n",
      "  - \"\"\": 0.2783\n",
      "  - \"at\": 0.2473\n",
      "  - \"he\": 0.3372\n",
      "  - \"from\": 0.2379\n",
      "  - \"be\": 0.3345\n",
      "  - \"this\": 0.4033\n",
      "  - \"i\": 1.0000\n",
      "  - \"an\": 0.2224\n",
      "  - \"his\": 0.2472\n",
      "  - \"are\": 0.2790\n",
      "  - \"not\": 0.3457\n",
      "  - \"has\": 0.2194\n",
      "  - \"have\": 0.3972\n",
      "  - \"but\": 0.4432\n",
      "  - \"or\": 0.2584\n",
      "  - \"utc\": 0.3419\n",
      "  - \"which\": 0.2334\n",
      "  - \"were\": 0.2174\n",
      "  - \"–\": 0.2063\n",
      "  - \"said\": 0.3731\n",
      "  - \"they\": 0.4064\n",
      "  - \"also\": 0.3017\n",
      "  - \"one\": 0.3112\n",
      "  - \"who\": 0.2943\n",
      "  - \"had\": 0.2549\n",
      "  - \"talk\": 0.4292\n",
      "  - \"new\": 0.2233\n",
      "  - \"their\": 0.2193\n",
      "\n",
      "Similarities with \"an\":\n",
      "  - \",\": 0.4290\n",
      "  - \"the\": 0.5235\n",
      "  - \".\": 0.4060\n",
      "  - \"of\": 0.4119\n",
      "  - \"-\": 0.3810\n",
      "  - \"and\": 0.4609\n",
      "  - \"in\": 0.4128\n",
      "  - \"to\": 0.3668\n",
      "  - \"'\": 0.4047\n",
      "  - \"a\": 0.5197\n",
      "  - \")\": 0.3219\n",
      "  - \"(\": 0.3330\n",
      "  - \"is\": 0.4810\n",
      "  - \"s\": 0.3569\n",
      "  - \"for\": 0.3802\n",
      "  - \"was\": 0.4060\n",
      "  - \"on\": 0.3580\n",
      "  - \"that\": 0.4072\n",
      "  - \"as\": 0.3673\n",
      "  - \"it\": 0.4183\n",
      "  - \"with\": 0.3966\n",
      "  - \"by\": 0.3496\n",
      "  - \"\"\": 0.2753\n",
      "  - \"at\": 0.3460\n",
      "  - \"he\": 0.3305\n",
      "  - \"from\": 0.3671\n",
      "  - \"be\": 0.2608\n",
      "  - \"this\": 0.3515\n",
      "  - \"i\": 0.2224\n",
      "  - \"an\": 1.0000\n",
      "  - \"his\": 0.3208\n",
      "  - \"are\": 0.2883\n",
      "  - \"not\": 0.3164\n",
      "  - \"has\": 0.3500\n",
      "  - \"have\": 0.2807\n",
      "  - \"but\": 0.3380\n",
      "  - \"or\": 0.3149\n",
      "  - \"utc\": 0.1510\n",
      "  - \"which\": 0.4345\n",
      "  - \"were\": 0.2443\n",
      "  - \"–\": 0.2469\n",
      "  - \"said\": 0.3442\n",
      "  - \"they\": 0.2894\n",
      "  - \"also\": 0.4155\n",
      "  - \"one\": 0.4193\n",
      "  - \"who\": 0.3418\n",
      "  - \"had\": 0.3085\n",
      "  - \"talk\": 0.2158\n",
      "  - \"new\": 0.3204\n",
      "  - \"their\": 0.3010\n",
      "\n",
      "Similarities with \"his\":\n",
      "  - \",\": 0.4085\n",
      "  - \"the\": 0.4642\n",
      "  - \".\": 0.3845\n",
      "  - \"of\": 0.3677\n",
      "  - \"-\": 0.3274\n",
      "  - \"and\": 0.4821\n",
      "  - \"in\": 0.4220\n",
      "  - \"to\": 0.3972\n",
      "  - \"'\": 0.2922\n",
      "  - \"a\": 0.3884\n",
      "  - \")\": 0.1946\n",
      "  - \"(\": 0.2103\n",
      "  - \"is\": 0.2667\n",
      "  - \"s\": 0.4312\n",
      "  - \"for\": 0.3596\n",
      "  - \"was\": 0.4279\n",
      "  - \"on\": 0.3316\n",
      "  - \"that\": 0.3790\n",
      "  - \"as\": 0.3112\n",
      "  - \"it\": 0.2834\n",
      "  - \"with\": 0.3652\n",
      "  - \"by\": 0.2840\n",
      "  - \"\"\": 0.3284\n",
      "  - \"at\": 0.3473\n",
      "  - \"he\": 0.6718\n",
      "  - \"from\": 0.3798\n",
      "  - \"be\": 0.2261\n",
      "  - \"this\": 0.3377\n",
      "  - \"i\": 0.2472\n",
      "  - \"an\": 0.3208\n",
      "  - \"his\": 1.0000\n",
      "  - \"are\": 0.2106\n",
      "  - \"not\": 0.2352\n",
      "  - \"has\": 0.3400\n",
      "  - \"have\": 0.2781\n",
      "  - \"but\": 0.3962\n",
      "  - \"or\": 0.1892\n",
      "  - \"utc\": 0.1727\n",
      "  - \"which\": 0.4056\n",
      "  - \"were\": 0.3030\n",
      "  - \"–\": 0.2412\n",
      "  - \"said\": 0.3433\n",
      "  - \"they\": 0.3635\n",
      "  - \"also\": 0.4229\n",
      "  - \"one\": 0.3654\n",
      "  - \"who\": 0.4254\n",
      "  - \"had\": 0.4113\n",
      "  - \"talk\": 0.1721\n",
      "  - \"new\": 0.2849\n",
      "  - \"their\": 0.5999\n",
      "\n",
      "Similarities with \"are\":\n",
      "  - \",\": 0.4065\n",
      "  - \"the\": 0.4323\n",
      "  - \".\": 0.4112\n",
      "  - \"of\": 0.4061\n",
      "  - \"-\": 0.3836\n",
      "  - \"and\": 0.4727\n",
      "  - \"in\": 0.3343\n",
      "  - \"to\": 0.3624\n",
      "  - \"'\": 0.3215\n",
      "  - \"a\": 0.3544\n",
      "  - \")\": 0.3034\n",
      "  - \"(\": 0.3167\n",
      "  - \"is\": 0.5279\n",
      "  - \"s\": 0.2989\n",
      "  - \"for\": 0.3719\n",
      "  - \"was\": 0.3241\n",
      "  - \"on\": 0.3436\n",
      "  - \"that\": 0.3965\n",
      "  - \"as\": 0.3464\n",
      "  - \"it\": 0.3888\n",
      "  - \"with\": 0.4395\n",
      "  - \"by\": 0.3314\n",
      "  - \"\"\": 0.2901\n",
      "  - \"at\": 0.3162\n",
      "  - \"he\": 0.2225\n",
      "  - \"from\": 0.3398\n",
      "  - \"be\": 0.4102\n",
      "  - \"this\": 0.3710\n",
      "  - \"i\": 0.2790\n",
      "  - \"an\": 0.2883\n",
      "  - \"his\": 0.2106\n",
      "  - \"are\": 1.0000\n",
      "  - \"not\": 0.3520\n",
      "  - \"has\": 0.3823\n",
      "  - \"have\": 0.5541\n",
      "  - \"but\": 0.3767\n",
      "  - \"or\": 0.4453\n",
      "  - \"utc\": 0.2372\n",
      "  - \"which\": 0.3954\n",
      "  - \"were\": 0.7092\n",
      "  - \"–\": 0.1889\n",
      "  - \"said\": 0.2158\n",
      "  - \"they\": 0.4418\n",
      "  - \"also\": 0.3793\n",
      "  - \"one\": 0.4321\n",
      "  - \"who\": 0.1894\n",
      "  - \"had\": 0.3417\n",
      "  - \"talk\": 0.2958\n",
      "  - \"new\": 0.2501\n",
      "  - \"their\": 0.4480\n",
      "\n",
      "Similarities with \"not\":\n",
      "  - \",\": 0.3309\n",
      "  - \"the\": 0.4128\n",
      "  - \".\": 0.4074\n",
      "  - \"of\": 0.3210\n",
      "  - \"-\": 0.3146\n",
      "  - \"and\": 0.2993\n",
      "  - \"in\": 0.2867\n",
      "  - \"to\": 0.3926\n",
      "  - \"'\": 0.3124\n",
      "  - \"a\": 0.3220\n",
      "  - \")\": 0.2591\n",
      "  - \"(\": 0.2794\n",
      "  - \"is\": 0.3399\n",
      "  - \"s\": 0.2731\n",
      "  - \"for\": 0.3433\n",
      "  - \"was\": 0.2393\n",
      "  - \"on\": 0.3073\n",
      "  - \"that\": 0.5074\n",
      "  - \"as\": 0.3524\n",
      "  - \"it\": 0.4246\n",
      "  - \"with\": 0.2955\n",
      "  - \"by\": 0.2776\n",
      "  - \"\"\": 0.1880\n",
      "  - \"at\": 0.2273\n",
      "  - \"he\": 0.1892\n",
      "  - \"from\": 0.2040\n",
      "  - \"be\": 0.4354\n",
      "  - \"this\": 0.4615\n",
      "  - \"i\": 0.3457\n",
      "  - \"an\": 0.3164\n",
      "  - \"his\": 0.2352\n",
      "  - \"are\": 0.3520\n",
      "  - \"not\": 1.0000\n",
      "  - \"has\": 0.2955\n",
      "  - \"have\": 0.4176\n",
      "  - \"but\": 0.4288\n",
      "  - \"or\": 0.4141\n",
      "  - \"utc\": 0.2886\n",
      "  - \"which\": 0.3630\n",
      "  - \"were\": 0.2299\n",
      "  - \"–\": 0.1130\n",
      "  - \"said\": 0.3548\n",
      "  - \"they\": 0.3792\n",
      "  - \"also\": 0.3889\n",
      "  - \"one\": 0.3958\n",
      "  - \"who\": 0.1929\n",
      "  - \"had\": 0.2611\n",
      "  - \"talk\": 0.3692\n",
      "  - \"new\": 0.2526\n",
      "  - \"their\": 0.3176\n",
      "\n",
      "Similarities with \"has\":\n",
      "  - \",\": 0.4605\n",
      "  - \"the\": 0.4344\n",
      "  - \".\": 0.4610\n",
      "  - \"of\": 0.3852\n",
      "  - \"-\": 0.3589\n",
      "  - \"and\": 0.4638\n",
      "  - \"in\": 0.4603\n",
      "  - \"to\": 0.3517\n",
      "  - \"'\": 0.3341\n",
      "  - \"a\": 0.4008\n",
      "  - \")\": 0.3670\n",
      "  - \"(\": 0.3531\n",
      "  - \"is\": 0.5250\n",
      "  - \"s\": 0.3621\n",
      "  - \"for\": 0.3980\n",
      "  - \"was\": 0.4701\n",
      "  - \"on\": 0.3558\n",
      "  - \"that\": 0.4058\n",
      "  - \"as\": 0.3553\n",
      "  - \"it\": 0.3599\n",
      "  - \"with\": 0.4468\n",
      "  - \"by\": 0.3790\n",
      "  - \"\"\": 0.2632\n",
      "  - \"at\": 0.3817\n",
      "  - \"he\": 0.3476\n",
      "  - \"from\": 0.3753\n",
      "  - \"be\": 0.3045\n",
      "  - \"this\": 0.4014\n",
      "  - \"i\": 0.2194\n",
      "  - \"an\": 0.3500\n",
      "  - \"his\": 0.3400\n",
      "  - \"are\": 0.3823\n",
      "  - \"not\": 0.2955\n",
      "  - \"has\": 1.0000\n",
      "  - \"have\": 0.6437\n",
      "  - \"but\": 0.3196\n",
      "  - \"or\": 0.2435\n",
      "  - \"utc\": 0.2571\n",
      "  - \"which\": 0.4193\n",
      "  - \"were\": 0.2805\n",
      "  - \"–\": 0.2046\n",
      "  - \"said\": 0.2939\n",
      "  - \"they\": 0.3009\n",
      "  - \"also\": 0.4369\n",
      "  - \"one\": 0.3687\n",
      "  - \"who\": 0.2781\n",
      "  - \"had\": 0.6138\n",
      "  - \"talk\": 0.2971\n",
      "  - \"new\": 0.3130\n",
      "  - \"their\": 0.2724\n",
      "\n",
      "Similarities with \"have\":\n",
      "  - \",\": 0.3552\n",
      "  - \"the\": 0.4200\n",
      "  - \".\": 0.4177\n",
      "  - \"of\": 0.3464\n",
      "  - \"-\": 0.3116\n",
      "  - \"and\": 0.4441\n",
      "  - \"in\": 0.3587\n",
      "  - \"to\": 0.3873\n",
      "  - \"'\": 0.3281\n",
      "  - \"a\": 0.3516\n",
      "  - \")\": 0.2831\n",
      "  - \"(\": 0.2989\n",
      "  - \"is\": 0.3340\n",
      "  - \"s\": 0.3159\n",
      "  - \"for\": 0.3770\n",
      "  - \"was\": 0.3196\n",
      "  - \"on\": 0.3485\n",
      "  - \"that\": 0.5365\n",
      "  - \"as\": 0.3407\n",
      "  - \"it\": 0.4107\n",
      "  - \"with\": 0.4165\n",
      "  - \"by\": 0.3369\n",
      "  - \"\"\": 0.2679\n",
      "  - \"at\": 0.3224\n",
      "  - \"he\": 0.2858\n",
      "  - \"from\": 0.3511\n",
      "  - \"be\": 0.5131\n",
      "  - \"this\": 0.4623\n",
      "  - \"i\": 0.3972\n",
      "  - \"an\": 0.2807\n",
      "  - \"his\": 0.2781\n",
      "  - \"are\": 0.5541\n",
      "  - \"not\": 0.4176\n",
      "  - \"has\": 0.6437\n",
      "  - \"have\": 1.0000\n",
      "  - \"but\": 0.4902\n",
      "  - \"or\": 0.4003\n",
      "  - \"utc\": 0.2803\n",
      "  - \"which\": 0.4588\n",
      "  - \"were\": 0.5037\n",
      "  - \"–\": 0.1867\n",
      "  - \"said\": 0.3551\n",
      "  - \"they\": 0.5029\n",
      "  - \"also\": 0.4147\n",
      "  - \"one\": 0.4003\n",
      "  - \"who\": 0.3387\n",
      "  - \"had\": 0.6569\n",
      "  - \"talk\": 0.3392\n",
      "  - \"new\": 0.2782\n",
      "  - \"their\": 0.4415\n",
      "\n",
      "Similarities with \"but\":\n",
      "  - \",\": 0.3084\n",
      "  - \"the\": 0.4758\n",
      "  - \".\": 0.4602\n",
      "  - \"of\": 0.3688\n",
      "  - \"-\": 0.3343\n",
      "  - \"and\": 0.5961\n",
      "  - \"in\": 0.3873\n",
      "  - \"to\": 0.5141\n",
      "  - \"'\": 0.3604\n",
      "  - \"a\": 0.4337\n",
      "  - \")\": 0.2025\n",
      "  - \"(\": 0.2536\n",
      "  - \"is\": 0.3657\n",
      "  - \"s\": 0.3293\n",
      "  - \"for\": 0.3847\n",
      "  - \"was\": 0.3688\n",
      "  - \"on\": 0.3520\n",
      "  - \"that\": 0.6532\n",
      "  - \"as\": 0.3820\n",
      "  - \"it\": 0.4901\n",
      "  - \"with\": 0.4127\n",
      "  - \"by\": 0.2726\n",
      "  - \"\"\": 0.2651\n",
      "  - \"at\": 0.3411\n",
      "  - \"he\": 0.3972\n",
      "  - \"from\": 0.3707\n",
      "  - \"be\": 0.4302\n",
      "  - \"this\": 0.4521\n",
      "  - \"i\": 0.4432\n",
      "  - \"an\": 0.3380\n",
      "  - \"his\": 0.3962\n",
      "  - \"are\": 0.3767\n",
      "  - \"not\": 0.4288\n",
      "  - \"has\": 0.3196\n",
      "  - \"have\": 0.4902\n",
      "  - \"but\": 1.0000\n",
      "  - \"or\": 0.4411\n",
      "  - \"utc\": 0.2932\n",
      "  - \"which\": 0.6801\n",
      "  - \"were\": 0.3818\n",
      "  - \"–\": 0.2177\n",
      "  - \"said\": 0.4863\n",
      "  - \"they\": 0.5319\n",
      "  - \"also\": 0.4027\n",
      "  - \"one\": 0.4309\n",
      "  - \"who\": 0.4598\n",
      "  - \"had\": 0.4170\n",
      "  - \"talk\": 0.3449\n",
      "  - \"new\": 0.2720\n",
      "  - \"their\": 0.4042\n",
      "\n",
      "Similarities with \"or\":\n",
      "  - \",\": 0.3191\n",
      "  - \"the\": 0.4006\n",
      "  - \".\": 0.3740\n",
      "  - \"of\": 0.3538\n",
      "  - \"-\": 0.3519\n",
      "  - \"and\": 0.4796\n",
      "  - \"in\": 0.2872\n",
      "  - \"to\": 0.3890\n",
      "  - \"'\": 0.3643\n",
      "  - \"a\": 0.4007\n",
      "  - \")\": 0.3415\n",
      "  - \"(\": 0.3597\n",
      "  - \"is\": 0.3357\n",
      "  - \"s\": 0.3015\n",
      "  - \"for\": 0.3300\n",
      "  - \"was\": 0.2025\n",
      "  - \"on\": 0.2902\n",
      "  - \"that\": 0.4106\n",
      "  - \"as\": 0.3621\n",
      "  - \"it\": 0.3481\n",
      "  - \"with\": 0.3554\n",
      "  - \"by\": 0.2382\n",
      "  - \"\"\": 0.2733\n",
      "  - \"at\": 0.2134\n",
      "  - \"he\": 0.1501\n",
      "  - \"from\": 0.2654\n",
      "  - \"be\": 0.4143\n",
      "  - \"this\": 0.4209\n",
      "  - \"i\": 0.2584\n",
      "  - \"an\": 0.3149\n",
      "  - \"his\": 0.1892\n",
      "  - \"are\": 0.4453\n",
      "  - \"not\": 0.4141\n",
      "  - \"has\": 0.2435\n",
      "  - \"have\": 0.4003\n",
      "  - \"but\": 0.4411\n",
      "  - \"or\": 1.0000\n",
      "  - \"utc\": 0.2652\n",
      "  - \"which\": 0.4334\n",
      "  - \"were\": 0.3190\n",
      "  - \"–\": 0.1661\n",
      "  - \"said\": 0.2651\n",
      "  - \"they\": 0.3550\n",
      "  - \"also\": 0.3621\n",
      "  - \"one\": 0.3952\n",
      "  - \"who\": 0.2210\n",
      "  - \"had\": 0.2500\n",
      "  - \"talk\": 0.3030\n",
      "  - \"new\": 0.1764\n",
      "  - \"their\": 0.3271\n",
      "\n",
      "Similarities with \"utc\":\n",
      "  - \",\": 0.3256\n",
      "  - \"the\": 0.2050\n",
      "  - \".\": 0.3789\n",
      "  - \"of\": 0.1789\n",
      "  - \"-\": 0.3464\n",
      "  - \"and\": 0.2434\n",
      "  - \"in\": 0.2020\n",
      "  - \"to\": 0.2090\n",
      "  - \"'\": 0.2767\n",
      "  - \"a\": 0.1852\n",
      "  - \")\": 0.4105\n",
      "  - \"(\": 0.4313\n",
      "  - \"is\": 0.2189\n",
      "  - \"s\": 0.1715\n",
      "  - \"for\": 0.2402\n",
      "  - \"was\": 0.1760\n",
      "  - \"on\": 0.1983\n",
      "  - \"that\": 0.2412\n",
      "  - \"as\": 0.2208\n",
      "  - \"it\": 0.2442\n",
      "  - \"with\": 0.1654\n",
      "  - \"by\": 0.1878\n",
      "  - \"\"\": 0.2090\n",
      "  - \"at\": 0.2127\n",
      "  - \"he\": 0.1672\n",
      "  - \"from\": 0.1741\n",
      "  - \"be\": 0.2560\n",
      "  - \"this\": 0.4021\n",
      "  - \"i\": 0.3419\n",
      "  - \"an\": 0.1510\n",
      "  - \"his\": 0.1727\n",
      "  - \"are\": 0.2372\n",
      "  - \"not\": 0.2886\n",
      "  - \"has\": 0.2571\n",
      "  - \"have\": 0.2803\n",
      "  - \"but\": 0.2932\n",
      "  - \"or\": 0.2652\n",
      "  - \"utc\": 1.0000\n",
      "  - \"which\": 0.2403\n",
      "  - \"were\": 0.1357\n",
      "  - \"–\": 0.2553\n",
      "  - \"said\": 0.1702\n",
      "  - \"they\": 0.1739\n",
      "  - \"also\": 0.1559\n",
      "  - \"one\": 0.2110\n",
      "  - \"who\": 0.2416\n",
      "  - \"had\": 0.1234\n",
      "  - \"talk\": 0.6616\n",
      "  - \"new\": 0.1825\n",
      "  - \"their\": 0.1598\n",
      "\n",
      "Similarities with \"which\":\n",
      "  - \",\": 0.3898\n",
      "  - \"the\": 0.6465\n",
      "  - \".\": 0.4946\n",
      "  - \"of\": 0.5006\n",
      "  - \"-\": 0.3681\n",
      "  - \"and\": 0.6658\n",
      "  - \"in\": 0.4883\n",
      "  - \"to\": 0.4803\n",
      "  - \"'\": 0.3376\n",
      "  - \"a\": 0.5183\n",
      "  - \")\": 0.3048\n",
      "  - \"(\": 0.2855\n",
      "  - \"is\": 0.4054\n",
      "  - \"s\": 0.3497\n",
      "  - \"for\": 0.4294\n",
      "  - \"was\": 0.3848\n",
      "  - \"on\": 0.4530\n",
      "  - \"that\": 0.7366\n",
      "  - \"as\": 0.4127\n",
      "  - \"it\": 0.5677\n",
      "  - \"with\": 0.5273\n",
      "  - \"by\": 0.4564\n",
      "  - \"\"\": 0.3948\n",
      "  - \"at\": 0.3978\n",
      "  - \"he\": 0.3633\n",
      "  - \"from\": 0.4547\n",
      "  - \"be\": 0.4102\n",
      "  - \"this\": 0.5679\n",
      "  - \"i\": 0.2334\n",
      "  - \"an\": 0.4345\n",
      "  - \"his\": 0.4056\n",
      "  - \"are\": 0.3954\n",
      "  - \"not\": 0.3630\n",
      "  - \"has\": 0.4193\n",
      "  - \"have\": 0.4588\n",
      "  - \"but\": 0.6801\n",
      "  - \"or\": 0.4334\n",
      "  - \"utc\": 0.2403\n",
      "  - \"which\": 1.0000\n",
      "  - \"were\": 0.3714\n",
      "  - \"–\": 0.2389\n",
      "  - \"said\": 0.3852\n",
      "  - \"they\": 0.4494\n",
      "  - \"also\": 0.5814\n",
      "  - \"one\": 0.4825\n",
      "  - \"who\": 0.4463\n",
      "  - \"had\": 0.3525\n",
      "  - \"talk\": 0.2714\n",
      "  - \"new\": 0.3693\n",
      "  - \"their\": 0.4440\n",
      "\n",
      "Similarities with \"were\":\n",
      "  - \",\": 0.3851\n",
      "  - \"the\": 0.4365\n",
      "  - \".\": 0.4073\n",
      "  - \"of\": 0.3664\n",
      "  - \"-\": 0.3254\n",
      "  - \"and\": 0.4625\n",
      "  - \"in\": 0.3627\n",
      "  - \"to\": 0.3728\n",
      "  - \"'\": 0.2683\n",
      "  - \"a\": 0.2746\n",
      "  - \")\": 0.2244\n",
      "  - \"(\": 0.2620\n",
      "  - \"is\": 0.2729\n",
      "  - \"s\": 0.3258\n",
      "  - \"for\": 0.3669\n",
      "  - \"was\": 0.5998\n",
      "  - \"on\": 0.3637\n",
      "  - \"that\": 0.3538\n",
      "  - \"as\": 0.3336\n",
      "  - \"it\": 0.3291\n",
      "  - \"with\": 0.3935\n",
      "  - \"by\": 0.4172\n",
      "  - \"\"\": 0.2392\n",
      "  - \"at\": 0.3232\n",
      "  - \"he\": 0.3242\n",
      "  - \"from\": 0.3559\n",
      "  - \"be\": 0.4211\n",
      "  - \"this\": 0.2871\n",
      "  - \"i\": 0.2174\n",
      "  - \"an\": 0.2443\n",
      "  - \"his\": 0.3030\n",
      "  - \"are\": 0.7092\n",
      "  - \"not\": 0.2299\n",
      "  - \"has\": 0.2805\n",
      "  - \"have\": 0.5037\n",
      "  - \"but\": 0.3818\n",
      "  - \"or\": 0.3190\n",
      "  - \"utc\": 0.1357\n",
      "  - \"which\": 0.3714\n",
      "  - \"were\": 1.0000\n",
      "  - \"–\": 0.2253\n",
      "  - \"said\": 0.2623\n",
      "  - \"they\": 0.4877\n",
      "  - \"also\": 0.3338\n",
      "  - \"one\": 0.4016\n",
      "  - \"who\": 0.2689\n",
      "  - \"had\": 0.6114\n",
      "  - \"talk\": 0.1939\n",
      "  - \"new\": 0.2619\n",
      "  - \"their\": 0.4906\n",
      "\n",
      "Similarities with \"–\":\n",
      "  - \",\": 0.4485\n",
      "  - \"the\": 0.3194\n",
      "  - \".\": 0.4081\n",
      "  - \"of\": 0.3121\n",
      "  - \"-\": 0.4731\n",
      "  - \"and\": 0.4187\n",
      "  - \"in\": 0.3037\n",
      "  - \"to\": 0.2479\n",
      "  - \"'\": 0.4101\n",
      "  - \"a\": 0.2607\n",
      "  - \")\": 0.5241\n",
      "  - \"(\": 0.5239\n",
      "  - \"is\": 0.2517\n",
      "  - \"s\": 0.2846\n",
      "  - \"for\": 0.3485\n",
      "  - \"was\": 0.3205\n",
      "  - \"on\": 0.2296\n",
      "  - \"that\": 0.1914\n",
      "  - \"as\": 0.2881\n",
      "  - \"it\": 0.1565\n",
      "  - \"with\": 0.2693\n",
      "  - \"by\": 0.2592\n",
      "  - \"\"\": 0.2880\n",
      "  - \"at\": 0.3489\n",
      "  - \"he\": 0.2829\n",
      "  - \"from\": 0.3096\n",
      "  - \"be\": 0.1246\n",
      "  - \"this\": 0.1735\n",
      "  - \"i\": 0.2063\n",
      "  - \"an\": 0.2469\n",
      "  - \"his\": 0.2412\n",
      "  - \"are\": 0.1889\n",
      "  - \"not\": 0.1130\n",
      "  - \"has\": 0.2046\n",
      "  - \"have\": 0.1867\n",
      "  - \"but\": 0.2177\n",
      "  - \"or\": 0.1661\n",
      "  - \"utc\": 0.2553\n",
      "  - \"which\": 0.2389\n",
      "  - \"were\": 0.2253\n",
      "  - \"–\": 1.0000\n",
      "  - \"said\": 0.1693\n",
      "  - \"they\": 0.1853\n",
      "  - \"also\": 0.2492\n",
      "  - \"one\": 0.2244\n",
      "  - \"who\": 0.3039\n",
      "  - \"had\": 0.1982\n",
      "  - \"talk\": 0.2591\n",
      "  - \"new\": 0.2456\n",
      "  - \"their\": 0.2003\n",
      "\n",
      "Similarities with \"said\":\n",
      "  - \",\": 0.3667\n",
      "  - \"the\": 0.3810\n",
      "  - \".\": 0.4129\n",
      "  - \"of\": 0.3073\n",
      "  - \"-\": 0.3348\n",
      "  - \"and\": 0.3821\n",
      "  - \"in\": 0.2904\n",
      "  - \"to\": 0.4054\n",
      "  - \"'\": 0.3478\n",
      "  - \"a\": 0.3443\n",
      "  - \")\": 0.2439\n",
      "  - \"(\": 0.2436\n",
      "  - \"is\": 0.3072\n",
      "  - \"s\": 0.3839\n",
      "  - \"for\": 0.3013\n",
      "  - \"was\": 0.3526\n",
      "  - \"on\": 0.3099\n",
      "  - \"that\": 0.5332\n",
      "  - \"as\": 0.3537\n",
      "  - \"it\": 0.3132\n",
      "  - \"with\": 0.3487\n",
      "  - \"by\": 0.3107\n",
      "  - \"\"\": 0.4398\n",
      "  - \"at\": 0.2784\n",
      "  - \"he\": 0.2856\n",
      "  - \"from\": 0.3181\n",
      "  - \"be\": 0.2991\n",
      "  - \"this\": 0.3700\n",
      "  - \"i\": 0.3731\n",
      "  - \"an\": 0.3442\n",
      "  - \"his\": 0.3433\n",
      "  - \"are\": 0.2158\n",
      "  - \"not\": 0.3548\n",
      "  - \"has\": 0.2939\n",
      "  - \"have\": 0.3551\n",
      "  - \"but\": 0.4863\n",
      "  - \"or\": 0.2651\n",
      "  - \"utc\": 0.1702\n",
      "  - \"which\": 0.3852\n",
      "  - \"were\": 0.2623\n",
      "  - \"–\": 0.1693\n",
      "  - \"said\": 1.0000\n",
      "  - \"they\": 0.3477\n",
      "  - \"also\": 0.4311\n",
      "  - \"one\": 0.3601\n",
      "  - \"who\": 0.3370\n",
      "  - \"had\": 0.4499\n",
      "  - \"talk\": 0.2212\n",
      "  - \"new\": 0.2427\n",
      "  - \"their\": 0.2723\n",
      "\n",
      "Similarities with \"they\":\n",
      "  - \",\": 0.3911\n",
      "  - \"the\": 0.4527\n",
      "  - \".\": 0.3677\n",
      "  - \"of\": 0.3076\n",
      "  - \"-\": 0.3213\n",
      "  - \"and\": 0.4807\n",
      "  - \"in\": 0.3937\n",
      "  - \"to\": 0.4824\n",
      "  - \"'\": 0.3221\n",
      "  - \"a\": 0.3485\n",
      "  - \")\": 0.2250\n",
      "  - \"(\": 0.2134\n",
      "  - \"is\": 0.2942\n",
      "  - \"s\": 0.3007\n",
      "  - \"for\": 0.3271\n",
      "  - \"was\": 0.3394\n",
      "  - \"on\": 0.3236\n",
      "  - \"that\": 0.5066\n",
      "  - \"as\": 0.3107\n",
      "  - \"it\": 0.4417\n",
      "  - \"with\": 0.3914\n",
      "  - \"by\": 0.2832\n",
      "  - \"\"\": 0.2696\n",
      "  - \"at\": 0.3260\n",
      "  - \"he\": 0.4326\n",
      "  - \"from\": 0.3252\n",
      "  - \"be\": 0.3966\n",
      "  - \"this\": 0.3588\n",
      "  - \"i\": 0.4064\n",
      "  - \"an\": 0.2894\n",
      "  - \"his\": 0.3635\n",
      "  - \"are\": 0.4418\n",
      "  - \"not\": 0.3792\n",
      "  - \"has\": 0.3009\n",
      "  - \"have\": 0.5029\n",
      "  - \"but\": 0.5319\n",
      "  - \"or\": 0.3550\n",
      "  - \"utc\": 0.1739\n",
      "  - \"which\": 0.4494\n",
      "  - \"were\": 0.4877\n",
      "  - \"–\": 0.1853\n",
      "  - \"said\": 0.3477\n",
      "  - \"they\": 1.0000\n",
      "  - \"also\": 0.3864\n",
      "  - \"one\": 0.3480\n",
      "  - \"who\": 0.4517\n",
      "  - \"had\": 0.4152\n",
      "  - \"talk\": 0.2122\n",
      "  - \"new\": 0.2983\n",
      "  - \"their\": 0.6740\n",
      "\n",
      "Similarities with \"also\":\n",
      "  - \",\": 0.5216\n",
      "  - \"the\": 0.5490\n",
      "  - \".\": 0.4873\n",
      "  - \"of\": 0.4869\n",
      "  - \"-\": 0.4237\n",
      "  - \"and\": 0.6205\n",
      "  - \"in\": 0.4976\n",
      "  - \"to\": 0.4587\n",
      "  - \"'\": 0.4129\n",
      "  - \"a\": 0.4644\n",
      "  - \")\": 0.3228\n",
      "  - \"(\": 0.2875\n",
      "  - \"is\": 0.4076\n",
      "  - \"s\": 0.3943\n",
      "  - \"for\": 0.5157\n",
      "  - \"was\": 0.4157\n",
      "  - \"on\": 0.4097\n",
      "  - \"that\": 0.5833\n",
      "  - \"as\": 0.4897\n",
      "  - \"it\": 0.3991\n",
      "  - \"with\": 0.4630\n",
      "  - \"by\": 0.4029\n",
      "  - \"\"\": 0.3325\n",
      "  - \"at\": 0.4312\n",
      "  - \"he\": 0.4187\n",
      "  - \"from\": 0.4160\n",
      "  - \"be\": 0.3620\n",
      "  - \"this\": 0.4470\n",
      "  - \"i\": 0.3017\n",
      "  - \"an\": 0.4155\n",
      "  - \"his\": 0.4229\n",
      "  - \"are\": 0.3793\n",
      "  - \"not\": 0.3889\n",
      "  - \"has\": 0.4369\n",
      "  - \"have\": 0.4147\n",
      "  - \"but\": 0.4027\n",
      "  - \"or\": 0.3621\n",
      "  - \"utc\": 0.1559\n",
      "  - \"which\": 0.5814\n",
      "  - \"were\": 0.3338\n",
      "  - \"–\": 0.2492\n",
      "  - \"said\": 0.4311\n",
      "  - \"they\": 0.3864\n",
      "  - \"also\": 1.0000\n",
      "  - \"one\": 0.4697\n",
      "  - \"who\": 0.3746\n",
      "  - \"had\": 0.3552\n",
      "  - \"talk\": 0.2280\n",
      "  - \"new\": 0.3177\n",
      "  - \"their\": 0.4146\n",
      "\n",
      "Similarities with \"one\":\n",
      "  - \",\": 0.4630\n",
      "  - \"the\": 0.6059\n",
      "  - \".\": 0.4869\n",
      "  - \"of\": 0.4692\n",
      "  - \"-\": 0.4412\n",
      "  - \"and\": 0.4881\n",
      "  - \"in\": 0.4364\n",
      "  - \"to\": 0.4380\n",
      "  - \"'\": 0.3987\n",
      "  - \"a\": 0.5032\n",
      "  - \")\": 0.3461\n",
      "  - \"(\": 0.3300\n",
      "  - \"is\": 0.4067\n",
      "  - \"s\": 0.3819\n",
      "  - \"for\": 0.4635\n",
      "  - \"was\": 0.3957\n",
      "  - \"on\": 0.4198\n",
      "  - \"that\": 0.4954\n",
      "  - \"as\": 0.4024\n",
      "  - \"it\": 0.4653\n",
      "  - \"with\": 0.4398\n",
      "  - \"by\": 0.4003\n",
      "  - \"\"\": 0.3030\n",
      "  - \"at\": 0.4159\n",
      "  - \"he\": 0.3874\n",
      "  - \"from\": 0.3848\n",
      "  - \"be\": 0.3513\n",
      "  - \"this\": 0.4543\n",
      "  - \"i\": 0.3112\n",
      "  - \"an\": 0.4193\n",
      "  - \"his\": 0.3654\n",
      "  - \"are\": 0.4321\n",
      "  - \"not\": 0.3958\n",
      "  - \"has\": 0.3687\n",
      "  - \"have\": 0.4003\n",
      "  - \"but\": 0.4309\n",
      "  - \"or\": 0.3952\n",
      "  - \"utc\": 0.2110\n",
      "  - \"which\": 0.4825\n",
      "  - \"were\": 0.4016\n",
      "  - \"–\": 0.2244\n",
      "  - \"said\": 0.3601\n",
      "  - \"they\": 0.3480\n",
      "  - \"also\": 0.4697\n",
      "  - \"one\": 1.0000\n",
      "  - \"who\": 0.3309\n",
      "  - \"had\": 0.3896\n",
      "  - \"talk\": 0.2544\n",
      "  - \"new\": 0.3428\n",
      "  - \"their\": 0.4028\n",
      "\n",
      "Similarities with \"who\":\n",
      "  - \",\": 0.3217\n",
      "  - \"the\": 0.3617\n",
      "  - \".\": 0.3527\n",
      "  - \"of\": 0.3380\n",
      "  - \"-\": 0.2722\n",
      "  - \"and\": 0.4885\n",
      "  - \"in\": 0.3571\n",
      "  - \"to\": 0.3530\n",
      "  - \"'\": 0.3351\n",
      "  - \"a\": 0.3611\n",
      "  - \")\": 0.3488\n",
      "  - \"(\": 0.3307\n",
      "  - \"is\": 0.3073\n",
      "  - \"s\": 0.2977\n",
      "  - \"for\": 0.3697\n",
      "  - \"was\": 0.3467\n",
      "  - \"on\": 0.2817\n",
      "  - \"that\": 0.4302\n",
      "  - \"as\": 0.3590\n",
      "  - \"it\": 0.2633\n",
      "  - \"with\": 0.3918\n",
      "  - \"by\": 0.3256\n",
      "  - \"\"\": 0.2298\n",
      "  - \"at\": 0.2987\n",
      "  - \"he\": 0.5640\n",
      "  - \"from\": 0.3789\n",
      "  - \"be\": 0.2026\n",
      "  - \"this\": 0.2629\n",
      "  - \"i\": 0.2943\n",
      "  - \"an\": 0.3418\n",
      "  - \"his\": 0.4254\n",
      "  - \"are\": 0.1894\n",
      "  - \"not\": 0.1929\n",
      "  - \"has\": 0.2781\n",
      "  - \"have\": 0.3387\n",
      "  - \"but\": 0.4598\n",
      "  - \"or\": 0.2210\n",
      "  - \"utc\": 0.2416\n",
      "  - \"which\": 0.4463\n",
      "  - \"were\": 0.2689\n",
      "  - \"–\": 0.3039\n",
      "  - \"said\": 0.3370\n",
      "  - \"they\": 0.4517\n",
      "  - \"also\": 0.3746\n",
      "  - \"one\": 0.3309\n",
      "  - \"who\": 1.0000\n",
      "  - \"had\": 0.3363\n",
      "  - \"talk\": 0.2235\n",
      "  - \"new\": 0.2579\n",
      "  - \"their\": 0.3646\n",
      "\n",
      "Similarities with \"had\":\n",
      "  - \",\": 0.3879\n",
      "  - \"the\": 0.4339\n",
      "  - \".\": 0.4085\n",
      "  - \"of\": 0.3860\n",
      "  - \"-\": 0.2965\n",
      "  - \"and\": 0.4304\n",
      "  - \"in\": 0.4096\n",
      "  - \"to\": 0.3821\n",
      "  - \"'\": 0.2804\n",
      "  - \"a\": 0.3762\n",
      "  - \")\": 0.2286\n",
      "  - \"(\": 0.2659\n",
      "  - \"is\": 0.2889\n",
      "  - \"s\": 0.3927\n",
      "  - \"for\": 0.3609\n",
      "  - \"was\": 0.6005\n",
      "  - \"on\": 0.3610\n",
      "  - \"that\": 0.3999\n",
      "  - \"as\": 0.3713\n",
      "  - \"it\": 0.3036\n",
      "  - \"with\": 0.4619\n",
      "  - \"by\": 0.3470\n",
      "  - \"\"\": 0.2418\n",
      "  - \"at\": 0.3547\n",
      "  - \"he\": 0.3924\n",
      "  - \"from\": 0.3669\n",
      "  - \"be\": 0.2515\n",
      "  - \"this\": 0.3135\n",
      "  - \"i\": 0.2549\n",
      "  - \"an\": 0.3085\n",
      "  - \"his\": 0.4113\n",
      "  - \"are\": 0.3417\n",
      "  - \"not\": 0.2611\n",
      "  - \"has\": 0.6138\n",
      "  - \"have\": 0.6569\n",
      "  - \"but\": 0.4170\n",
      "  - \"or\": 0.2500\n",
      "  - \"utc\": 0.1234\n",
      "  - \"which\": 0.3525\n",
      "  - \"were\": 0.6114\n",
      "  - \"–\": 0.1982\n",
      "  - \"said\": 0.4499\n",
      "  - \"they\": 0.4152\n",
      "  - \"also\": 0.3552\n",
      "  - \"one\": 0.3896\n",
      "  - \"who\": 0.3363\n",
      "  - \"had\": 1.0000\n",
      "  - \"talk\": 0.1770\n",
      "  - \"new\": 0.2491\n",
      "  - \"their\": 0.4070\n",
      "\n",
      "Similarities with \"talk\":\n",
      "  - \",\": 0.3371\n",
      "  - \"the\": 0.2662\n",
      "  - \".\": 0.4099\n",
      "  - \"of\": 0.2276\n",
      "  - \"-\": 0.3473\n",
      "  - \"and\": 0.2665\n",
      "  - \"in\": 0.2557\n",
      "  - \"to\": 0.2466\n",
      "  - \"'\": 0.3356\n",
      "  - \"a\": 0.2649\n",
      "  - \")\": 0.4670\n",
      "  - \"(\": 0.4832\n",
      "  - \"is\": 0.2649\n",
      "  - \"s\": 0.1701\n",
      "  - \"for\": 0.2700\n",
      "  - \"was\": 0.1835\n",
      "  - \"on\": 0.2147\n",
      "  - \"that\": 0.2984\n",
      "  - \"as\": 0.2969\n",
      "  - \"it\": 0.3040\n",
      "  - \"with\": 0.1993\n",
      "  - \"by\": 0.2320\n",
      "  - \"\"\": 0.1886\n",
      "  - \"at\": 0.2474\n",
      "  - \"he\": 0.1667\n",
      "  - \"from\": 0.1980\n",
      "  - \"be\": 0.3275\n",
      "  - \"this\": 0.4553\n",
      "  - \"i\": 0.4292\n",
      "  - \"an\": 0.2158\n",
      "  - \"his\": 0.1721\n",
      "  - \"are\": 0.2958\n",
      "  - \"not\": 0.3692\n",
      "  - \"has\": 0.2971\n",
      "  - \"have\": 0.3392\n",
      "  - \"but\": 0.3449\n",
      "  - \"or\": 0.3030\n",
      "  - \"utc\": 0.6616\n",
      "  - \"which\": 0.2714\n",
      "  - \"were\": 0.1939\n",
      "  - \"–\": 0.2591\n",
      "  - \"said\": 0.2212\n",
      "  - \"they\": 0.2122\n",
      "  - \"also\": 0.2280\n",
      "  - \"one\": 0.2544\n",
      "  - \"who\": 0.2235\n",
      "  - \"had\": 0.1770\n",
      "  - \"talk\": 1.0000\n",
      "  - \"new\": 0.2023\n",
      "  - \"their\": 0.1905\n",
      "\n",
      "Similarities with \"new\":\n",
      "  - \",\": 0.4277\n",
      "  - \"the\": 0.4359\n",
      "  - \".\": 0.4146\n",
      "  - \"of\": 0.3946\n",
      "  - \"-\": 0.3562\n",
      "  - \"and\": 0.4307\n",
      "  - \"in\": 0.4019\n",
      "  - \"to\": 0.3895\n",
      "  - \"'\": 0.3129\n",
      "  - \"a\": 0.3297\n",
      "  - \")\": 0.2816\n",
      "  - \"(\": 0.3012\n",
      "  - \"is\": 0.2806\n",
      "  - \"s\": 0.3038\n",
      "  - \"for\": 0.3393\n",
      "  - \"was\": 0.3516\n",
      "  - \"on\": 0.3277\n",
      "  - \"that\": 0.3534\n",
      "  - \"as\": 0.2462\n",
      "  - \"it\": 0.3383\n",
      "  - \"with\": 0.3093\n",
      "  - \"by\": 0.3231\n",
      "  - \"\"\": 0.3005\n",
      "  - \"at\": 0.3295\n",
      "  - \"he\": 0.2828\n",
      "  - \"from\": 0.3682\n",
      "  - \"be\": 0.2219\n",
      "  - \"this\": 0.3319\n",
      "  - \"i\": 0.2233\n",
      "  - \"an\": 0.3204\n",
      "  - \"his\": 0.2849\n",
      "  - \"are\": 0.2501\n",
      "  - \"not\": 0.2526\n",
      "  - \"has\": 0.3130\n",
      "  - \"have\": 0.2782\n",
      "  - \"but\": 0.2720\n",
      "  - \"or\": 0.1764\n",
      "  - \"utc\": 0.1825\n",
      "  - \"which\": 0.3693\n",
      "  - \"were\": 0.2619\n",
      "  - \"–\": 0.2456\n",
      "  - \"said\": 0.2427\n",
      "  - \"they\": 0.2983\n",
      "  - \"also\": 0.3177\n",
      "  - \"one\": 0.3428\n",
      "  - \"who\": 0.2579\n",
      "  - \"had\": 0.2491\n",
      "  - \"talk\": 0.2023\n",
      "  - \"new\": 1.0000\n",
      "  - \"their\": 0.3124\n",
      "\n",
      "Similarities with \"their\":\n",
      "  - \",\": 0.3509\n",
      "  - \"the\": 0.5333\n",
      "  - \".\": 0.3642\n",
      "  - \"of\": 0.3765\n",
      "  - \"-\": 0.2973\n",
      "  - \"and\": 0.5092\n",
      "  - \"in\": 0.3456\n",
      "  - \"to\": 0.4696\n",
      "  - \"'\": 0.2767\n",
      "  - \"a\": 0.3504\n",
      "  - \")\": 0.1785\n",
      "  - \"(\": 0.1947\n",
      "  - \"is\": 0.2145\n",
      "  - \"s\": 0.3601\n",
      "  - \"for\": 0.3851\n",
      "  - \"was\": 0.2838\n",
      "  - \"on\": 0.3559\n",
      "  - \"that\": 0.4321\n",
      "  - \"as\": 0.3354\n",
      "  - \"it\": 0.3008\n",
      "  - \"with\": 0.4425\n",
      "  - \"by\": 0.3336\n",
      "  - \"\"\": 0.3309\n",
      "  - \"at\": 0.2814\n",
      "  - \"he\": 0.3040\n",
      "  - \"from\": 0.3191\n",
      "  - \"be\": 0.3468\n",
      "  - \"this\": 0.3742\n",
      "  - \"i\": 0.2193\n",
      "  - \"an\": 0.3010\n",
      "  - \"his\": 0.5999\n",
      "  - \"are\": 0.4480\n",
      "  - \"not\": 0.3176\n",
      "  - \"has\": 0.2724\n",
      "  - \"have\": 0.4415\n",
      "  - \"but\": 0.4042\n",
      "  - \"or\": 0.3271\n",
      "  - \"utc\": 0.1598\n",
      "  - \"which\": 0.4440\n",
      "  - \"were\": 0.4906\n",
      "  - \"–\": 0.2003\n",
      "  - \"said\": 0.2723\n",
      "  - \"they\": 0.6740\n",
      "  - \"also\": 0.4146\n",
      "  - \"one\": 0.4028\n",
      "  - \"who\": 0.3646\n",
      "  - \"had\": 0.4070\n",
      "  - \"talk\": 0.1905\n",
      "  - \"new\": 0.3124\n",
      "  - \"their\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_new_retrofitted_gensim_dict = convert_matrix_to_dict(subset_retrofitted_wordVecMat, subset_retrofitted_wordList)\n",
    "new_retrofitted_gensim_similarity_matrix = generate_cosine_similarity_matrix(subset_new_retrofitted_gensim_dict)\n",
    "print_vec_similarities(wordList_gensim, new_retrofitted_gensim_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between love and sex: 0.3213976283166737\n",
      "Similarity between tiger and cat: 0.4484269447717373\n",
      "Similarity between tiger and tiger: 1.0\n",
      "Similarity between book and paper: 0.5720403096134581\n",
      "Similarity between computer and keyboard: 0.4902811841797181\n",
      "Similarity between computer and internet: 0.49881600720718505\n",
      "Similarity between plane and car: 0.4094837589536879\n",
      "Similarity between train and car: 0.5024532628135742\n",
      "Similarity between telephone and communication: 0.36768667290283835\n",
      "Similarity between television and radio: 0.6536351915117727\n",
      "Similarity between media and radio: 0.3209847572779895\n",
      "Similarity between drug and abuse: 0.42865908078081993\n",
      "Similarity between bread and butter: 0.8071053286387004\n",
      "Similarity between cucumber and potato: 0.6915610413305441\n",
      "Similarity between doctor and nurse: 0.5676552249319599\n",
      "Similarity between professor and doctor: 0.5076438957646101\n",
      "Similarity between student and professor: 0.45578493020098115\n",
      "Similarity between smart and student: 0.2940012954759296\n",
      "Similarity between smart and stupid: 0.39617691896753526\n",
      "Similarity between company and stock: 0.46077310446425557\n",
      "Similarity between stock and market: 0.5385185527611723\n",
      "Similarity between stock and phone: 0.2537954804905717\n",
      "Similarity between stock and jaguar: 0.24541597641521512\n",
      "Similarity between stock and egg: 0.17158691336839396\n",
      "Similarity between fertility and egg: 0.29469937801142465\n",
      "Similarity between stock and live: 0.1875328178693288\n",
      "Similarity between stock and life: 0.16227126965853364\n",
      "Similarity between book and library: 0.38331650882604734\n",
      "Similarity between bank and money: 0.3817619966211285\n",
      "Similarity between wood and forest: 0.4513705729701868\n",
      "Similarity between money and cash: 0.7500799480892854\n",
      "Similarity between professor and cucumber: 0.09663273096654584\n",
      "Similarity between king and cabbage: 0.1682218573112498\n",
      "Similarity between king and queen: 0.6852263900680836\n",
      "Similarity between king and rook: 0.29040233820098826\n",
      "Similarity between bishop and rabbi: 0.44022479884512833\n",
      "Similarity between holy and sex: 0.10285249855240487\n",
      "Similarity between fuck and sex: 0.2583644208592573\n",
      "Similarity between football and soccer: 0.732992880777482\n",
      "Similarity between football and basketball: 0.7350006454613549\n",
      "Similarity between football and tennis: 0.4979653308167488\n",
      "Similarity between tennis and racket: 0.3102321036702446\n",
      "Similarity between law and lawyer: 0.546109843782221\n",
      "Similarity between movie and star: 0.38941927715237745\n",
      "Similarity between movie and popcorn: 0.4541689909060221\n",
      "Similarity between movie and critic: 0.33297393132298003\n",
      "Similarity between movie and theater: 0.46441696370849056\n",
      "Similarity between physics and proton: 0.3705833220077318\n",
      "Similarity between physics and chemistry: 0.8327436735273508\n",
      "Similarity between space and chemistry: 0.23392004698030291\n",
      "Similarity between alcohol and chemistry: 0.2859536043744763\n",
      "Similarity between vodka and gin: 0.5707338038897669\n",
      "Similarity between vodka and brandy: 0.6550828058805547\n",
      "Similarity between drink and car: 0.3548774555088685\n",
      "Similarity between drink and ear: 0.21796934800647\n",
      "Similarity between drink and mouth: 0.2979787069985778\n",
      "Similarity between drink and eat: 0.6519650698252718\n",
      "Similarity between baby and mother: 0.6094044955749424\n",
      "Similarity between drink and mother: 0.2769888776806356\n",
      "Similarity between car and automobile: 0.6533339368005837\n",
      "Similarity between gem and jewel: 0.6713427529333966\n",
      "Similarity between journey and voyage: 0.5104090589807883\n",
      "Similarity between boy and lad: 0.5799155222244894\n",
      "Similarity between coast and shore: 0.6010836392294177\n",
      "Similarity between asylum and madhouse: 0.3119981062009018\n",
      "Similarity between magician and wizard: 0.467031460051122\n",
      "Similarity between midday and noon: 0.7981434151232776\n",
      "Similarity between furnace and stove: 0.6711621765208311\n",
      "Similarity between food and fruit: 0.5270629824510354\n",
      "Similarity between bird and cock: 0.46689846889330416\n",
      "Similarity between bird and crane: 0.49624411866584933\n",
      "Similarity between tool and implement: 0.4377713516384724\n",
      "Similarity between brother and monk: 0.3294016727339705\n",
      "Similarity between crane and implement: 0.13885292653754006\n",
      "Similarity between lad and brother: 0.28885342906104405\n",
      "Similarity between journey and car: 0.24120297213219638\n",
      "Similarity between monk and oracle: 0.2347094685478309\n",
      "Similarity between cemetery and woodland: 0.395080038464906\n",
      "Similarity between food and rooster: 0.24790878465146252\n",
      "Similarity between coast and hill: 0.3147352473168207\n",
      "Similarity between forest and graveyard: 0.24633859114233356\n",
      "Similarity between shore and woodland: 0.38322100534530096\n",
      "Similarity between monk and slave: 0.2541671122536606\n",
      "Similarity between coast and forest: 0.3989787151730762\n",
      "Similarity between lad and wizard: 0.22255492846786296\n",
      "Similarity between chord and smile: 0.24309603147008846\n",
      "Similarity between glass and magician: 0.273117680911635\n",
      "Similarity between noon and string: 0.11740553954822505\n",
      "Similarity between rooster and voyage: 0.08814789651141473\n",
      "Similarity between money and dollar: 0.471340879007201\n",
      "Similarity between money and cash: 0.7500799480892854\n",
      "Similarity between money and currency: 0.457569525785614\n",
      "Similarity between money and wealth: 0.49691614869804546\n",
      "Similarity between money and property: 0.36332694292999324\n",
      "Similarity between money and possession: 0.26704900150074873\n",
      "Similarity between money and bank: 0.3817619966211285\n",
      "Similarity between money and deposit: 0.3864688457388147\n",
      "Similarity between money and withdrawal: 0.1838273498787751\n",
      "Similarity between money and laundering: 0.4877732859775846\n",
      "Similarity between money and operation: 0.12615036874456567\n",
      "Similarity between tiger and jaguar: 0.5153315923593653\n",
      "Similarity between tiger and feline: 0.39987473998456113\n",
      "Similarity between tiger and carnivore: 0.3377899887983067\n",
      "Similarity between tiger and mammal: 0.3139792918670319\n",
      "Similarity between tiger and animal: 0.34446900444212614\n",
      "Similarity between tiger and organism: 0.1727525390581988\n",
      "Similarity between tiger and fauna: 0.22103905147723926\n",
      "Similarity between tiger and zoo: 0.45376140773023\n",
      "Similarity between psychology and psychiatry: 0.7696149098639089\n",
      "Similarity between psychology and anxiety: 0.39844391637524584\n",
      "Similarity between psychology and fear: 0.1964973180767654\n",
      "Similarity between psychology and depression: 0.2608744652717436\n",
      "Similarity between psychology and clinic: 0.43601861730354335\n",
      "Similarity between psychology and doctor: 0.4309964190930099\n",
      "Similarity between psychology and mind: 0.3569885743635376\n",
      "Similarity between psychology and health: 0.4966090103404135\n",
      "Similarity between psychology and science: 0.6665620157509726\n",
      "Similarity between psychology and discipline: 0.3314297807693717\n",
      "Similarity between psychology and cognition: 0.6759983910138431\n",
      "Similarity between planet and star: 0.519761477538004\n",
      "Similarity between planet and constellation: 0.4977904510675082\n",
      "Similarity between planet and moon: 0.574677769918392\n",
      "Similarity between planet and sun: 0.4611190640217323\n",
      "Similarity between planet and galaxy: 0.5653450099902443\n",
      "Similarity between planet and space: 0.4130999340303344\n",
      "Similarity between planet and astronomer: 0.3480027755941616\n",
      "Similarity between precedent and example: 0.3482787912209587\n",
      "Similarity between precedent and information: 0.19287277733703073\n",
      "Similarity between precedent and cognition: 0.0829476856720916\n",
      "Similarity between precedent and law: 0.37162107814369055\n",
      "Similarity between precedent and collection: 0.14303226231401617\n",
      "Similarity between precedent and group: 0.09534828286901065\n",
      "Similarity between precedent and antecedent: 0.2910666566807138\n",
      "Similarity between cup and coffee: 0.20797550452839314\n",
      "Similarity between cup and tableware: 0.16531159656431751\n",
      "Similarity between cup and article: 0.10711890223332217\n",
      "Similarity between cup and artifact: 0.1080216076937357\n",
      "Similarity between cup and object: 0.07251832640887662\n",
      "Similarity between cup and entity: 0.1394455736511904\n",
      "Similarity between cup and drink: 0.20242876395843212\n",
      "Similarity between cup and food: 0.0807667007012764\n",
      "Similarity between cup and substance: 0.05640861645139696\n",
      "Similarity between cup and liquid: 0.15859567621272105\n",
      "Similarity between jaguar and cat: 0.32344008680522707\n",
      "Similarity between jaguar and car: 0.47178156153854967\n",
      "Similarity between energy and secretary: 0.14870377486678638\n",
      "Similarity between secretary and senate: 0.5107258624859309\n",
      "Similarity between energy and laboratory: 0.3690024035447106\n",
      "Similarity between computer and laboratory: 0.4192700792767917\n",
      "Similarity between weapon and secret: 0.3756434854529629\n",
      "Similarity between investigation and effort: 0.31541103128757547\n",
      "Similarity between news and report: 0.42827631995825033\n",
      "Similarity between canyon and landscape: 0.31199456760585376\n",
      "Similarity between image and surface: 0.2006105310106386\n",
      "Similarity between discovery and space: 0.23063979442308355\n",
      "Similarity between water and seepage: 0.5381568167791105\n",
      "Similarity between sign and recess: 0.16953179194067447\n",
      "Similarity between mile and kilometer: 0.7193962460158686\n",
      "Similarity between computer and news: 0.2285664151144775\n",
      "Similarity between territory and surface: 0.17314869458684998\n",
      "Similarity between atmosphere and landscape: 0.3094822829874144\n",
      "Similarity between president and medal: 0.3244123254801186\n",
      "Similarity between war and troops: 0.42153916415040305\n",
      "Similarity between record and number: 0.2982527996274973\n",
      "Similarity between skin and eye: 0.5272105414526764\n",
      "Similarity between theater and history: 0.24835518385837307\n",
      "Similarity between volunteer and motto: 0.31070638950154694\n",
      "Similarity between prejudice and recognition: 0.1569417528015616\n",
      "Similarity between decoration and valor: 0.5058515653001794\n",
      "Similarity between century and year: 0.3005721451838296\n",
      "Similarity between century and nation: 0.21864306199795505\n",
      "Similarity between delay and racism: 0.09040073832228933\n",
      "Similarity between delay and news: 0.15109896237642725\n",
      "Similarity between minister and party: 0.41903029933289204\n",
      "Similarity between peace and plan: 0.27627026579666114\n",
      "Similarity between minority and peace: 0.23639444584679556\n",
      "Similarity between attempt and peace: 0.19563450568731033\n",
      "Similarity between government and crisis: 0.40390346060649457\n",
      "Similarity between deployment and departure: 0.35505174295597863\n",
      "Similarity between deployment and withdrawal: 0.4294599502486629\n",
      "Similarity between energy and crisis: 0.24562669856959823\n",
      "Similarity between announcement and news: 0.4144241780349597\n",
      "Similarity between announcement and effort: 0.29492976985580477\n",
      "Similarity between stroke and hospital: 0.3218452148656307\n",
      "Similarity between disability and death: 0.2270331948422127\n",
      "Similarity between victim and emergency: 0.2543353428930358\n",
      "Similarity between treatment and recovery: 0.4401718058418478\n",
      "Similarity between journal and association: 0.36273327977460246\n",
      "Similarity between doctor and personnel: 0.23438268637054316\n",
      "Similarity between doctor and liability: 0.13848875298442692\n",
      "Similarity between liability and insurance: 0.5834748552408184\n",
      "Similarity between school and center: 0.3671201646462656\n",
      "Similarity between reason and hypertension: 0.144423274776883\n",
      "Similarity between reason and criterion: 0.42197100760154505\n",
      "Similarity between hundred and percent: 0.3963913899953862\n",
      "Similarity between hospital and infrastructure: 0.22509102964735866\n",
      "Similarity between death and row: 0.24864641736479412\n",
      "Similarity between death and inmate: 0.3408138610282122\n",
      "Similarity between lawyer and evidence: 0.1865072638891968\n",
      "Similarity between life and death: 0.3931775431368855\n",
      "Similarity between life and term: 0.21162744982172005\n",
      "Similarity between word and similarity: 0.30616878851257395\n",
      "Similarity between board and recommendation: 0.37079919685341345\n",
      "Similarity between governor and interview: 0.13601457824475652\n",
      "Similarity between peace and atmosphere: 0.22281773646342043\n",
      "Similarity between peace and insurance: 0.18095855845062\n",
      "Similarity between territory and kilometer: 0.2984545710206714\n",
      "Similarity between travel and activity: 0.25162807367655476\n",
      "Similarity between competition and price: 0.2980840078723433\n",
      "Similarity between consumer and confidence: 0.18944417036385178\n",
      "Similarity between consumer and energy: 0.4368284673985543\n",
      "Similarity between problem and airport: 0.10641880311909256\n",
      "Similarity between car and flight: 0.3147276417699101\n",
      "Similarity between credit and card: 0.467679804710897\n",
      "Similarity between credit and information: 0.3236116230803933\n",
      "Similarity between hotel and reservation: 0.3312412119391827\n",
      "Similarity between grocery and money: 0.31541913433912633\n",
      "Similarity between registration and arrangement: 0.22005397134358096\n",
      "Similarity between arrangement and accommodation: 0.37577253442072794\n",
      "Similarity between month and hotel: 0.1843158001493066\n",
      "Similarity between type and kind: 0.5226445651278104\n",
      "Similarity between arrival and hotel: 0.2379218396577449\n",
      "Similarity between bed and closet: 0.514430679723206\n",
      "Similarity between closet and clothes: 0.46717911310929067\n",
      "Similarity between situation and conclusion: 0.36559048912987213\n",
      "Similarity between situation and isolation: 0.32896806680719665\n",
      "Similarity between impartiality and interest: 0.25425832004418547\n",
      "Similarity between direction and combination: 0.2255150923554187\n",
      "Similarity between street and place: 0.2814481774100284\n",
      "Similarity between street and avenue: 0.7944696888211877\n",
      "Similarity between street and block: 0.3272265135369973\n",
      "Similarity between street and children: 0.21689014006223406\n",
      "Similarity between listing and proximity: 0.12003172246279278\n",
      "Similarity between listing and category: 0.4023952990054468\n",
      "Similarity between cell and phone: 0.42862021128596994\n",
      "Similarity between production and hike: 0.0938559905103312\n",
      "Similarity between benchmark and index: 0.575850437852915\n",
      "Similarity between media and trading: 0.17525306262110277\n",
      "Similarity between media and gain: 0.26197772989341506\n",
      "Similarity between dividend and payment: 0.5364874542444955\n",
      "Similarity between dividend and calculation: 0.3507031694077017\n",
      "Similarity between calculation and computation: 0.687704014429982\n",
      "Similarity between currency and market: 0.45398172042498663\n",
      "Similarity between oil and stock: 0.32418891516998\n",
      "Similarity between announcement and production: 0.24923300212796567\n",
      "Similarity between announcement and warning: 0.2613362807351249\n",
      "Similarity between profit and warning: 0.05171257099383002\n",
      "Similarity between profit and loss: 0.19591486661921312\n",
      "Similarity between dollar and yen: 0.4977735716983742\n",
      "Similarity between dollar and buck: 0.3611598893952871\n",
      "Similarity between dollar and profit: 0.33508607815462904\n",
      "Similarity between dollar and loss: 0.12362969070393039\n",
      "Similarity between computer and software: 0.6468620907416638\n",
      "Similarity between network and hardware: 0.3177904514376401\n",
      "Similarity between phone and equipment: 0.30961041225437763\n",
      "Similarity between equipment and maker: 0.24174991446424224\n",
      "Similarity between luxury and car: 0.4174046574650426\n",
      "Similarity between five and month: 0.43915704514951853\n",
      "Similarity between report and gain: 0.05346097750505492\n",
      "Similarity between investor and earning: 0.17898559190018085\n",
      "Similarity between liquid and water: 0.5512252578990561\n",
      "Similarity between baseball and season: 0.40714763631610656\n",
      "Similarity between game and victory: 0.32052317597447866\n",
      "Similarity between game and team: 0.3602181279448463\n",
      "Similarity between marathon and sprint: 0.5465444238280975\n",
      "Similarity between game and series: 0.42139494523759286\n",
      "Similarity between game and defeat: 0.35115345990732666\n",
      "Similarity between seven and series: 0.41648462403604086\n",
      "Similarity between seafood and sea: 0.3306571798167045\n",
      "Similarity between seafood and food: 0.6978338489756991\n",
      "Similarity between seafood and lobster: 0.7010583437527236\n",
      "Similarity between lobster and food: 0.4759274122251642\n",
      "Similarity between lobster and wine: 0.41791529878156675\n",
      "Similarity between food and preparation: 0.4106230287433116\n",
      "Similarity between video and archive: 0.1356101712212238\n",
      "Similarity between start and year: 0.3445289658727612\n",
      "Similarity between start and match: 0.31434689209982597\n",
      "Similarity between game and round: 0.28621710641695486\n",
      "Similarity between boxing and round: 0.19154012630495382\n",
      "Similarity between championship and tournament: 0.7444553855765738\n",
      "Similarity between fighting and defeating: 0.29337143790539577\n",
      "Similarity between line and insurance: 0.14398239143371\n",
      "Similarity between day and summer: 0.37266241017908114\n",
      "Similarity between summer and drought: 0.2621906266869796\n",
      "Similarity between summer and nature: 0.1853843496297093\n",
      "Similarity between day and dawn: 0.37852736143093796\n",
      "Similarity between nature and environment: 0.5263190161943299\n",
      "Similarity between environment and ecology: 0.5759535900602069\n",
      "Similarity between nature and man: 0.28627321889494334\n",
      "Similarity between man and woman: 0.6658470105380624\n",
      "Similarity between man and governor: 0.22529994330088682\n",
      "Similarity between murder and manslaughter: 0.7050093014917536\n",
      "Similarity between soap and opera: 0.5597841918886536\n",
      "Similarity between opera and performance: 0.34825366227979776\n",
      "Similarity between life and lesson: 0.2927790955891807\n",
      "Similarity between focus and life: 0.37360468819818893\n",
      "Similarity between production and crew: 0.24360373090856294\n",
      "Similarity between television and film: 0.568723193092448\n",
      "Similarity between lover and quarrel: 0.44007927187742446\n",
      "Similarity between viewer and serial: 0.20212685614156517\n",
      "Similarity between possibility and girl: 0.15245454509034184\n",
      "Similarity between population and development: 0.22605376529930335\n",
      "Similarity between morality and importance: 0.31423138018774754\n",
      "Similarity between morality and marriage: 0.38633256898497703\n",
      "Similarity between gender and equality: 0.5862350883267482\n",
      "Similarity between change and attitude: 0.311045702462625\n",
      "Similarity between family and planning: 0.150242238240384\n",
      "Similarity between opera and industry: 0.1999320091974873\n",
      "Similarity between sugar and approach: 0.09573850955033071\n",
      "Similarity between practice and institution: 0.34664063586638316\n",
      "Similarity between ministry and culture: 0.35637908192730194\n",
      "Similarity between problem and challenge: 0.3147201635668211\n",
      "Similarity between size and prominence: 0.19908037983633792\n",
      "Similarity between country and citizen: 0.3858730649025389\n",
      "Similarity between planet and people: 0.16107155519293073\n",
      "Similarity between development and issue: 0.1699497042447709\n",
      "Similarity between experience and music: 0.27052414819144976\n",
      "Similarity between music and project: 0.3108060141539901\n",
      "Similarity between glass and metal: 0.47637366241410384\n",
      "Similarity between aluminum and metal: 0.5818277272279189\n",
      "Similarity between chance and credibility: 0.3169952828269589\n",
      "Similarity between exhibit and memorabilia: 0.38623598677526755\n",
      "Similarity between concert and virtuoso: 0.48561499552017373\n",
      "Similarity between rock and jazz: 0.5421643742927887\n",
      "Similarity between museum and theater: 0.4679903266892968\n",
      "Similarity between observation and architecture: 0.24924606890034434\n",
      "Similarity between space and world: 0.16201209623194165\n",
      "Similarity between preservation and world: 0.1278478491868286\n",
      "Similarity between admission and ticket: 0.41643268596243277\n",
      "Similarity between shower and thunderstorm: 0.48556203317784374\n",
      "Similarity between shower and flood: 0.24936297450262368\n",
      "Similarity between weather and forecast: 0.642593401789184\n",
      "Similarity between disaster and area: 0.25571124304709214\n",
      "Similarity between governor and office: 0.42209747699357636\n",
      "Similarity between architecture and century: 0.30194647996254603\n"
     ]
    }
   ],
   "source": [
    "# Only considering the words in the lexicon similarity file\n",
    "# Retrive words from the lexical similarity file\n",
    "\n",
    "def print_lexical_similarities(wordVecs, lines):\n",
    "    # Create a list to store the words\n",
    "    word_list = []\n",
    "\n",
    "    # Iterate over the lines and extract the words\n",
    "    for line in lines:\n",
    "        words = line.strip().split('\\t')\n",
    "        word1 = words[0]\n",
    "        word2 = words[1]\n",
    "        word_list.append((word1, word2))  # Store the words as a tuple\n",
    "\n",
    "    # Determine the subset of words present in the wordVecs file while preserving the order\n",
    "    subset = [word for word in word_list if word[0] in wordVecs and word[1] in wordVecs]\n",
    "\n",
    "    # Create a dictionary to map words to indices\n",
    "    w2i = {word: index for index, word in enumerate(wordVecs)}\n",
    "\n",
    "    # Create an empty list to store the similarities\n",
    "    similarities = []\n",
    "\n",
    "    # Iterate over each tuple in the subset\n",
    "    for word1, word2 in subset:\n",
    "        if word1 in wordVecs and word2 in wordVecs:\n",
    "            # Retrieve the embeddings for the words\n",
    "            embedding1 = wordVecs[word1]\n",
    "            embedding2 = wordVecs[word2]\n",
    "\n",
    "            # Calculate the similarity between the embeddings\n",
    "            similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "            # Append the similarity value to the list of similarities\n",
    "            similarities.append(similarity_score)\n",
    "\n",
    "    # Print the similarities\n",
    "    for i, similarity_score in enumerate(similarities):\n",
    "        print(f\"Similarity between {subset[i][0]} and {subset[i][1]}: {similarity_score[0][0]}\")\n",
    "\n",
    "\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print_lexical_similarities(wordVecs_gensim, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between love and sex: 0.3213976283166737\n",
      "Similarity between tiger and cat: 0.4484269447717373\n",
      "Similarity between tiger and tiger: 1.0\n",
      "Similarity between book and paper: 0.5720403096134581\n",
      "Similarity between computer and keyboard: 0.4902811841797181\n",
      "Similarity between computer and internet: 0.49881600720718505\n",
      "Similarity between plane and car: 0.4094837589536879\n",
      "Similarity between train and car: 0.5024532628135742\n",
      "Similarity between telephone and communication: 0.36768667290283835\n",
      "Similarity between television and radio: 0.6536351915117727\n",
      "Similarity between media and radio: 0.3209847572779895\n",
      "Similarity between drug and abuse: 0.42865908078081993\n",
      "Similarity between bread and butter: 0.8071053286387004\n",
      "Similarity between cucumber and potato: 0.6915610413305441\n",
      "Similarity between doctor and nurse: 0.5676552249319599\n",
      "Similarity between professor and doctor: 0.5076438957646101\n",
      "Similarity between student and professor: 0.45578493020098115\n",
      "Similarity between smart and student: 0.2940012954759296\n",
      "Similarity between smart and stupid: 0.39617691896753526\n",
      "Similarity between company and stock: 0.46077310446425557\n",
      "Similarity between stock and market: 0.5385185527611723\n",
      "Similarity between stock and phone: 0.2537954804905717\n",
      "Similarity between stock and jaguar: 0.24541597641521512\n",
      "Similarity between stock and egg: 0.17158691336839396\n",
      "Similarity between fertility and egg: 0.29469937801142465\n",
      "Similarity between stock and live: 0.1875328178693288\n",
      "Similarity between stock and life: 0.16227126965853364\n",
      "Similarity between book and library: 0.38331650882604734\n",
      "Similarity between bank and money: 0.3817619966211285\n",
      "Similarity between wood and forest: 0.4513705729701868\n",
      "Similarity between money and cash: 0.7500799480892854\n",
      "Similarity between professor and cucumber: 0.09663273096654584\n",
      "Similarity between king and cabbage: 0.1682218573112498\n",
      "Similarity between king and queen: 0.6852263900680836\n",
      "Similarity between king and rook: 0.29040233820098826\n",
      "Similarity between bishop and rabbi: 0.44022479884512833\n",
      "Similarity between holy and sex: 0.10285249855240487\n",
      "Similarity between fuck and sex: 0.2583644208592573\n",
      "Similarity between football and soccer: 0.732992880777482\n",
      "Similarity between football and basketball: 0.7350006454613549\n",
      "Similarity between football and tennis: 0.4979653308167488\n",
      "Similarity between tennis and racket: 0.3102321036702446\n",
      "Similarity between law and lawyer: 0.546109843782221\n",
      "Similarity between movie and star: 0.38941927715237745\n",
      "Similarity between movie and popcorn: 0.4541689909060221\n",
      "Similarity between movie and critic: 0.33297393132298003\n",
      "Similarity between movie and theater: 0.46441696370849056\n",
      "Similarity between physics and proton: 0.3705833220077318\n",
      "Similarity between physics and chemistry: 0.8327436735273508\n",
      "Similarity between space and chemistry: 0.23392004698030291\n",
      "Similarity between alcohol and chemistry: 0.2859536043744763\n",
      "Similarity between vodka and gin: 0.5707338038897669\n",
      "Similarity between vodka and brandy: 0.6550828058805547\n",
      "Similarity between drink and car: 0.3548774555088685\n",
      "Similarity between drink and ear: 0.21796934800647\n",
      "Similarity between drink and mouth: 0.2979787069985778\n",
      "Similarity between drink and eat: 0.6519650698252718\n",
      "Similarity between baby and mother: 0.6094044955749424\n",
      "Similarity between drink and mother: 0.2769888776806356\n",
      "Similarity between car and automobile: 0.6533339368005837\n",
      "Similarity between gem and jewel: 0.6713427529333966\n",
      "Similarity between journey and voyage: 0.5104090589807883\n",
      "Similarity between boy and lad: 0.5799155222244894\n",
      "Similarity between coast and shore: 0.6010836392294177\n",
      "Similarity between asylum and madhouse: 0.3119981062009018\n",
      "Similarity between magician and wizard: 0.467031460051122\n",
      "Similarity between midday and noon: 0.7981434151232776\n",
      "Similarity between furnace and stove: 0.6711621765208311\n",
      "Similarity between food and fruit: 0.5270629824510354\n",
      "Similarity between bird and cock: 0.46689846889330416\n",
      "Similarity between bird and crane: 0.49624411866584933\n",
      "Similarity between tool and implement: 0.4377713516384724\n",
      "Similarity between brother and monk: 0.3294016727339705\n",
      "Similarity between crane and implement: 0.13885292653754006\n",
      "Similarity between lad and brother: 0.28885342906104405\n",
      "Similarity between journey and car: 0.24120297213219638\n",
      "Similarity between monk and oracle: 0.2347094685478309\n",
      "Similarity between cemetery and woodland: 0.395080038464906\n",
      "Similarity between food and rooster: 0.24790878465146252\n",
      "Similarity between coast and hill: 0.3147352473168207\n",
      "Similarity between forest and graveyard: 0.24633859114233356\n",
      "Similarity between shore and woodland: 0.38322100534530096\n",
      "Similarity between monk and slave: 0.2541671122536606\n",
      "Similarity between coast and forest: 0.3989787151730762\n",
      "Similarity between lad and wizard: 0.22255492846786296\n",
      "Similarity between chord and smile: 0.24309603147008846\n",
      "Similarity between glass and magician: 0.273117680911635\n",
      "Similarity between noon and string: 0.11740553954822505\n",
      "Similarity between rooster and voyage: 0.08814789651141473\n",
      "Similarity between money and dollar: 0.471340879007201\n",
      "Similarity between money and cash: 0.7500799480892854\n",
      "Similarity between money and currency: 0.457569525785614\n",
      "Similarity between money and wealth: 0.49691614869804546\n",
      "Similarity between money and property: 0.36332694292999324\n",
      "Similarity between money and possession: 0.26704900150074873\n",
      "Similarity between money and bank: 0.3817619966211285\n",
      "Similarity between money and deposit: 0.3864688457388147\n",
      "Similarity between money and withdrawal: 0.1838273498787751\n",
      "Similarity between money and laundering: 0.4877732859775846\n",
      "Similarity between money and operation: 0.12615036874456567\n",
      "Similarity between tiger and jaguar: 0.5153315923593653\n",
      "Similarity between tiger and feline: 0.39987473998456113\n",
      "Similarity between tiger and carnivore: 0.3377899887983067\n",
      "Similarity between tiger and mammal: 0.3139792918670319\n",
      "Similarity between tiger and animal: 0.34446900444212614\n",
      "Similarity between tiger and organism: 0.1727525390581988\n",
      "Similarity between tiger and fauna: 0.22103905147723926\n",
      "Similarity between tiger and zoo: 0.45376140773023\n",
      "Similarity between psychology and psychiatry: 0.7696149098639089\n",
      "Similarity between psychology and anxiety: 0.39844391637524584\n",
      "Similarity between psychology and fear: 0.1964973180767654\n",
      "Similarity between psychology and depression: 0.2608744652717436\n",
      "Similarity between psychology and clinic: 0.43601861730354335\n",
      "Similarity between psychology and doctor: 0.4309964190930099\n",
      "Similarity between psychology and mind: 0.3569885743635376\n",
      "Similarity between psychology and health: 0.4966090103404135\n",
      "Similarity between psychology and science: 0.6665620157509726\n",
      "Similarity between psychology and discipline: 0.3314297807693717\n",
      "Similarity between psychology and cognition: 0.6759983910138431\n",
      "Similarity between planet and star: 0.519761477538004\n",
      "Similarity between planet and constellation: 0.4977904510675082\n",
      "Similarity between planet and moon: 0.574677769918392\n",
      "Similarity between planet and sun: 0.4611190640217323\n",
      "Similarity between planet and galaxy: 0.5653450099902443\n",
      "Similarity between planet and space: 0.4130999340303344\n",
      "Similarity between planet and astronomer: 0.3480027755941616\n",
      "Similarity between precedent and example: 0.3482787912209587\n",
      "Similarity between precedent and information: 0.19287277733703073\n",
      "Similarity between precedent and cognition: 0.0829476856720916\n",
      "Similarity between precedent and law: 0.37162107814369055\n",
      "Similarity between precedent and collection: 0.14303226231401617\n",
      "Similarity between precedent and group: 0.09534828286901065\n",
      "Similarity between precedent and antecedent: 0.2910666566807138\n",
      "Similarity between cup and coffee: 0.20797550452839314\n",
      "Similarity between cup and tableware: 0.16531159656431751\n",
      "Similarity between cup and article: 0.10711890223332217\n",
      "Similarity between cup and artifact: 0.1080216076937357\n",
      "Similarity between cup and object: 0.07251832640887662\n",
      "Similarity between cup and entity: 0.1394455736511904\n",
      "Similarity between cup and drink: 0.20242876395843212\n",
      "Similarity between cup and food: 0.0807667007012764\n",
      "Similarity between cup and substance: 0.05640861645139696\n",
      "Similarity between cup and liquid: 0.15859567621272105\n",
      "Similarity between jaguar and cat: 0.32344008680522707\n",
      "Similarity between jaguar and car: 0.47178156153854967\n",
      "Similarity between energy and secretary: 0.14870377486678638\n",
      "Similarity between secretary and senate: 0.5107258624859309\n",
      "Similarity between energy and laboratory: 0.3690024035447106\n",
      "Similarity between computer and laboratory: 0.4192700792767917\n",
      "Similarity between weapon and secret: 0.3756434854529629\n",
      "Similarity between investigation and effort: 0.31541103128757547\n",
      "Similarity between news and report: 0.42827631995825033\n",
      "Similarity between canyon and landscape: 0.31199456760585376\n",
      "Similarity between image and surface: 0.2006105310106386\n",
      "Similarity between discovery and space: 0.23063979442308355\n",
      "Similarity between water and seepage: 0.5381568167791105\n",
      "Similarity between sign and recess: 0.16953179194067447\n",
      "Similarity between mile and kilometer: 0.7193962460158686\n",
      "Similarity between computer and news: 0.2285664151144775\n",
      "Similarity between territory and surface: 0.17314869458684998\n",
      "Similarity between atmosphere and landscape: 0.3094822829874144\n",
      "Similarity between president and medal: 0.3244123254801186\n",
      "Similarity between war and troops: 0.42153916415040305\n",
      "Similarity between record and number: 0.2982527996274973\n",
      "Similarity between skin and eye: 0.5272105414526764\n",
      "Similarity between theater and history: 0.24835518385837307\n",
      "Similarity between volunteer and motto: 0.31070638950154694\n",
      "Similarity between prejudice and recognition: 0.1569417528015616\n",
      "Similarity between decoration and valor: 0.5058515653001794\n",
      "Similarity between century and year: 0.3005721451838296\n",
      "Similarity between century and nation: 0.21864306199795505\n",
      "Similarity between delay and racism: 0.09040073832228933\n",
      "Similarity between delay and news: 0.15109896237642725\n",
      "Similarity between minister and party: 0.41903029933289204\n",
      "Similarity between peace and plan: 0.27627026579666114\n",
      "Similarity between minority and peace: 0.23639444584679556\n",
      "Similarity between attempt and peace: 0.19563450568731033\n",
      "Similarity between government and crisis: 0.40390346060649457\n",
      "Similarity between deployment and departure: 0.35505174295597863\n",
      "Similarity between deployment and withdrawal: 0.4294599502486629\n",
      "Similarity between energy and crisis: 0.24562669856959823\n",
      "Similarity between announcement and news: 0.4144241780349597\n",
      "Similarity between announcement and effort: 0.29492976985580477\n",
      "Similarity between stroke and hospital: 0.3218452148656307\n",
      "Similarity between disability and death: 0.2270331948422127\n",
      "Similarity between victim and emergency: 0.2543353428930358\n",
      "Similarity between treatment and recovery: 0.4401718058418478\n",
      "Similarity between journal and association: 0.36273327977460246\n",
      "Similarity between doctor and personnel: 0.23438268637054316\n",
      "Similarity between doctor and liability: 0.13848875298442692\n",
      "Similarity between liability and insurance: 0.5834748552408184\n",
      "Similarity between school and center: 0.3671201646462656\n",
      "Similarity between reason and hypertension: 0.144423274776883\n",
      "Similarity between reason and criterion: 0.42197100760154505\n",
      "Similarity between hundred and percent: 0.3963913899953862\n",
      "Similarity between hospital and infrastructure: 0.22509102964735866\n",
      "Similarity between death and row: 0.24864641736479412\n",
      "Similarity between death and inmate: 0.3408138610282122\n",
      "Similarity between lawyer and evidence: 0.1865072638891968\n",
      "Similarity between life and death: 0.3931775431368855\n",
      "Similarity between life and term: 0.21162744982172005\n",
      "Similarity between word and similarity: 0.30616878851257395\n",
      "Similarity between board and recommendation: 0.37079919685341345\n",
      "Similarity between governor and interview: 0.13601457824475652\n",
      "Similarity between peace and atmosphere: 0.22281773646342043\n",
      "Similarity between peace and insurance: 0.18095855845062\n",
      "Similarity between territory and kilometer: 0.2984545710206714\n",
      "Similarity between travel and activity: 0.25162807367655476\n",
      "Similarity between competition and price: 0.2980840078723433\n",
      "Similarity between consumer and confidence: 0.18944417036385178\n",
      "Similarity between consumer and energy: 0.4368284673985543\n",
      "Similarity between problem and airport: 0.10641880311909256\n",
      "Similarity between car and flight: 0.3147276417699101\n",
      "Similarity between credit and card: 0.467679804710897\n",
      "Similarity between credit and information: 0.3236116230803933\n",
      "Similarity between hotel and reservation: 0.3312412119391827\n",
      "Similarity between grocery and money: 0.31541913433912633\n",
      "Similarity between registration and arrangement: 0.22005397134358096\n",
      "Similarity between arrangement and accommodation: 0.37577253442072794\n",
      "Similarity between month and hotel: 0.1843158001493066\n",
      "Similarity between type and kind: 0.5226445651278104\n",
      "Similarity between arrival and hotel: 0.2379218396577449\n",
      "Similarity between bed and closet: 0.514430679723206\n",
      "Similarity between closet and clothes: 0.46717911310929067\n",
      "Similarity between situation and conclusion: 0.36559048912987213\n",
      "Similarity between situation and isolation: 0.32896806680719665\n",
      "Similarity between impartiality and interest: 0.25425832004418547\n",
      "Similarity between direction and combination: 0.2255150923554187\n",
      "Similarity between street and place: 0.2814481774100284\n",
      "Similarity between street and avenue: 0.7944696888211877\n",
      "Similarity between street and block: 0.3272265135369973\n",
      "Similarity between street and children: 0.21689014006223406\n",
      "Similarity between listing and proximity: 0.12003172246279278\n",
      "Similarity between listing and category: 0.4023952990054468\n",
      "Similarity between cell and phone: 0.42862021128596994\n",
      "Similarity between production and hike: 0.0938559905103312\n",
      "Similarity between benchmark and index: 0.575850437852915\n",
      "Similarity between media and trading: 0.17525306262110277\n",
      "Similarity between media and gain: 0.26197772989341506\n",
      "Similarity between dividend and payment: 0.5364874542444955\n",
      "Similarity between dividend and calculation: 0.3507031694077017\n",
      "Similarity between calculation and computation: 0.687704014429982\n",
      "Similarity between currency and market: 0.45398172042498663\n",
      "Similarity between oil and stock: 0.32418891516998\n",
      "Similarity between announcement and production: 0.24923300212796567\n",
      "Similarity between announcement and warning: 0.2613362807351249\n",
      "Similarity between profit and warning: 0.05171257099383002\n",
      "Similarity between profit and loss: 0.19591486661921312\n",
      "Similarity between dollar and yen: 0.4977735716983742\n",
      "Similarity between dollar and buck: 0.3611598893952871\n",
      "Similarity between dollar and profit: 0.33508607815462904\n",
      "Similarity between dollar and loss: 0.12362969070393039\n",
      "Similarity between computer and software: 0.6468620907416638\n",
      "Similarity between network and hardware: 0.3177904514376401\n",
      "Similarity between phone and equipment: 0.30961041225437763\n",
      "Similarity between equipment and maker: 0.24174991446424224\n",
      "Similarity between luxury and car: 0.4174046574650426\n",
      "Similarity between five and month: 0.43915704514951853\n",
      "Similarity between report and gain: 0.05346097750505492\n",
      "Similarity between investor and earning: 0.17898559190018085\n",
      "Similarity between liquid and water: 0.5512252578990561\n",
      "Similarity between baseball and season: 0.40714763631610656\n",
      "Similarity between game and victory: 0.32052317597447866\n",
      "Similarity between game and team: 0.3602181279448463\n",
      "Similarity between marathon and sprint: 0.5465444238280975\n",
      "Similarity between game and series: 0.42139494523759286\n",
      "Similarity between game and defeat: 0.35115345990732666\n",
      "Similarity between seven and series: 0.41648462403604086\n",
      "Similarity between seafood and sea: 0.3306571798167045\n",
      "Similarity between seafood and food: 0.6978338489756991\n",
      "Similarity between seafood and lobster: 0.7010583437527236\n",
      "Similarity between lobster and food: 0.4759274122251642\n",
      "Similarity between lobster and wine: 0.41791529878156675\n",
      "Similarity between food and preparation: 0.4106230287433116\n",
      "Similarity between video and archive: 0.1356101712212238\n",
      "Similarity between start and year: 0.3445289658727612\n",
      "Similarity between start and match: 0.31434689209982597\n",
      "Similarity between game and round: 0.28621710641695486\n",
      "Similarity between boxing and round: 0.19154012630495382\n",
      "Similarity between championship and tournament: 0.7444553855765738\n",
      "Similarity between fighting and defeating: 0.29337143790539577\n",
      "Similarity between line and insurance: 0.14398239143371\n",
      "Similarity between day and summer: 0.37266241017908114\n",
      "Similarity between summer and drought: 0.2621906266869796\n",
      "Similarity between summer and nature: 0.1853843496297093\n",
      "Similarity between day and dawn: 0.37852736143093796\n",
      "Similarity between nature and environment: 0.5263190161943299\n",
      "Similarity between environment and ecology: 0.5759535900602069\n",
      "Similarity between nature and man: 0.28627321889494334\n",
      "Similarity between man and woman: 0.6658470105380624\n",
      "Similarity between man and governor: 0.22529994330088682\n",
      "Similarity between murder and manslaughter: 0.7050093014917536\n",
      "Similarity between soap and opera: 0.5597841918886536\n",
      "Similarity between opera and performance: 0.34825366227979776\n",
      "Similarity between life and lesson: 0.2927790955891807\n",
      "Similarity between focus and life: 0.37360468819818893\n",
      "Similarity between production and crew: 0.24360373090856294\n",
      "Similarity between television and film: 0.568723193092448\n",
      "Similarity between lover and quarrel: 0.44007927187742446\n",
      "Similarity between viewer and serial: 0.20212685614156517\n",
      "Similarity between possibility and girl: 0.15245454509034184\n",
      "Similarity between population and development: 0.22605376529930335\n",
      "Similarity between morality and importance: 0.31423138018774754\n",
      "Similarity between morality and marriage: 0.38633256898497703\n",
      "Similarity between gender and equality: 0.5862350883267482\n",
      "Similarity between change and attitude: 0.311045702462625\n",
      "Similarity between family and planning: 0.150242238240384\n",
      "Similarity between opera and industry: 0.1999320091974873\n",
      "Similarity between sugar and approach: 0.09573850955033071\n",
      "Similarity between practice and institution: 0.34664063586638316\n",
      "Similarity between ministry and culture: 0.35637908192730194\n",
      "Similarity between problem and challenge: 0.3147201635668211\n",
      "Similarity between size and prominence: 0.19908037983633792\n",
      "Similarity between country and citizen: 0.3858730649025389\n",
      "Similarity between planet and people: 0.16107155519293073\n",
      "Similarity between development and issue: 0.1699497042447709\n",
      "Similarity between experience and music: 0.27052414819144976\n",
      "Similarity between music and project: 0.3108060141539901\n",
      "Similarity between glass and metal: 0.47637366241410384\n",
      "Similarity between aluminum and metal: 0.5818277272279189\n",
      "Similarity between chance and credibility: 0.3169952828269589\n",
      "Similarity between exhibit and memorabilia: 0.38623598677526755\n",
      "Similarity between concert and virtuoso: 0.48561499552017373\n",
      "Similarity between rock and jazz: 0.5421643742927887\n",
      "Similarity between museum and theater: 0.4679903266892968\n",
      "Similarity between observation and architecture: 0.24924606890034434\n",
      "Similarity between space and world: 0.16201209623194165\n",
      "Similarity between preservation and world: 0.1278478491868286\n",
      "Similarity between admission and ticket: 0.41643268596243277\n",
      "Similarity between shower and thunderstorm: 0.48556203317784374\n",
      "Similarity between shower and flood: 0.24936297450262368\n",
      "Similarity between weather and forecast: 0.642593401789184\n",
      "Similarity between disaster and area: 0.25571124304709214\n",
      "Similarity between governor and office: 0.42209747699357636\n",
      "Similarity between architecture and century: 0.30194647996254603\n"
     ]
    }
   ],
   "source": [
    "def get_lexical_similarities(wordVecs, lines, subset):\n",
    "    similarities = []\n",
    "\n",
    "    for word1, word2 in subset:\n",
    "        if word1 in wordVecs and word2 in wordVecs:\n",
    "            embedding1 = wordVecs[word1]\n",
    "            embedding2 = wordVecs[word2]\n",
    "\n",
    "            similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "            similarities.append((word1, word2, similarity_score[0][0]))\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "word_list = []\n",
    "for line in lines:\n",
    "    words = line.strip().split('\\t')\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    word_list.append((word1, word2))\n",
    "\n",
    "subset = [(word1, word2) for word1, word2 in word_list if word1 in wordVecs_gensim and word2 in wordVecs_gensim]\n",
    "\n",
    "similarities = get_lexical_similarities(wordVecs_gensim, lines, subset)\n",
    "\n",
    "for word1, word2, similarity_score in similarities:\n",
    "    print(f\"Similarity between {word1} and {word2}: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m nb_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m16\u001b[39m):\n\u001b[0;32m      8\u001b[0m     retrofitted_EN_vec \u001b[39m=\u001b[39m retrofitting_wordVecs_test(wordVecMat_gensim, neighbors_matrix_gensim, alpha, beta, nb_iter)\n\u001b[1;32m----> 9\u001b[0m     embed_update_EN \u001b[39m=\u001b[39m measure_embedding_updates(wordVecMat_gensim, retrofitted_EN_vec)\n\u001b[0;32m     10\u001b[0m     \u001b[39m# print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m embed_update_EN \u001b[39m>\u001b[39m best_embed_update_EN:\n",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m, in \u001b[0;36mmeasure_embedding_updates\u001b[1;34m(original_matrix, retrofitted_matrix)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeasure_embedding_updates\u001b[39m(original_matrix, retrofitted_matrix):\n\u001b[1;32m----> 2\u001b[0m     absolute_diff \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(original_matrix \u001b[39m-\u001b[39;49m retrofitted_matrix)\n\u001b[0;32m      3\u001b[0m     mean_absolute_diff \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(absolute_diff)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_absolute_diff\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update_EN = -1  # Variable to store the best similarity score\n",
    "best_params_EN = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_EN_vec = retrofitting_wordVecs_test(wordVecMat_gensim, neighbors_matrix_gensim, alpha, beta, nb_iter)\n",
    "            embed_update_EN = measure_embedding_updates(wordVecMat_gensim, retrofitted_EN_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update_EN > best_embed_update_EN:\n",
    "                best_embed_update_EN = embed_update_EN\n",
    "                best_params_EN = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params_EN)\n",
    "print(\"Best embedding update:\", best_embed_update_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gensim = convert_matrix_to_dict(retrofitted_wordVecs_gensim, wordList_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing down the vectors in ../data/English/output_vectors/output_vectors.txt\n"
     ]
    }
   ],
   "source": [
    "print_word_vecs(output_gensim, outFileName_gensim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/French/word_embeddings/vecs100-linear-frwiki \n"
     ]
    }
   ],
   "source": [
    "wordVecs_FR = read_word_vecs(\"../data/French/word_embeddings/vecs100-linear-frwiki\")\n",
    "lexical_similarity_FR = read_lexicon(\"../data/French/lexicon/rg65_french.txt\")\n",
    "output_file_FR = \"../data/French/output_vectors/output_vectors.txt\"\n",
    "outFileName_FR = output_file_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_lexicon_FR(target_words, relation_types):\n",
    "    lexicon = {}\n",
    "        \n",
    "    for word in target_words:\n",
    "        related_words = []\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Skip word if no synsets found\n",
    "        if not word_synsets:\n",
    "            continue\n",
    "\n",
    "        for syn in word_synsets:\n",
    "            for lemma in syn.lemmas('fra'):\n",
    "                if lemma.name() != word:\n",
    "                    if \"synonyms\" in relation_types:\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"antonyms\" in relation_types:\n",
    "                if syn.lemmas('fra')[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            if \"hyponyms\" in relation_types:\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"hypernyms\" in relation_types:\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"meronyms\" in relation_types:\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"holonyms\" in relation_types:\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"homonyms\" in relation_types:\n",
    "                for lemma in syn.lemmas('fra'):\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wordnet.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ninan/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ninan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ninan/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ninan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m neighbors_dict \u001b[39m=\u001b[39m get_wordnet_lexicon_FR(wordList, \u001b[39m\"\u001b[39;49m\u001b[39msynonyms\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[78], line 13\u001b[0m, in \u001b[0;36mget_wordnet_lexicon_FR\u001b[1;34m(target_words, relation_types)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m syn \u001b[39min\u001b[39;00m word_synsets:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mfor\u001b[39;00m lemma \u001b[39min\u001b[39;00m syn\u001b[39m.\u001b[39;49mlemmas(\u001b[39m'\u001b[39;49m\u001b[39mfra\u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[0;32m     14\u001b[0m         \u001b[39mif\u001b[39;00m lemma\u001b[39m.\u001b[39mname() \u001b[39m!=\u001b[39m word:\n\u001b[0;32m     15\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msynonyms\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m relation_types:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\reader\\wordnet.py:502\u001b[0m, in \u001b[0;36mSynset.lemmas\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lemmas\n\u001b[0;32m    501\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name:\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wordnet_corpus_reader\u001b[39m.\u001b[39;49m_load_lang_data(lang)\n\u001b[0;32m    503\u001b[0m     lemmark \u001b[39m=\u001b[39m []\n\u001b[0;32m    504\u001b[0m     lemmy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlemma_names(lang)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1303\u001b[0m, in \u001b[0;36mWordNetCorpusReader._load_lang_data\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_omw_reader \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39momw_langs:\n\u001b[1;32m-> 1303\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_omw()\n\u001b[0;32m   1305\u001b[0m \u001b[39mif\u001b[39;00m lang \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangs():\n\u001b[0;32m   1306\u001b[0m     \u001b[39mraise\u001b[39;00m WordNetError(\u001b[39m\"\u001b[39m\u001b[39mLanguage is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1338\u001b[0m, in \u001b[0;36mWordNetCorpusReader.add_omw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_omw\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1338\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_provs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_omw_reader)\n\u001b[0;32m   1339\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39momw_langs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenances\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1325\u001b[0m, in \u001b[0;36mWordNetCorpusReader.add_provs\u001b[1;34m(self, reader)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_provs\u001b[39m(\u001b[39mself\u001b[39m, reader):\n\u001b[0;32m   1324\u001b[0m     \u001b[39m\"\"\"Add languages from Multilingual Wordnet to the provenance dictionary\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1325\u001b[0m     fileids \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mfileids()\n\u001b[0;32m   1326\u001b[0m     \u001b[39mfor\u001b[39;00m fileid \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m   1327\u001b[0m         prov, langfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ninan/nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\share\\\\nltk_data'\n    - 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ninan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "neighbors_dict = get_wordnet_lexicon_FR(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_FR = get_embeddings_words(wordVecs_FR)\n",
    "wordVecMat_FR = convert_dict_to_matrix(wordVecs_FR)\n",
    "neighbors_dict_FR = get_wordnet_lexicon(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "retrieve_neighbors_embedding_matrix() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m neighbors_matrix_FR \u001b[39m=\u001b[39m retrieve_neighbors_embedding_matrix(wordVecMat_FR, wordList_FR, \u001b[39m\"\u001b[39;49m\u001b[39msynonyms\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfra\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: retrieve_neighbors_embedding_matrix() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "neighbors_matrix_FR = retrieve_neighbors_embedding_matrix(wordVecMat_FR, wordList_FR, neighbors_dict_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrofitted_wordVecs_FR, updates_FR= retrofitting_wordVecs_test(wordVecMat_FR, neighbors_matrix_FR, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cos_similarity = calculate_average_cosine_similarity(wordVecMat_FR, retrofitted_wordVecs_FR)\n",
    "avg_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/French/lexicon/rg65_french.txt', 'r') as file:\n",
    "    lines_FR = file.readlines()\n",
    "\n",
    "print_lexical_similarities(wordVecs_FR, lines_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update_FR = -1  # Variable to store the best similarity score\n",
    "best_params_FR = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_FR_vec = retrofitting_wordVecs_test(wordVecMat_FR, neighbors_matrix_FR, alpha, beta, nb_iter)\n",
    "            embed_update_FR = measure_embedding_updates(wordVecMat_FR, retrofitted_FR_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update_FR > best_embed_update_FR:\n",
    "                best_embed_update_FR = embed_update_FR\n",
    "                best_params_FR = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params_FR)\n",
    "print(\"Best embedding update:\", best_embed_update_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_FR = convert_matrix_to_dict(retrofitted_wordVecs_FR, wordList_FR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
