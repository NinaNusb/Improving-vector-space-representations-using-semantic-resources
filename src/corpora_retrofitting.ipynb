{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\ninan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import argparse\n",
    "import gzip\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import urllib.request\n",
    "import io\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora, matutils\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained Word2Vec model\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNumber = re.compile(r'\\d+.*')\n",
    "\n",
    "def norm_word(word):\n",
    "  \"\"\"\n",
    "  - input: word\n",
    "  - return: a normalized version of it\n",
    "  Normalization process: includes checking if the word is a number or a punctuation mark and replacing it with special tokens\n",
    "  \"\"\"\n",
    "  if isNumber.search(word.lower()):\n",
    "    return '---num---'\n",
    "  # check if the word consists only of non-alphanumeric characters by removing all non-alphanumeric characters from the word \n",
    "  # and checking if the result is an empty string\n",
    "  elif re.sub(r'\\W+', '', word) == '':\n",
    "    return '---punc---'\n",
    "  else:\n",
    "  # if input word not a number nor a punctuation mark, return a lowercase version of input word\n",
    "    return word.lower()\n",
    "  \n",
    "\n",
    "  \n",
    "''' Read all the word vectors and normalize them '''\n",
    "def read_word_vecs(filename):\n",
    "  \"\"\"\n",
    "  - input: name of the file containing the word vectors\n",
    "  \"\"\"\n",
    "  wordVectors = {}\n",
    "  with open(filename, 'r', encoding='utf-8') as fileObject:\n",
    "    first_line = True\n",
    "    for line in fileObject:\n",
    "      line = line.strip().lower()\n",
    "      # Skip the first line\n",
    "      if first_line:\n",
    "        first_line =False\n",
    "        continue\n",
    "      # The first word is assumed to be the word itself, and the remaining words are assumed to be the components of the word vector\n",
    "      word = line.split()[0]\n",
    "      # initialize a numpy array of zeros with the same length as the word vector\n",
    "      wordVectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "      for index, vecVal in enumerate(line.split()[1:]):\n",
    "        # assign the values in the numpy array to the corresponding components of the word vector\n",
    "        wordVectors[word][index] = float(vecVal)\n",
    "      ''' normalize weight vector '''\n",
    "      # divide each element by the square root of the sum of the squares of all the elements in the array\n",
    "      # plus a small constant (1e-6) to avoid division by zero\n",
    "      wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
    "  \n",
    "  # standard error indicating that the vectors have been read from the file \n",
    "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "  return wordVectors\n",
    "\n",
    "  ''' Write word vectors to file '''\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "  \"\"\"\n",
    "  - input: a dictionary wordVectors where keys are words and values are their corresponding word vectors\n",
    "           file name outFileName\n",
    "  \"\"\"\n",
    "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
    "  outFile = open(outFileName, 'w', encoding= 'utf-8')  \n",
    "  for word, values in wordVectors.items():\n",
    "    outFile.write(word+' ')\n",
    "    for val in wordVectors[word]:\n",
    "      # write the word vectors to the ouptut file in the format:\n",
    "      # word1 val1 val2 val3 ...\n",
    "      # word2 val1 val2 val3 ...\n",
    "      # ...\n",
    "      outFile.write('%.4f' %(val)+' ')\n",
    "    outFile.write('\\n')      \n",
    "  outFile.close()\n",
    "\n",
    "''' Read the PPDB word relations as a dictionary '''\n",
    "def read_lexicon(filename):\n",
    "    lexicon = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.lower().strip().split()\n",
    "            lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the same format for the toy corpus as for the provided word embeddings\n",
    "def convert_matrix_to_dict(wordVecMat, wordList):\n",
    "    wordVecs = {}\n",
    "\n",
    "    for i, word in enumerate(wordList):\n",
    "        wordVecs[word] = wordVecMat[i]\n",
    "\n",
    "    return wordVecs\n",
    "\n",
    "def convert_dict_to_matrix(wordVecs):\n",
    "    wordVecMat = np.stack(list(wordVecs.values()))\n",
    "    return wordVecMat\n",
    "\n",
    "def vectorize_list(corpus):\n",
    "    corpus_vecs = [model[word] for word in corpus]\n",
    "\n",
    "    return corpus_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same input format as the real corpus\n",
    "toy_corpus = [\"cat\", \"tiger\", \"computer\", \"keyboard\", \"plane\", \"car\", \"doctor\", \"nurse\", \"love\", \"sex\"]\n",
    "toy_corpus_list_vecs = vectorize_list(toy_corpus)\n",
    "toy_wordVecs = convert_matrix_to_dict(toy_corpus_list_vecs, toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    similarity = dot_product / norm_product\n",
    "    return similarity\n",
    "\n",
    "def generate_cosine_similarity_matrix(dict_vecs): \n",
    "    num_vectors = len(dict_vecs)\n",
    "    similarity_matrix = np.zeros((num_vectors, num_vectors))\n",
    "    for i, word1 in enumerate(dict_vecs):\n",
    "        for j, word2 in enumerate(dict_vecs):\n",
    "            similarity_matrix[i, j] = calculate_cosine_similarity(dict_vecs[word1], dict_vecs[word2])\n",
    "    return similarity_matrix\n",
    "\n",
    "def print_vec_similarities(wordList, similarity_matrix):\n",
    "    for word, vec in zip(wordList, similarity_matrix):\n",
    "        print(f'Similarities with \"{word}\":')\n",
    "        for i in range(len(vec)):\n",
    "            similarity = vec[i]\n",
    "            print(f'  - \"{wordList[i]}\": {similarity:.4f}')\n",
    "        print()\n",
    "\n",
    "def print_similarity_difference(similarity_matrix, retrofitted_similarity_matrix):\n",
    "    difference = np.abs(similarity_matrix - retrofitted_similarity_matrix)\n",
    "    print(\"Similarity Difference Matrix:\")\n",
    "    print(difference)\n",
    "\n",
    "def cosine_similarity_matrix(matrix1, matrix2):\n",
    "    dot_product = np.sum(matrix1 * matrix2)\n",
    "    norm_matrix1 = np.linalg.norm(matrix1)\n",
    "    norm_matrix2 = np.linalg.norm(matrix2)\n",
    "    cosine_similarity = dot_product / (norm_matrix1 * norm_matrix2)\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5173\n",
      "  - \"computer\": 0.1732\n",
      "  - \"keyboard\": 0.1834\n",
      "  - \"plane\": 0.1833\n",
      "  - \"car\": 0.2153\n",
      "  - \"doctor\": 0.1292\n",
      "  - \"nurse\": 0.1594\n",
      "  - \"love\": 0.1406\n",
      "  - \"sex\": 0.1368\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0677\n",
      "  - \"keyboard\": 0.0654\n",
      "  - \"plane\": 0.1660\n",
      "  - \"car\": 0.1672\n",
      "  - \"doctor\": 0.0835\n",
      "  - \"nurse\": 0.1111\n",
      "  - \"love\": 0.0871\n",
      "  - \"sex\": 0.2222\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732\n",
      "  - \"tiger\": 0.0677\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.3964\n",
      "  - \"plane\": 0.1909\n",
      "  - \"car\": 0.2461\n",
      "  - \"doctor\": 0.1628\n",
      "  - \"nurse\": 0.2178\n",
      "  - \"love\": 0.0573\n",
      "  - \"sex\": 0.1853\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834\n",
      "  - \"tiger\": 0.0654\n",
      "  - \"computer\": 0.3964\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1006\n",
      "  - \"car\": 0.1498\n",
      "  - \"doctor\": 0.0850\n",
      "  - \"nurse\": 0.1220\n",
      "  - \"love\": 0.1591\n",
      "  - \"sex\": 0.0943\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833\n",
      "  - \"tiger\": 0.1660\n",
      "  - \"computer\": 0.1909\n",
      "  - \"keyboard\": 0.1006\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.3780\n",
      "  - \"doctor\": 0.1879\n",
      "  - \"nurse\": 0.0978\n",
      "  - \"love\": 0.1080\n",
      "  - \"sex\": 0.0587\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153\n",
      "  - \"tiger\": 0.1672\n",
      "  - \"computer\": 0.2461\n",
      "  - \"keyboard\": 0.1498\n",
      "  - \"plane\": 0.3780\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1895\n",
      "  - \"nurse\": 0.1306\n",
      "  - \"love\": 0.0842\n",
      "  - \"sex\": 0.1169\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292\n",
      "  - \"tiger\": 0.0835\n",
      "  - \"computer\": 0.1628\n",
      "  - \"keyboard\": 0.0850\n",
      "  - \"plane\": 0.1879\n",
      "  - \"car\": 0.1895\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6320\n",
      "  - \"love\": 0.0831\n",
      "  - \"sex\": 0.1994\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594\n",
      "  - \"tiger\": 0.1111\n",
      "  - \"computer\": 0.2178\n",
      "  - \"keyboard\": 0.1220\n",
      "  - \"plane\": 0.0978\n",
      "  - \"car\": 0.1306\n",
      "  - \"doctor\": 0.6320\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.0631\n",
      "  - \"sex\": 0.1997\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406\n",
      "  - \"tiger\": 0.0871\n",
      "  - \"computer\": 0.0573\n",
      "  - \"keyboard\": 0.1591\n",
      "  - \"plane\": 0.1080\n",
      "  - \"car\": 0.0842\n",
      "  - \"doctor\": 0.0831\n",
      "  - \"nurse\": 0.0631\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.2639\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368\n",
      "  - \"tiger\": 0.2222\n",
      "  - \"computer\": 0.1853\n",
      "  - \"keyboard\": 0.0943\n",
      "  - \"plane\": 0.0587\n",
      "  - \"car\": 0.1169\n",
      "  - \"doctor\": 0.1994\n",
      "  - \"nurse\": 0.1997\n",
      "  - \"love\": 0.2639\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = generate_cosine_similarity_matrix(toy_wordVecs)\n",
    "print_vec_similarities(toy_corpus, similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_lexicon(target_words, relation_types):\n",
    "    lexicon = {}\n",
    "        \n",
    "    for word in target_words:\n",
    "        related_words = []\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Skip word if no synsets found\n",
    "        if not word_synsets:\n",
    "            continue\n",
    "\n",
    "        for syn in word_synsets:\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    if \"synonyms\" in relation_types:\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"antonyms\" in relation_types:\n",
    "                if syn.lemmas()[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            if \"hyponyms\" in relation_types:\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"hypernyms\" in relation_types:\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"meronyms\" in relation_types:\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"holonyms\" in relation_types:\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas():\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"homonyms\" in relation_types:\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wordnet.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecMat = convert_dict_to_matrix(toy_wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(toy_corpus_list_vecs)) \n",
    "\n",
    "print(type(wordVecMat)) \n",
    "print(wordVecMat.shape)  \n",
    "print(wordVecMat.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 10)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(similarity_matrix)) \n",
    "print(similarity_matrix.shape)  \n",
    "print(similarity_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for the big corpus to retrive the word list from the keys\n",
    "def get_embeddings_words(wordVecs):\n",
    "    wordList = list(wordVecs.keys()) # TODO: or set?\n",
    "    return wordList\n",
    "\n",
    "wordList = get_embeddings_words(toy_wordVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neighbors_embedding_matrix(wordList, relation_type):\n",
    "    # Retrieve synonyms for each word\n",
    "    neighbors_dict = get_wordnet_lexicon(wordList, relation_type)\n",
    "    \n",
    "    # Compute average embedding\n",
    "    average_embeddings = []\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        embeddings = [\n",
    "            model.get_vector(neighbor)\n",
    "            for neighbor in neighbors\n",
    "            if model.has_index_for(neighbor)\n",
    "        ]\n",
    "        if len(embeddings) > 0:\n",
    "            average_embedding = np.sum(embeddings, axis=0) / len(embeddings)\n",
    "        else:\n",
    "            # Handle the case where a word has no embeddings for its synonyms\n",
    "            average_embedding = np.zeros(model.vector_size)  # Use a zero vector\n",
    "        average_embeddings.append(average_embedding)\n",
    "    \n",
    "    # Create the word embedding matrix\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "\n",
    "    return neighbors_embedding_matrix\n",
    "\n",
    "   \n",
    "    \n",
    "neighbors_matrix = create_neighbors_embedding_matrix(wordList, \"synonyms\")\n",
    "\n",
    "# récupérer la liste des syn dans wordnet\n",
    "# vectorise chaque syn\n",
    "# BOW des synonymes (sum) pour n'avoir qu'un embedding \n",
    "# BOW_syn_cat\n",
    "# BOW_syn_dog= neighbors_matrix, shape (10, embedding_size) donc same size as wordVecs_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(neighbors_matrix))  # <class 'numpy.ndarray'>\n",
    "print(neighbors_matrix.shape)  # (m, n)\n",
    "print(neighbors_matrix.ndim)   # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat))  # <class 'numpy.ndarray'>\n",
    "print(wordVecMat.shape)  # (m, n)\n",
    "print(wordVecMat.ndim)   # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00676925  0.17245371 -0.32560504 -0.02995695  0.24918167  0.06591797\n",
      "  0.07754347  0.02062197  0.12026186 -0.24899179  0.11663705 -0.43598995\n",
      " -0.0165247  -0.43618888  0.05141873 -0.2046328   0.09973597  0.0461245\n",
      " -0.4420053   0.08787028  0.26338252 -0.1518453   0.11536521 -0.14642108\n",
      " -0.04931188  0.32405486 -0.15808557  0.31547716  0.40427201 -0.007934\n",
      " -0.00195312 -0.20903128 -0.03433369 -0.0849519   0.01340795  0.15198545\n",
      " -0.07754234  0.04350902  0.00148463  0.24564164  0.00093107  0.03174506\n",
      " -0.14347048  0.13828702  0.08990253 -0.03038646  0.37447442 -0.05503337\n",
      "  0.00611821  0.00164738 -0.15880896  0.13665545  0.34953252  0.14862174\n",
      " -0.0103189  -0.14141733  0.15832859 -0.30018898  0.35803392  0.18776052\n",
      "  0.28351056  0.18667716 -0.01274052  0.19207764  0.04131345  0.04772498\n",
      "  0.20808016  0.02882668 -0.10890771 -0.12867793  0.40634721 -0.03548855\n",
      " -0.15468343 -0.01318077 -0.11528298  0.38514766  0.11844098 -0.08239746\n",
      " -0.02005401 -0.06692618  0.08503554 -0.00346544  0.1995228  -0.07893711\n",
      " -0.34383251  0.04050474 -0.2636391  -0.14694101 -0.1783899  -0.0918257\n",
      " -0.0131429   0.04139088 -0.06987395 -0.25662345 -0.17748967 -0.27476671\n",
      " -0.01672363 -0.17666287 -0.33676034 -0.18375199 -0.24108435 -0.02301817\n",
      " -0.0048376   0.25613178  0.07612101 -0.20470513  0.0953064  -0.03844798\n",
      "  0.04892759 -0.11653646 -0.07707384  0.29826298  0.11636692  0.32834201\n",
      " -0.31238471 -0.00066913 -0.02505154 -0.23873901 -0.22574163 -0.04350577\n",
      " -0.03468753 -0.05264395 -0.30785455  0.02505323 -0.34844179  0.11226626\n",
      "  0.10000073  0.1189044   0.04702872 -0.14758527  0.13923419  0.04321967\n",
      " -0.15519799 -0.06542969 -0.02879789 -0.11844042 -0.02837584  0.09594444\n",
      "  0.10183038  0.10198523 -0.06081022  0.02417896 -0.15294054  0.04794516\n",
      "  0.06066442  0.15922716 -0.19703731  0.17048589 -0.12367983 -0.15320898\n",
      "  0.22272971  0.09056261 -0.31849727 -0.08702935 -0.13079947 -0.05045121\n",
      "  0.09113679  0.05342385  0.03645833  0.22512252 -0.10272612 -0.05701814\n",
      "  0.09566696 -0.13046604 -0.12130398  0.00603117 -0.04856138  0.21248373\n",
      "  0.17837637  0.02974899 -0.08555999 -0.03059218 -0.04345082  0.23660165\n",
      " -0.08013295  0.09815696  0.07034867 -0.12642416 -0.242515    0.27940539\n",
      " -0.16617132  0.01541816 -0.00099126  0.0524677  -0.03739194  0.07791251\n",
      "  0.20973601  0.3004286   0.12838844  0.06714884  0.02089154 -0.16434733\n",
      "  0.12680845  0.06278935 -0.01593244  0.184226    0.07969835 -0.18987359\n",
      " -0.11193918  0.18708067  0.06552409 -0.27893744 -0.0675354  -0.05583897\n",
      "  0.04787643  0.11910897 -0.01438636 -0.13045473  0.10323758 -0.26696325\n",
      " -0.40234262 -0.02973316 -0.0515894  -0.08254327 -0.11705977 -0.0447789\n",
      "  0.16213056 -0.31804127 -0.10855222  0.12037037  0.18612897  0.11705413\n",
      " -0.22371081 -0.27236599 -0.2145137  -0.07454851  0.07460587 -0.07304269\n",
      "  0.22288344  0.00952374  0.10791355  0.03955135 -0.03007846  0.04184525\n",
      " -0.28511386  0.05631058  0.27960488  0.02073415 -0.08864961  0.066971\n",
      "  0.23477738 -0.20298824  0.03927584  0.03217344  0.33202221  0.05400481\n",
      "  0.05493726 -0.036039    0.22249632 -0.13921215 -0.33608415  0.18813239\n",
      " -0.28481038  0.02863178 -0.04690326  0.27920871 -0.1286395   0.1268627\n",
      " -0.15716192  0.07503933 -0.08402846  0.04506429 -0.04430474 -0.1314799\n",
      "  0.06542573  0.24106775 -0.10216381  0.02992079 -0.27746017 -0.02798801\n",
      " -0.07106696  0.04219563 -0.15462466  0.13976994  0.05472141  0.09482377\n",
      "  0.1267994   0.16184228  0.17293295 -0.14490651  0.2610078   0.05841742\n",
      " -0.20719062  0.28466684  0.13873743 -0.08083654 -0.09813182  0.04846644\n",
      "  0.08657498  0.13033097 -0.25310149  0.11868851 -0.1590384   0.03733656\n",
      "  0.42320421 -0.01760412  0.18546778  0.49357605 -0.276461    0.01063255]\n"
     ]
    }
   ],
   "source": [
    "difference = toy_corpus_list_vecs[0] - neighbors_matrix[0]\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter):\n",
    "    # Create a deep copy of wordVecMat \n",
    "    newWordVecMat = np.copy(wordVecMat, order='K')\n",
    "    updates = []\n",
    "    \n",
    "    for _ in range(nb_iter):\n",
    "        # Calculate the number of neighbors for each word\n",
    "        # numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\n",
    "        \n",
    "        # Update the word embeddings using retrofitting formula\n",
    "        newWordVecMat = (alpha * newWordVecMat + beta * neighbors_mean_matrix) / (alpha + beta)\n",
    "\n",
    "        # Calculate the updates\n",
    "        update = newWordVecMat - wordVecMat\n",
    "        updates.append(update)\n",
    "\n",
    "        # Update the wordVecMat for the next iteration\n",
    "        wordVecMat = newWordVecMat\n",
    "        # TODO: calculer similarité après chaque itération\n",
    "        # Stoping criterion\n",
    "        if np.linalg.norm(updates) < 1e-2:\n",
    "            break # TODO: return the embedding\n",
    "\n",
    "    # Convert the matrix back to a dictionary of word vectors\n",
    "    # retrofitted_wordVecs = dict(zip(wordList, newWordVecMat))\n",
    "\n",
    "    return newWordVecMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = get_wordnet_lexicon(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'touch_on': ['doctor'],\n",
       " 'lactate': ['nurse'],\n",
       " 'be_intimate': ['love'],\n",
       " 'Caterpillar': ['cat'],\n",
       " 'disgorge': ['cat'],\n",
       " 'level': ['plane'],\n",
       " 'airplane': ['plane'],\n",
       " 'upchuck': ['cat'],\n",
       " 'railcar': ['car'],\n",
       " 'erotic_love': ['love'],\n",
       " 'computing_machine': ['computer'],\n",
       " 'khat': ['cat'],\n",
       " 'sophisticate': ['doctor'],\n",
       " 'sick': ['cat'],\n",
       " 'enjoy': ['love'],\n",
       " 'motorcar': ['car'],\n",
       " 'shave': ['plane'],\n",
       " 'honk': ['cat'],\n",
       " 'know': ['love'],\n",
       " 'woodworking_plane': ['plane'],\n",
       " 'data_processor': ['computer'],\n",
       " 'kat': ['cat'],\n",
       " 'elevator_car': ['car'],\n",
       " 'computing_device': ['computer'],\n",
       " 'suck': ['nurse'],\n",
       " 'plane': ['sheet',\n",
       "  'aeroplane',\n",
       "  'planer',\n",
       "  'planing_machine',\n",
       "  'flat',\n",
       "  'shave',\n",
       "  \"carpenter's_plane\",\n",
       "  'level',\n",
       "  'woodworking_plane',\n",
       "  'skim',\n",
       "  'airplane'],\n",
       " 'bed': ['love'],\n",
       " 'tiger': ['Panthera_tigris'],\n",
       " 'making_love': ['love'],\n",
       " 'barf': ['cat'],\n",
       " 'doctor': ['touch_on',\n",
       "  'physician',\n",
       "  'mend',\n",
       "  'medico',\n",
       "  'furbish_up',\n",
       "  'MD',\n",
       "  'repair',\n",
       "  'Doctor',\n",
       "  'sophisticate',\n",
       "  'restore',\n",
       "  'Dr.',\n",
       "  'doc',\n",
       "  'Doctor_of_the_Church',\n",
       "  'doctor_up',\n",
       "  'fix',\n",
       "  'bushel'],\n",
       " 'car': ['automobile',\n",
       "  'auto',\n",
       "  'machine',\n",
       "  'railway_car',\n",
       "  'gondola',\n",
       "  'elevator_car',\n",
       "  'railroad_car',\n",
       "  'motorcar',\n",
       "  'cable_car',\n",
       "  'railcar'],\n",
       " 'gender': ['sex'],\n",
       " 'planer': ['plane'],\n",
       " 'regurgitate': ['cat'],\n",
       " 'nursemaid': ['nurse'],\n",
       " 'information_processing_system': ['computer'],\n",
       " 'puke': ['cat'],\n",
       " 'sex_activity': ['sex'],\n",
       " 'computerized_tomography': ['cat'],\n",
       " 'Dr.': ['doctor'],\n",
       " 'arouse': ['sex'],\n",
       " 'eff': ['love'],\n",
       " 'CAT': ['cat'],\n",
       " 'spue': ['cat'],\n",
       " 'sexual_love': ['love'],\n",
       " 'lovemaking': ['love'],\n",
       " 'physician': ['doctor'],\n",
       " 'automobile': ['car'],\n",
       " 'furbish_up': ['doctor'],\n",
       " 'sleep_with': ['love'],\n",
       " 'Panthera_tigris': ['tiger'],\n",
       " 'chuck': ['cat'],\n",
       " 'have_sex': ['love'],\n",
       " 'bozo': ['cat'],\n",
       " 'computed_axial_tomography': ['cat'],\n",
       " 'medico': ['doctor'],\n",
       " 'auto': ['car'],\n",
       " 'hold': ['nurse'],\n",
       " 'bonk': ['love'],\n",
       " 'love': ['be_intimate',\n",
       "  'have_it_away',\n",
       "  'have_a_go_at_it',\n",
       "  'roll_in_the_hay',\n",
       "  'eff',\n",
       "  'beloved',\n",
       "  'sexual_love',\n",
       "  'lovemaking',\n",
       "  'erotic_love',\n",
       "  'hump',\n",
       "  'sleep_with',\n",
       "  'sleep_together',\n",
       "  'lie_with',\n",
       "  'enjoy',\n",
       "  'make_out',\n",
       "  'have_sex',\n",
       "  'dear',\n",
       "  'dearest',\n",
       "  'know',\n",
       "  'jazz',\n",
       "  'screw',\n",
       "  'get_laid',\n",
       "  'bonk',\n",
       "  'do_it',\n",
       "  'fuck',\n",
       "  'love_life',\n",
       "  'bed',\n",
       "  'honey',\n",
       "  'making_love',\n",
       "  'bang',\n",
       "  'make_love',\n",
       "  'have_intercourse',\n",
       "  'get_it_on',\n",
       "  'have_it_off',\n",
       "  'passion'],\n",
       " 'cable_car': ['car'],\n",
       " 'sheet': ['plane'],\n",
       " 'bang': ['love'],\n",
       " 'figurer': ['computer'],\n",
       " 'make_love': ['love'],\n",
       " 'electronic_computer': ['computer'],\n",
       " 'aeroplane': ['plane'],\n",
       " 'nanny': ['nurse'],\n",
       " 'African_tea': ['cat'],\n",
       " 'CT': ['cat'],\n",
       " 'Arabian_tea': ['cat'],\n",
       " 'get_it_on': ['love'],\n",
       " 'cat': ['big_cat',\n",
       "  'puke',\n",
       "  'Caterpillar',\n",
       "  'computerized_tomography',\n",
       "  'disgorge',\n",
       "  'quat',\n",
       "  'CAT',\n",
       "  'spue',\n",
       "  'computerized_axial_tomography',\n",
       "  'upchuck',\n",
       "  'spew',\n",
       "  'khat',\n",
       "  'regorge',\n",
       "  'true_cat',\n",
       "  'sick',\n",
       "  'retch',\n",
       "  'hombre',\n",
       "  'chuck',\n",
       "  'bozo',\n",
       "  'honk',\n",
       "  'computed_axial_tomography',\n",
       "  'kat',\n",
       "  'computed_tomography',\n",
       "  'purge',\n",
       "  'throw_up',\n",
       "  'qat',\n",
       "  'vomit',\n",
       "  'guy',\n",
       "  \"cat-o'-nine-tails\",\n",
       "  'vomit_up',\n",
       "  'be_sick',\n",
       "  'barf',\n",
       "  'African_tea',\n",
       "  'CT',\n",
       "  'regurgitate',\n",
       "  'Arabian_tea',\n",
       "  'cast'],\n",
       " \"carpenter's_plane\": ['plane'],\n",
       " 'passion': ['love'],\n",
       " 'reckoner': ['computer'],\n",
       " 'sex': ['wind_up',\n",
       "  'sexual_activity',\n",
       "  'sex_activity',\n",
       "  'gender',\n",
       "  'sexual_urge',\n",
       "  'sexual_practice',\n",
       "  'arouse',\n",
       "  'excite',\n",
       "  'turn_on',\n",
       "  'sexuality'],\n",
       " 'have_a_go_at_it': ['love'],\n",
       " 'quat': ['cat'],\n",
       " 'bushel': ['doctor'],\n",
       " 'entertain': ['nurse'],\n",
       " 'beloved': ['love'],\n",
       " 'sleep_together': ['love'],\n",
       " 'sexual_activity': ['sex'],\n",
       " 'turn_on': ['sex'],\n",
       " 'hombre': ['cat'],\n",
       " 'give_suck': ['nurse'],\n",
       " 'Doctor_of_the_Church': ['doctor'],\n",
       " 'dear': ['love'],\n",
       " 'dearest': ['love'],\n",
       " 'calculator': ['computer'],\n",
       " 'machine': ['car'],\n",
       " 'repair': ['doctor'],\n",
       " 'get_laid': ['love'],\n",
       " 'vomit': ['cat'],\n",
       " 'do_it': ['love'],\n",
       " 'fuck': ['love'],\n",
       " 'love_life': ['love'],\n",
       " \"cat-o'-nine-tails\": ['cat'],\n",
       " 'vomit_up': ['cat'],\n",
       " 'estimator': ['computer'],\n",
       " 'be_sick': ['cat'],\n",
       " 'wind_up': ['sex'],\n",
       " 'harbour': ['nurse'],\n",
       " 'have_it_off': ['love'],\n",
       " 'cast': ['cat'],\n",
       " 'harbor': ['nurse'],\n",
       " 'big_cat': ['cat'],\n",
       " 'mend': ['doctor'],\n",
       " 'have_it_away': ['love'],\n",
       " 'keyboard': [],\n",
       " 'sexual_urge': ['sex'],\n",
       " 'roll_in_the_hay': ['love'],\n",
       " 'planing_machine': ['plane'],\n",
       " 'wet-nurse': ['nurse'],\n",
       " 'skim': ['plane'],\n",
       " 'computerized_axial_tomography': ['cat'],\n",
       " 'spew': ['cat'],\n",
       " 'hump': ['love'],\n",
       " 'Doctor': ['doctor'],\n",
       " 'regorge': ['cat'],\n",
       " 'restore': ['doctor'],\n",
       " 'true_cat': ['cat'],\n",
       " 'lie_with': ['love'],\n",
       " 'make_out': ['love'],\n",
       " 'retch': ['cat'],\n",
       " 'fix': ['doctor'],\n",
       " 'jazz': ['love'],\n",
       " 'breastfeed': ['nurse'],\n",
       " 'computed_tomography': ['cat'],\n",
       " 'screw': ['love'],\n",
       " 'computer': ['calculator',\n",
       "  'computing_machine',\n",
       "  'figurer',\n",
       "  'electronic_computer',\n",
       "  'computing_device',\n",
       "  'estimator',\n",
       "  'reckoner',\n",
       "  'information_processing_system',\n",
       "  'data_processor'],\n",
       " 'MD': ['doctor'],\n",
       " 'purge': ['cat'],\n",
       " 'throw_up': ['cat'],\n",
       " 'suckle': ['nurse'],\n",
       " 'qat': ['cat'],\n",
       " 'sexual_practice': ['sex'],\n",
       " 'railroad_car': ['car'],\n",
       " 'guy': ['cat'],\n",
       " 'excite': ['sex'],\n",
       " 'nurse': ['harbor',\n",
       "  'lactate',\n",
       "  'breastfeed',\n",
       "  'hold',\n",
       "  'suckle',\n",
       "  'harbour',\n",
       "  'nanny',\n",
       "  'suck',\n",
       "  'give_suck',\n",
       "  'wet-nurse',\n",
       "  'nursemaid',\n",
       "  'entertain'],\n",
       " 'doctor_up': ['doctor'],\n",
       " 'honey': ['love'],\n",
       " 'gondola': ['car'],\n",
       " 'railway_car': ['car'],\n",
       " 'have_intercourse': ['love'],\n",
       " 'flat': ['plane'],\n",
       " 'doc': ['doctor'],\n",
       " 'sexuality': ['sex']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_graph_from_synonyms(synonyms_dict):\n",
    "    graph = {}\n",
    "    \n",
    "    # Create a set of all unique words in the dictionary\n",
    "    words = set(synonyms_dict.keys()).union(*synonyms_dict.values())\n",
    "    \n",
    "    # Initialize an empty adjacency dictionary for each word\n",
    "    for word in words:\n",
    "        graph[word] = set()\n",
    "    \n",
    "    # Iterate through the synonyms dictionary\n",
    "    for word, synonyms in synonyms_dict.items():\n",
    "        # Add synonyms to the adjacency set for the word\n",
    "        graph[word].update(synonyms)\n",
    "        \n",
    "        # Add the word as a synonym to each synonym's adjacency set\n",
    "        for synonym in synonyms:\n",
    "            graph[synonym].add(word)\n",
    "    \n",
    "    # Convert the adjacency sets to lists\n",
    "    graph = {word: list(adjacency_set) for word, adjacency_set in graph.items()}\n",
    "    \n",
    "    return graph\n",
    "\n",
    "graph = generate_graph_from_synonyms(lexicon)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs_article(Q, Q_hat, graph, alpha, beta, num_iterations=10):\n",
    "    num_words = Q.shape[0]\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        Q_new = np.zeros_like(Q)\n",
    "        for i in range(num_words):\n",
    "            neighbors = graph[i]\n",
    "            numerator = np.sum(beta[i, j] * Q[j] for j in neighbors) + alpha[i] * Q_hat[i]\n",
    "            denominator = np.sum(beta[i, j] for j in neighbors) + alpha[i]\n",
    "            Q_new[i] = numerator / denominator\n",
    "        Q = Q_new\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,300) (100,250) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retrofitted_toy_vecs \u001b[39m=\u001b[39m retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, nb_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mretrofitting_wordVecs\u001b[1;34m(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m updates \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_iter):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Calculate the number of neighbors for each word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[39m# Update the word embeddings using retrofitting formula\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     newWordVecMat \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39;49m newWordVecMat \u001b[39m+\u001b[39;49m beta \u001b[39m*\u001b[39;49m neighbors_mean_matrix) \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m beta)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Calculate the updates\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     update \u001b[39m=\u001b[39m newWordVecMat \u001b[39m-\u001b[39m wordVecMat\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,300) (100,250) "
     ]
    }
   ],
   "source": [
    "retrofitted_toy_vecs = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newVecs = retrofitting_wordVecs_article(wordVecMat, neighbors_matrix, graph, alpha=1, beta=1, num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 6\n",
      "Number of edges: 5\n",
      "Neighbors of cat: ['kitten', 'animal', 'pet']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_to_keep = [\"cat\", \"kitten\", \"dog\", \"puppy\", \"animal\", \"pet\"]\n",
    "# Construct an empty graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_nodes_from(words_to_keep)\n",
    "\n",
    "# Add edges between related words\n",
    "graph.add_edges_from([('cat', 'kitten'), ('dog', 'puppy'), ('cat', 'animal'), ('dog', 'animal'), ('cat', 'pet')])\n",
    "\n",
    "# Print the graph information\n",
    "print(\"Number of nodes:\", graph.number_of_nodes())\n",
    "print(\"Number of edges:\", graph.number_of_edges())\n",
    "\n",
    "# Access neighbors of a word\n",
    "word = 'cat'\n",
    "neighbors = graph.neighbors(word)\n",
    "print(\"Neighbors of\", word + \":\", list(neighbors))\n",
    "\n",
    "# Create the layout for the graph\n",
    "layout = nx.spring_layout(graph)\n",
    "\n",
    "# Draw the nodes\n",
    "nx.draw_networkx_nodes(graph, pos=layout, node_color='lightblue', node_size=500)\n",
    "\n",
    "# Draw the edges\n",
    "nx.draw_networkx_edges(graph, pos=layout, edge_color='gray')\n",
    "\n",
    "# Add labels to the nodes\n",
    "nx.draw_networkx_labels(graph, pos=layout, font_color='black')\n",
    "\n",
    "# Set plot properties\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00338463, -0.08622685,  0.16280252, ..., -0.24678802,\n",
       "          0.1382305 , -0.00531627],\n",
       "        [ 0.08007812, -0.06433105,  0.02783203, ..., -0.08123779,\n",
       "          0.03717041, -0.03466797],\n",
       "        [ 0.00255203,  0.02276611, -0.13401031, ..., -0.07601929,\n",
       "          0.05496216,  0.07133484],\n",
       "        ...,\n",
       "        [ 0.04763455,  0.11241319, -0.07042948, ..., -0.00129022,\n",
       "         -0.09945594, -0.13053046],\n",
       "        [-0.03367829,  0.04928064, -0.00994873, ...,  0.08418322,\n",
       "          0.06436348,  0.01234341],\n",
       "        [ 0.04439545, -0.02075195,  0.11517334, ..., -0.06329346,\n",
       "          0.06408691, -0.06994629]]),\n",
       " array([[-0.00169231, -0.04311343,  0.08140126, ..., -0.12339401,\n",
       "          0.06911525, -0.00265814],\n",
       "        [ 0.04003906, -0.03216553,  0.01391602, ..., -0.0406189 ,\n",
       "          0.01858521, -0.01733398],\n",
       "        [ 0.00127602,  0.01138306, -0.06700516, ..., -0.03800964,\n",
       "          0.02748108,  0.03566742],\n",
       "        ...,\n",
       "        [ 0.02381727,  0.0562066 , -0.03521474, ..., -0.00064511,\n",
       "         -0.04972797, -0.06526523],\n",
       "        [-0.01683915,  0.02464032, -0.00497437, ...,  0.04209161,\n",
       "          0.03218174,  0.0061717 ],\n",
       "        [ 0.02219772, -0.01037598,  0.05758667, ..., -0.03164673,\n",
       "          0.03204346, -0.03497314]]),\n",
       " array([[-0.00084616, -0.02155671,  0.04070063, ..., -0.06169701,\n",
       "          0.03455763, -0.00132907],\n",
       "        [ 0.02001953, -0.01608276,  0.00695801, ..., -0.02030945,\n",
       "          0.0092926 , -0.00866699],\n",
       "        [ 0.00063801,  0.00569153, -0.03350258, ..., -0.01900482,\n",
       "          0.01374054,  0.01783371],\n",
       "        ...,\n",
       "        [ 0.01190864,  0.0281033 , -0.01760737, ..., -0.00032255,\n",
       "         -0.02486398, -0.03263262],\n",
       "        [-0.00841957,  0.01232016, -0.00248718, ...,  0.0210458 ,\n",
       "          0.01609087,  0.00308585],\n",
       "        [ 0.01109886, -0.00518799,  0.02879333, ..., -0.01582336,\n",
       "          0.01602173, -0.01748657]]),\n",
       " array([[-0.00042308, -0.01077836,  0.02035031, ..., -0.0308485 ,\n",
       "          0.01727881, -0.00066453],\n",
       "        [ 0.01000977, -0.00804138,  0.003479  , ..., -0.01015472,\n",
       "          0.0046463 , -0.0043335 ],\n",
       "        [ 0.000319  ,  0.00284576, -0.01675129, ..., -0.00950241,\n",
       "          0.00687027,  0.00891685],\n",
       "        ...,\n",
       "        [ 0.00595432,  0.01405165, -0.00880369, ..., -0.00016128,\n",
       "         -0.01243199, -0.01631631],\n",
       "        [-0.00420979,  0.00616008, -0.00124359, ...,  0.0105229 ,\n",
       "          0.00804543,  0.00154293],\n",
       "        [ 0.00554943, -0.00259399,  0.01439667, ..., -0.00791168,\n",
       "          0.00801086, -0.00874329]]),\n",
       " array([[-2.11539096e-04, -5.38917829e-03,  1.01751575e-02, ...,\n",
       "         -1.54242516e-02,  8.63940627e-03, -3.32267140e-04],\n",
       "        [ 5.00488281e-03, -4.02069092e-03,  1.73950195e-03, ...,\n",
       "         -5.07736206e-03,  2.32315063e-03, -2.16674805e-03],\n",
       "        [ 1.59502029e-04,  1.42288208e-03, -8.37564468e-03, ...,\n",
       "         -4.75120544e-03,  3.43513489e-03,  4.45842743e-03],\n",
       "        ...,\n",
       "        [ 2.97715928e-03,  7.02582463e-03, -4.40184260e-03, ...,\n",
       "         -8.06384487e-05, -6.21599623e-03, -8.15815385e-03],\n",
       "        [-2.10489333e-03,  3.08004022e-03, -6.21795654e-04, ...,\n",
       "          5.26145101e-03,  4.02271748e-03,  7.71462917e-04],\n",
       "        [ 2.77471542e-03, -1.29699707e-03,  7.19833374e-03, ...,\n",
       "         -3.95584106e-03,  4.00543213e-03, -4.37164307e-03]]),\n",
       " array([[-1.05769548e-04, -2.69458914e-03,  5.08757873e-03, ...,\n",
       "         -7.71212578e-03,  4.31970314e-03, -1.66133570e-04],\n",
       "        [ 2.50244141e-03, -2.01034546e-03,  8.69750977e-04, ...,\n",
       "         -2.53868103e-03,  1.16157532e-03, -1.08337402e-03],\n",
       "        [ 7.97510147e-05,  7.11441040e-04, -4.18782234e-03, ...,\n",
       "         -2.37560272e-03,  1.71756744e-03,  2.22921371e-03],\n",
       "        ...,\n",
       "        [ 1.48857964e-03,  3.51291231e-03, -2.20092130e-03, ...,\n",
       "         -4.03192244e-05, -3.10799811e-03, -4.07907693e-03],\n",
       "        [-1.05244666e-03,  1.54002011e-03, -3.10897827e-04, ...,\n",
       "          2.63072550e-03,  2.01135874e-03,  3.85731459e-04],\n",
       "        [ 1.38735771e-03, -6.48498535e-04,  3.59916687e-03, ...,\n",
       "         -1.97792053e-03,  2.00271606e-03, -2.18582153e-03]]),\n",
       " array([[-5.28847740e-05, -1.34729457e-03,  2.54378936e-03, ...,\n",
       "         -3.85606289e-03,  2.15985157e-03, -8.30667850e-05],\n",
       "        [ 1.25122070e-03, -1.00517273e-03,  4.34875488e-04, ...,\n",
       "         -1.26934052e-03,  5.80787659e-04, -5.41687012e-04],\n",
       "        [ 3.98755074e-05,  3.55720520e-04, -2.09391117e-03, ...,\n",
       "         -1.18780136e-03,  8.58783722e-04,  1.11460686e-03],\n",
       "        ...,\n",
       "        [ 7.44289820e-04,  1.75645616e-03, -1.10046065e-03, ...,\n",
       "         -2.01596122e-05, -1.55399906e-03, -2.03953846e-03],\n",
       "        [-5.26223332e-04,  7.70010054e-04, -1.55448914e-04, ...,\n",
       "          1.31536275e-03,  1.00567937e-03,  1.92865729e-04],\n",
       "        [ 6.93678856e-04, -3.24249268e-04,  1.79958344e-03, ...,\n",
       "         -9.88960266e-04,  1.00135803e-03, -1.09291077e-03]]),\n",
       " array([[-2.64423870e-05, -6.73647286e-04,  1.27189468e-03, ...,\n",
       "         -1.92803144e-03,  1.07992578e-03, -4.15333925e-05],\n",
       "        [ 6.25610352e-04, -5.02586365e-04,  2.17437744e-04, ...,\n",
       "         -6.34670258e-04,  2.90393829e-04, -2.70843506e-04],\n",
       "        [ 1.99377537e-05,  1.77860260e-04, -1.04695559e-03, ...,\n",
       "         -5.93900681e-04,  4.29391861e-04,  5.57303429e-04],\n",
       "        ...,\n",
       "        [ 3.72144910e-04,  8.78228078e-04, -5.50230325e-04, ...,\n",
       "         -1.00798061e-05, -7.76999528e-04, -1.01976923e-03],\n",
       "        [-2.63111666e-04,  3.85005027e-04, -7.77244568e-05, ...,\n",
       "          6.57681376e-04,  5.02839684e-04,  9.64328647e-05],\n",
       "        [ 3.46839428e-04, -1.62124634e-04,  8.99791718e-04, ...,\n",
       "         -4.94480133e-04,  5.00679016e-04, -5.46455383e-04]]),\n",
       " array([[-1.32211935e-05, -3.36823643e-04,  6.35947341e-04, ...,\n",
       "         -9.64015722e-04,  5.39962892e-04, -2.07666963e-05],\n",
       "        [ 3.12805176e-04, -2.51293182e-04,  1.08718872e-04, ...,\n",
       "         -3.17335129e-04,  1.45196915e-04, -1.35421753e-04],\n",
       "        [ 9.96887684e-06,  8.89301300e-05, -5.23477793e-04, ...,\n",
       "         -2.96950340e-04,  2.14695930e-04,  2.78651714e-04],\n",
       "        ...,\n",
       "        [ 1.86072455e-04,  4.39114039e-04, -2.75115162e-04, ...,\n",
       "         -5.03990304e-06, -3.88499764e-04, -5.09884616e-04],\n",
       "        [-1.31555833e-04,  1.92502514e-04, -3.88622284e-05, ...,\n",
       "          3.28840688e-04,  2.51419842e-04,  4.82164323e-05],\n",
       "        [ 1.73419714e-04, -8.10623169e-05,  4.49895859e-04, ...,\n",
       "         -2.47240067e-04,  2.50339508e-04, -2.73227692e-04]]),\n",
       " array([[-6.61059676e-06, -1.68411822e-04,  3.17973670e-04, ...,\n",
       "         -4.82007861e-04,  2.69981446e-04, -1.03833481e-05],\n",
       "        [ 1.56402588e-04, -1.25646591e-04,  5.43594360e-05, ...,\n",
       "         -1.58667564e-04,  7.25984573e-05, -6.77108765e-05],\n",
       "        [ 4.98443842e-06,  4.44650650e-05, -2.61738896e-04, ...,\n",
       "         -1.48475170e-04,  1.07347965e-04,  1.39325857e-04],\n",
       "        ...,\n",
       "        [ 9.30362276e-05,  2.19557020e-04, -1.37557581e-04, ...,\n",
       "         -2.51995152e-06, -1.94249882e-04, -2.54942308e-04],\n",
       "        [-6.57779165e-05,  9.62512568e-05, -1.94311142e-05, ...,\n",
       "          1.64420344e-04,  1.25709921e-04,  2.41082162e-05],\n",
       "        [ 8.67098570e-05, -4.05311584e-05,  2.24947929e-04, ...,\n",
       "         -1.23620033e-04,  1.25169754e-04, -1.36613846e-04]])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5173\n",
      "  - \"computer\": 0.1732\n",
      "  - \"keyboard\": 0.1834\n",
      "  - \"plane\": 0.1833\n",
      "  - \"car\": 0.2153\n",
      "  - \"doctor\": 0.1292\n",
      "  - \"nurse\": 0.1594\n",
      "  - \"love\": 0.1406\n",
      "  - \"sex\": 0.1368\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0677\n",
      "  - \"keyboard\": 0.0654\n",
      "  - \"plane\": 0.1660\n",
      "  - \"car\": 0.1672\n",
      "  - \"doctor\": 0.0835\n",
      "  - \"nurse\": 0.1111\n",
      "  - \"love\": 0.0871\n",
      "  - \"sex\": 0.2222\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732\n",
      "  - \"tiger\": 0.0677\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.3964\n",
      "  - \"plane\": 0.1909\n",
      "  - \"car\": 0.2461\n",
      "  - \"doctor\": 0.1628\n",
      "  - \"nurse\": 0.2178\n",
      "  - \"love\": 0.0573\n",
      "  - \"sex\": 0.1853\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834\n",
      "  - \"tiger\": 0.0654\n",
      "  - \"computer\": 0.3964\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1006\n",
      "  - \"car\": 0.1498\n",
      "  - \"doctor\": 0.0850\n",
      "  - \"nurse\": 0.1220\n",
      "  - \"love\": 0.1591\n",
      "  - \"sex\": 0.0943\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833\n",
      "  - \"tiger\": 0.1660\n",
      "  - \"computer\": 0.1909\n",
      "  - \"keyboard\": 0.1006\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.3780\n",
      "  - \"doctor\": 0.1879\n",
      "  - \"nurse\": 0.0978\n",
      "  - \"love\": 0.1080\n",
      "  - \"sex\": 0.0587\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153\n",
      "  - \"tiger\": 0.1672\n",
      "  - \"computer\": 0.2461\n",
      "  - \"keyboard\": 0.1498\n",
      "  - \"plane\": 0.3780\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1895\n",
      "  - \"nurse\": 0.1306\n",
      "  - \"love\": 0.0842\n",
      "  - \"sex\": 0.1169\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292\n",
      "  - \"tiger\": 0.0835\n",
      "  - \"computer\": 0.1628\n",
      "  - \"keyboard\": 0.0850\n",
      "  - \"plane\": 0.1879\n",
      "  - \"car\": 0.1895\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6320\n",
      "  - \"love\": 0.0831\n",
      "  - \"sex\": 0.1994\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594\n",
      "  - \"tiger\": 0.1111\n",
      "  - \"computer\": 0.2178\n",
      "  - \"keyboard\": 0.1220\n",
      "  - \"plane\": 0.0978\n",
      "  - \"car\": 0.1306\n",
      "  - \"doctor\": 0.6320\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.0631\n",
      "  - \"sex\": 0.1997\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406\n",
      "  - \"tiger\": 0.0871\n",
      "  - \"computer\": 0.0573\n",
      "  - \"keyboard\": 0.1591\n",
      "  - \"plane\": 0.1080\n",
      "  - \"car\": 0.0842\n",
      "  - \"doctor\": 0.0831\n",
      "  - \"nurse\": 0.0631\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.2639\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368\n",
      "  - \"tiger\": 0.2222\n",
      "  - \"computer\": 0.1853\n",
      "  - \"keyboard\": 0.0943\n",
      "  - \"plane\": 0.0587\n",
      "  - \"car\": 0.1169\n",
      "  - \"doctor\": 0.1994\n",
      "  - \"nurse\": 0.1997\n",
      "  - \"love\": 0.2639\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrofitted_toy_matrix = convert_dict_to_matrix(retrofitted_toy_vecs)\n",
    "retrofitted_similarity_matrix = generate_cosine_similarity_matrix(toy_wordVecs)\n",
    "print_vec_similarities(toy_corpus, retrofitted_similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5172961950302124"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrofitted_similarity_matrix[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5173 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1732 -> 0.1732 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1834 -> 0.1834 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1833 -> 0.1833 (Difference: 0.0000)\n",
      "  - \"car\": 0.2153 -> 0.2153 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1292 -> 0.1292 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1594 -> 0.1594 (Difference: 0.0000)\n",
      "  - \"love\": 0.1406 -> 0.1406 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1368 -> 0.1368 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5173 (Difference: 0.0000)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0677 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0654 -> 0.0654 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1660 -> 0.1660 (Difference: 0.0000)\n",
      "  - \"car\": 0.1672 -> 0.1672 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0835 -> 0.0835 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1111 -> 0.1111 (Difference: 0.0000)\n",
      "  - \"love\": 0.0871 -> 0.0871 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2222 -> 0.2222 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.1732 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0677 -> 0.0677 (Difference: 0.0000)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.3964 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1909 -> 0.1909 (Difference: 0.0000)\n",
      "  - \"car\": 0.2461 -> 0.2461 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1628 -> 0.1628 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.2178 -> 0.2178 (Difference: 0.0000)\n",
      "  - \"love\": 0.0573 -> 0.0573 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1853 -> 0.1853 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.1834 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0654 -> 0.0654 (Difference: 0.0000)\n",
      "  - \"computer\": 0.3964 -> 0.3964 (Difference: 0.0000)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1006 (Difference: 0.0000)\n",
      "  - \"car\": 0.1498 -> 0.1498 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0850 -> 0.0850 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1220 -> 0.1220 (Difference: 0.0000)\n",
      "  - \"love\": 0.1591 -> 0.1591 (Difference: 0.0000)\n",
      "  - \"sex\": 0.0943 -> 0.0943 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.1833 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1660 -> 0.1660 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1909 -> 0.1909 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1006 -> 0.1006 (Difference: 0.0000)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"car\": 0.3780 -> 0.3780 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1879 -> 0.1879 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.0978 -> 0.0978 (Difference: 0.0000)\n",
      "  - \"love\": 0.1080 -> 0.1080 (Difference: 0.0000)\n",
      "  - \"sex\": 0.0587 -> 0.0587 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2153 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1672 -> 0.1672 (Difference: 0.0000)\n",
      "  - \"computer\": 0.2461 -> 0.2461 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1498 -> 0.1498 (Difference: 0.0000)\n",
      "  - \"plane\": 0.3780 -> 0.3780 (Difference: 0.0000)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1895 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1306 -> 0.1306 (Difference: 0.0000)\n",
      "  - \"love\": 0.0842 -> 0.0842 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1169 -> 0.1169 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.1292 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0835 -> 0.0835 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1628 -> 0.1628 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0850 -> 0.0850 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1879 -> 0.1879 (Difference: 0.0000)\n",
      "  - \"car\": 0.1895 -> 0.1895 (Difference: 0.0000)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6320 (Difference: 0.0000)\n",
      "  - \"love\": 0.0831 -> 0.0831 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1994 -> 0.1994 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.1594 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.1111 -> 0.1111 (Difference: 0.0000)\n",
      "  - \"computer\": 0.2178 -> 0.2178 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1220 -> 0.1220 (Difference: 0.0000)\n",
      "  - \"plane\": 0.0978 -> 0.0978 (Difference: 0.0000)\n",
      "  - \"car\": 0.1306 -> 0.1306 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.6320 -> 0.6320 (Difference: 0.0000)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.0631 (Difference: 0.0000)\n",
      "  - \"sex\": 0.1997 -> 0.1997 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.1406 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.0871 -> 0.0871 (Difference: 0.0000)\n",
      "  - \"computer\": 0.0573 -> 0.0573 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.1591 -> 0.1591 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1080 -> 0.1080 (Difference: 0.0000)\n",
      "  - \"car\": 0.0842 -> 0.0842 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.0831 -> 0.0831 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.0631 -> 0.0631 (Difference: 0.0000)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.2639 (Difference: 0.0000)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.1368 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.2222 -> 0.2222 (Difference: 0.0000)\n",
      "  - \"computer\": 0.1853 -> 0.1853 (Difference: 0.0000)\n",
      "  - \"keyboard\": 0.0943 -> 0.0943 (Difference: 0.0000)\n",
      "  - \"plane\": 0.0587 -> 0.0587 (Difference: 0.0000)\n",
      "  - \"car\": 0.1169 -> 0.1169 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1994 -> 0.1994 (Difference: 0.0000)\n",
      "  - \"nurse\": 0.1997 -> 0.1997 (Difference: 0.0000)\n",
      "  - \"love\": 0.2639 -> 0.2639 (Difference: 0.0000)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "def print_vec_difference(wordList, similarity_matrix1, similarity_matrix2):\n",
    "    for i, word in enumerate(wordList):\n",
    "        print(f\"\\nSimilarities with \\\"{word}\\\":\")\n",
    "        for j, neighbor in enumerate(wordList):\n",
    "            similarity1 = similarity_matrix1[i, j]\n",
    "            similarity2 = similarity_matrix2[i, j]\n",
    "            difference = similarity2 - similarity1  # Calculate the difference\n",
    "            print(f\"  - \\\"{neighbor}\\\": {similarity1:.4f} -> {similarity2:.4f} (Difference: {difference:.4f})\")\n",
    "\n",
    "print_vec_difference(toy_corpus, similarity_matrix, retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Difference Matrix:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print_similarity_difference(similarity_matrix, retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between wordVecMat and retrofitted_toy_vec\n",
    "# similarity_score = cosine_similarity_matrix(wordVecMat, retrofitted_toy_vecs)\n",
    "# print(\"Cosine Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average embedding update: 0.12658860989283166\n"
     ]
    }
   ],
   "source": [
    "def measure_embedding_updates(original_matrix, retrofitted_matrix):\n",
    "    absolute_diff = np.abs(original_matrix - retrofitted_matrix)\n",
    "    mean_absolute_diff = np.mean(absolute_diff)\n",
    "    return mean_absolute_diff\n",
    "\n",
    "# Example usage\n",
    "update_measure = measure_embedding_updates(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Average embedding update:\", update_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation score: 0.42055888661904023\n",
      "Pearson correlation score: 0.4232245375810048\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def spearman_measure_embedding_similarity(original_matrix, retrofitted_matrix):\n",
    "    original_flat = original_matrix.flatten()\n",
    "    retrofitted_flat = retrofitted_matrix.flatten()\n",
    "    correlation, _ = spearmanr(original_flat, retrofitted_flat)\n",
    "    return correlation\n",
    "\n",
    "def pearson_measure_embedding_similarity(original_matrix, retrofitted_matrix):\n",
    "    original_flat = original_matrix.flatten()\n",
    "    retrofitted_flat = retrofitted_matrix.flatten()\n",
    "    correlation, _ = pearsonr(original_flat, retrofitted_flat)\n",
    "    return correlation\n",
    "\n",
    "similarity_score = spearman_measure_embedding_similarity(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Spearman correlation score:\", similarity_score)\n",
    "similarity_score = pearson_measure_embedding_similarity(wordVecMat, retrofitted_toy_vecs)\n",
    "print(\"Pearson correlation score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,300) (100,250) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m beta \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0.1\u001b[39m, \u001b[39m5.1\u001b[39m, \u001b[39m0.2\u001b[39m):\n\u001b[0;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m nb_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m         retrofitted_toy_vec, _ \u001b[39m=\u001b[39m retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n\u001b[0;32m     22\u001b[0m         cosine_sim \u001b[39m=\u001b[39m cosine_similarity(wordVecMat, retrofitted_toy_vec)\n\u001b[0;32m     24\u001b[0m         \u001b[39m# Calculate Spearman correlation against human evaluation scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mretrofitting_wordVecs\u001b[1;34m(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m updates \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_iter):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Calculate the number of neighbors for each word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[39m# Update the word embeddings using retrofitting formula\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     newWordVecMat \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39;49m newWordVecMat \u001b[39m+\u001b[39;49m beta \u001b[39m*\u001b[39;49m neighbors_mean_matrix) \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m beta)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Calculate the updates\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     update \u001b[39m=\u001b[39m newWordVecMat \u001b[39m-\u001b[39m wordVecMat\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,300) (100,250) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load human evaluation scores\n",
    "eval_file_path = r\"C:\\Users\\ninan\\OneDrive\\Bureau\\Université Paris Cité\\S2\\NLP project\\Improving-vector-space-representations-using-semantic-resources\\data\\English\\lexicon\\ws353_lexical_similarity.txt\"\n",
    "eval_scores = {}\n",
    "with open(eval_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        word1, word2, score = line.strip().split('\\t')\n",
    "        eval_scores[(word1, word2)] = float(score)\n",
    "\n",
    "# Find best values for hyperparameters\n",
    "best_similarity_score = -1  # Variable to store the best similarity score\n",
    "best_params = {}  # Dictionary to store the best hyperparameter values\n",
    "iteration_count = 0\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1, 16):\n",
    "            retrofitted_toy_vec = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "            cosine_sim = cosine_similarity(wordVecMat, retrofitted_toy_vec)\n",
    "\n",
    "            # Calculate Spearman correlation against human evaluation scores\n",
    "            eval_scores_list = []\n",
    "            cosine_sim_list = []\n",
    "            for (word1, word2), score in eval_scores.items():\n",
    "                if word1 in wordList and word2 in wordList:\n",
    "                    word1_index = wordList.index(word1)\n",
    "                    word2_index = wordList.index(word2)\n",
    "                    eval_scores_list.append(score)\n",
    "                    cosine_sim_list.append(cosine_sim[word1_index, word2_index])\n",
    "\n",
    "            # Check if there are valid pairs for comparison\n",
    "            if len(eval_scores_list) > 0 and len(cosine_sim_list) > 0:\n",
    "                correlation, _ = spearmanr(eval_scores_list, cosine_sim_list)\n",
    "                # print(\"alpha =\", alpha, \"beta =\", beta, \"nb_iter =\", nb_iter, \"correlation =\", correlation)\n",
    "\n",
    "                # Update best similarity score and parameters if improved\n",
    "                if correlation > best_similarity_score:\n",
    "                    best_similarity_score = correlation\n",
    "                    best_params = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "            iteration_count += 1\n",
    "            if iteration_count >= 100:\n",
    "                break\n",
    "        if iteration_count >= 100:\n",
    "            break\n",
    "    if iteration_count >= 100:\n",
    "        break\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best Spearman correlation score:\", best_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.1, 'beta': 1.1000000000000003, 'nb_iter': 15}\n",
      "Best embedding update: 0.12671235242449622\n"
     ]
    }
   ],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update = -1  # Variable to store the best similarity score\n",
    "best_params = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_toy_vec = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "            embed_update = measure_embedding_updates(wordVecMat, retrofitted_toy_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update > best_embed_update:\n",
    "                best_embed_update = embed_update\n",
    "                best_params = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best embedding update:\", best_embed_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000\n",
      "  - \"tiger\": 0.5134\n",
      "  - \"computer\": 0.2147\n",
      "  - \"keyboard\": 0.2270\n",
      "  - \"plane\": 0.2878\n",
      "  - \"car\": 0.2492\n",
      "  - \"doctor\": 0.2437\n",
      "  - \"nurse\": 0.3233\n",
      "  - \"love\": 0.3254\n",
      "  - \"sex\": 0.2202\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5134\n",
      "  - \"tiger\": 1.0000\n",
      "  - \"computer\": 0.0783\n",
      "  - \"keyboard\": 0.1118\n",
      "  - \"plane\": 0.1769\n",
      "  - \"car\": 0.1518\n",
      "  - \"doctor\": 0.0866\n",
      "  - \"nurse\": 0.1714\n",
      "  - \"love\": 0.1518\n",
      "  - \"sex\": 0.2417\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.2147\n",
      "  - \"tiger\": 0.0783\n",
      "  - \"computer\": 1.0000\n",
      "  - \"keyboard\": 0.4058\n",
      "  - \"plane\": 0.2883\n",
      "  - \"car\": 0.3039\n",
      "  - \"doctor\": 0.2093\n",
      "  - \"nurse\": 0.2180\n",
      "  - \"love\": 0.1358\n",
      "  - \"sex\": 0.1602\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.2270\n",
      "  - \"tiger\": 0.1118\n",
      "  - \"computer\": 0.4058\n",
      "  - \"keyboard\": 1.0000\n",
      "  - \"plane\": 0.1872\n",
      "  - \"car\": 0.1700\n",
      "  - \"doctor\": 0.1131\n",
      "  - \"nurse\": 0.1637\n",
      "  - \"love\": 0.2198\n",
      "  - \"sex\": 0.1140\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.2878\n",
      "  - \"tiger\": 0.1769\n",
      "  - \"computer\": 0.2883\n",
      "  - \"keyboard\": 0.1872\n",
      "  - \"plane\": 1.0000\n",
      "  - \"car\": 0.4366\n",
      "  - \"doctor\": 0.2202\n",
      "  - \"nurse\": 0.1756\n",
      "  - \"love\": 0.1935\n",
      "  - \"sex\": 0.0994\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2492\n",
      "  - \"tiger\": 0.1518\n",
      "  - \"computer\": 0.3039\n",
      "  - \"keyboard\": 0.1700\n",
      "  - \"plane\": 0.4366\n",
      "  - \"car\": 1.0000\n",
      "  - \"doctor\": 0.1865\n",
      "  - \"nurse\": 0.1453\n",
      "  - \"love\": 0.1766\n",
      "  - \"sex\": 0.1273\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.2437\n",
      "  - \"tiger\": 0.0866\n",
      "  - \"computer\": 0.2093\n",
      "  - \"keyboard\": 0.1131\n",
      "  - \"plane\": 0.2202\n",
      "  - \"car\": 0.1865\n",
      "  - \"doctor\": 1.0000\n",
      "  - \"nurse\": 0.6105\n",
      "  - \"love\": 0.1844\n",
      "  - \"sex\": 0.1923\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.3233\n",
      "  - \"tiger\": 0.1714\n",
      "  - \"computer\": 0.2180\n",
      "  - \"keyboard\": 0.1637\n",
      "  - \"plane\": 0.1756\n",
      "  - \"car\": 0.1453\n",
      "  - \"doctor\": 0.6105\n",
      "  - \"nurse\": 1.0000\n",
      "  - \"love\": 0.2675\n",
      "  - \"sex\": 0.3084\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.3254\n",
      "  - \"tiger\": 0.1518\n",
      "  - \"computer\": 0.1358\n",
      "  - \"keyboard\": 0.2198\n",
      "  - \"plane\": 0.1935\n",
      "  - \"car\": 0.1766\n",
      "  - \"doctor\": 0.1844\n",
      "  - \"nurse\": 0.2675\n",
      "  - \"love\": 1.0000\n",
      "  - \"sex\": 0.3760\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.2202\n",
      "  - \"tiger\": 0.2417\n",
      "  - \"computer\": 0.1602\n",
      "  - \"keyboard\": 0.1140\n",
      "  - \"plane\": 0.0994\n",
      "  - \"car\": 0.1273\n",
      "  - \"doctor\": 0.1923\n",
      "  - \"nurse\": 0.3084\n",
      "  - \"love\": 0.3760\n",
      "  - \"sex\": 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_retrofitted_toy_matrix = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha=0.1, beta=0.1, nb_iter=1)\n",
    "new_retrofitted_toy_dict = convert_matrix_to_dict(new_retrofitted_toy_matrix, wordList)\n",
    "new_retrofitted_similarity_matrix = generate_cosine_similarity_matrix(new_retrofitted_toy_dict)\n",
    "print_vec_similarities(toy_corpus, new_retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"computer\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"keyboard\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"plane\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"car\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"doctor\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"nurse\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"love\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"sex\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"keyboard\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"plane\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"car\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"doctor\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"nurse\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"love\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"sex\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"tiger\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"plane\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"car\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"doctor\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"nurse\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"love\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"sex\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"tiger\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"computer\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"car\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"doctor\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"nurse\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"love\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"sex\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"tiger\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"computer\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"keyboard\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"car\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"doctor\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"nurse\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"love\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"sex\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"tiger\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"computer\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"keyboard\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"plane\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"nurse\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"love\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"sex\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"tiger\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"computer\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"keyboard\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"plane\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"car\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"love\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"sex\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"tiger\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"computer\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"keyboard\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"plane\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"car\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"doctor\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"sex\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"tiger\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"computer\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"keyboard\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"plane\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"car\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"doctor\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"nurse\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "  - \"tiger\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "  - \"computer\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "  - \"keyboard\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "  - \"plane\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "  - \"car\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "  - \"doctor\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "  - \"nurse\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "  - \"love\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and after tuning hyperparam\n",
    "print_vec_difference(toy_corpus, similarity_matrix, new_retrofitted_similarity_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities with \"cat\":\n",
      "  - \"cat\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"tiger\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"computer\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"keyboard\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"plane\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"car\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"doctor\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"nurse\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"love\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"sex\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "\n",
      "Similarities with \"tiger\":\n",
      "  - \"cat\": 0.5173 -> 0.5134 (Difference: -0.0039)\n",
      "  - \"tiger\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"computer\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"keyboard\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"plane\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"car\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"doctor\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"nurse\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"love\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"sex\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "\n",
      "Similarities with \"computer\":\n",
      "  - \"cat\": 0.1732 -> 0.2147 (Difference: 0.0415)\n",
      "  - \"tiger\": 0.0677 -> 0.0783 (Difference: 0.0106)\n",
      "  - \"computer\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"keyboard\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"plane\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"car\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"doctor\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"nurse\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"love\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"sex\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "\n",
      "Similarities with \"keyboard\":\n",
      "  - \"cat\": 0.1834 -> 0.2270 (Difference: 0.0436)\n",
      "  - \"tiger\": 0.0654 -> 0.1118 (Difference: 0.0464)\n",
      "  - \"computer\": 0.3964 -> 0.4058 (Difference: 0.0094)\n",
      "  - \"keyboard\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"plane\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"car\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"doctor\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"nurse\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"love\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"sex\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "\n",
      "Similarities with \"plane\":\n",
      "  - \"cat\": 0.1833 -> 0.2878 (Difference: 0.1045)\n",
      "  - \"tiger\": 0.1660 -> 0.1769 (Difference: 0.0109)\n",
      "  - \"computer\": 0.1909 -> 0.2883 (Difference: 0.0974)\n",
      "  - \"keyboard\": 0.1006 -> 0.1872 (Difference: 0.0866)\n",
      "  - \"plane\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"car\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"doctor\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"nurse\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"love\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"sex\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "\n",
      "Similarities with \"car\":\n",
      "  - \"cat\": 0.2153 -> 0.2492 (Difference: 0.0339)\n",
      "  - \"tiger\": 0.1672 -> 0.1518 (Difference: -0.0154)\n",
      "  - \"computer\": 0.2461 -> 0.3039 (Difference: 0.0577)\n",
      "  - \"keyboard\": 0.1498 -> 0.1700 (Difference: 0.0201)\n",
      "  - \"plane\": 0.3780 -> 0.4366 (Difference: 0.0586)\n",
      "  - \"car\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"doctor\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"nurse\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"love\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"sex\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "\n",
      "Similarities with \"doctor\":\n",
      "  - \"cat\": 0.1292 -> 0.2437 (Difference: 0.1145)\n",
      "  - \"tiger\": 0.0835 -> 0.0866 (Difference: 0.0031)\n",
      "  - \"computer\": 0.1628 -> 0.2093 (Difference: 0.0465)\n",
      "  - \"keyboard\": 0.0850 -> 0.1131 (Difference: 0.0281)\n",
      "  - \"plane\": 0.1879 -> 0.2202 (Difference: 0.0323)\n",
      "  - \"car\": 0.1895 -> 0.1865 (Difference: -0.0031)\n",
      "  - \"doctor\": 1.0000 -> 1.0000 (Difference: -0.0000)\n",
      "  - \"nurse\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"love\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"sex\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "\n",
      "Similarities with \"nurse\":\n",
      "  - \"cat\": 0.1594 -> 0.3233 (Difference: 0.1639)\n",
      "  - \"tiger\": 0.1111 -> 0.1714 (Difference: 0.0603)\n",
      "  - \"computer\": 0.2178 -> 0.2180 (Difference: 0.0002)\n",
      "  - \"keyboard\": 0.1220 -> 0.1637 (Difference: 0.0417)\n",
      "  - \"plane\": 0.0978 -> 0.1756 (Difference: 0.0778)\n",
      "  - \"car\": 0.1306 -> 0.1453 (Difference: 0.0148)\n",
      "  - \"doctor\": 0.6320 -> 0.6105 (Difference: -0.0215)\n",
      "  - \"nurse\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"love\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"sex\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "\n",
      "Similarities with \"love\":\n",
      "  - \"cat\": 0.1406 -> 0.3254 (Difference: 0.1848)\n",
      "  - \"tiger\": 0.0871 -> 0.1518 (Difference: 0.0647)\n",
      "  - \"computer\": 0.0573 -> 0.1358 (Difference: 0.0784)\n",
      "  - \"keyboard\": 0.1591 -> 0.2198 (Difference: 0.0607)\n",
      "  - \"plane\": 0.1080 -> 0.1935 (Difference: 0.0855)\n",
      "  - \"car\": 0.0842 -> 0.1766 (Difference: 0.0924)\n",
      "  - \"doctor\": 0.0831 -> 0.1844 (Difference: 0.1013)\n",
      "  - \"nurse\": 0.0631 -> 0.2675 (Difference: 0.2044)\n",
      "  - \"love\": 1.0000 -> 1.0000 (Difference: 0.0000)\n",
      "  - \"sex\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "\n",
      "Similarities with \"sex\":\n",
      "  - \"cat\": 0.1368 -> 0.2202 (Difference: 0.0833)\n",
      "  - \"tiger\": 0.2222 -> 0.2417 (Difference: 0.0194)\n",
      "  - \"computer\": 0.1853 -> 0.1602 (Difference: -0.0251)\n",
      "  - \"keyboard\": 0.0943 -> 0.1140 (Difference: 0.0197)\n",
      "  - \"plane\": 0.0587 -> 0.0994 (Difference: 0.0407)\n",
      "  - \"car\": 0.1169 -> 0.1273 (Difference: 0.0104)\n",
      "  - \"doctor\": 0.1994 -> 0.1923 (Difference: -0.0071)\n",
      "  - \"nurse\": 0.1997 -> 0.3084 (Difference: 0.1087)\n",
      "  - \"love\": 0.2639 -> 0.3760 (Difference: 0.1121)\n",
      "  - \"sex\": 1.0000 -> 1.0000 (Difference: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Difference between retrofitted embeddings and after tuning hyperaparams\n",
    "print_vec_difference(toy_corpus, retrofitted_similarity_matrix, new_retrofitted_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>tiger</th>\n",
       "      <th>computer</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>plane</th>\n",
       "      <th>car</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "      <th>love</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>Before: 0.29245239862991185, After: 1.00000000...</td>\n",
       "      <td>Before: 0.35037322527996934, After: 0.20200897...</td>\n",
       "      <td>Before: 0.06136659928459301, After: 0.19719178...</td>\n",
       "      <td>Before: 0.18344955625364173, After: 0.20547455...</td>\n",
       "      <td>Before: 0.21134097025538556, After: 0.39445950...</td>\n",
       "      <td>Before: 0.136072027917602, After: 0.2634861035...</td>\n",
       "      <td>Before: 0.16547475026405636, After: 0.34338708...</td>\n",
       "      <td>Before: 0.24690899138830727, After: 0.54854860...</td>\n",
       "      <td>Before: 0.2989067535773432, After: 0.594053102...</td>\n",
       "      <td>Before: 0.1093022564011111, After: 0.259890845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>Before: 0.17347637872059773, After: 0.20200897...</td>\n",
       "      <td>Before: 0.48802072485209846, After: 1.00000000...</td>\n",
       "      <td>Before: 0.02942476361382193, After: 0.09025702...</td>\n",
       "      <td>Before: 0.06542581824273716, After: 0.18313975...</td>\n",
       "      <td>Before: 0.15090617196611955, After: 0.07307479...</td>\n",
       "      <td>Before: 0.16119598769364896, After: 0.10033040...</td>\n",
       "      <td>Before: 0.0774108002811773, After: 0.089083542...</td>\n",
       "      <td>Before: 0.1697249630498177, After: 0.202597136...</td>\n",
       "      <td>Before: 0.17516455047450735, After: 0.19102605...</td>\n",
       "      <td>Before: 0.14518818082220147, After: 0.18807845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>Before: 0.18984798734723213, After: 0.19719178...</td>\n",
       "      <td>Before: 0.05360514929979338, After: 0.09025702...</td>\n",
       "      <td>Before: 0.3047689400402037, After: 1.0</td>\n",
       "      <td>Before: 0.39639163439495995, After: 0.23934215...</td>\n",
       "      <td>Before: 0.28314903703275535, After: 0.40858220...</td>\n",
       "      <td>Before: 0.26826894486108244, After: 0.22732308...</td>\n",
       "      <td>Before: 0.18039329196329013, After: 0.20986951...</td>\n",
       "      <td>Before: 0.09297679298707379, After: 0.12068024...</td>\n",
       "      <td>Before: 0.1168711356729625, After: 0.234808759...</td>\n",
       "      <td>Before: 0.1158779845883439, After: 0.076953257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>Before: 0.20546589125510897, After: 0.20547455...</td>\n",
       "      <td>Before: 0.18314156317766062, After: 0.18313975...</td>\n",
       "      <td>Before: 0.2393287663091698, After: 0.239342157...</td>\n",
       "      <td>Before: 0.9999999999999996, After: 1.0</td>\n",
       "      <td>Before: 0.28971014677665063, After: 0.28970944...</td>\n",
       "      <td>Before: 0.15750632650840493, After: 0.15750846...</td>\n",
       "      <td>Before: 0.13804955762057758, After: 0.13804958...</td>\n",
       "      <td>Before: 0.16678031434047627, After: 0.16678347...</td>\n",
       "      <td>Before: 0.27407501845767246, After: 0.27407454...</td>\n",
       "      <td>Before: 0.10639718565916091, After: 0.10639914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>Before: 0.16761576142114898, After: 0.39445950...</td>\n",
       "      <td>Before: 0.06730278214710199, After: 0.07307479...</td>\n",
       "      <td>Before: 0.06606645196552532, After: 0.40858220...</td>\n",
       "      <td>Before: 0.10055138151211143, After: 0.28970944...</td>\n",
       "      <td>Before: 0.38405259310022044, After: 1.00000000...</td>\n",
       "      <td>Before: 0.3219357387657039, After: 0.354037032...</td>\n",
       "      <td>Before: 0.15146728859985834, After: 0.21833826...</td>\n",
       "      <td>Before: 0.10589313234551631, After: 0.28236372...</td>\n",
       "      <td>Before: 0.1936940498815496, After: 0.328525840...</td>\n",
       "      <td>Before: 0.05697047025775614, After: 0.11862340...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>Before: 0.15859645192729996, After: 0.26348610...</td>\n",
       "      <td>Before: -0.049076379792710186, After: 0.100330...</td>\n",
       "      <td>Before: 0.13968323112249992, After: 0.22732308...</td>\n",
       "      <td>Before: 0.14983822223318854, After: 0.15750846...</td>\n",
       "      <td>Before: 0.2597415410728325, After: 0.354037032...</td>\n",
       "      <td>Before: 0.6158574248530795, After: 1.000000000...</td>\n",
       "      <td>Before: 0.13778205919388814, After: 0.19190468...</td>\n",
       "      <td>Before: 0.0865221800713605, After: 0.160778501...</td>\n",
       "      <td>Before: 0.22133439390681858, After: 0.24947266...</td>\n",
       "      <td>Before: 0.059270287407368415, After: 0.0662935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>Before: 0.25878907406446877, After: 0.34338708...</td>\n",
       "      <td>Before: 0.017639064485218965, After: 0.0890835...</td>\n",
       "      <td>Before: 0.0978054983995635, After: 0.209869514...</td>\n",
       "      <td>Before: 0.08500327165730943, After: 0.13804958...</td>\n",
       "      <td>Before: 0.13439994429915442, After: 0.21833826...</td>\n",
       "      <td>Before: 0.09431570687955, After: 0.19190468704...</td>\n",
       "      <td>Before: 0.6128107065755533, After: 1.0</td>\n",
       "      <td>Before: 0.27845097194129365, After: 0.34448646...</td>\n",
       "      <td>Before: 0.19325643013428553, After: 0.33078374...</td>\n",
       "      <td>Before: 0.0745189116843016, After: 0.175547118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>Before: 0.18217682480604822, After: 0.54854860...</td>\n",
       "      <td>Before: 0.07222178145580241, After: 0.20259713...</td>\n",
       "      <td>Before: 0.11697573887301807, After: 0.12068024...</td>\n",
       "      <td>Before: 0.12199094008346709, After: 0.16678347...</td>\n",
       "      <td>Before: 0.12790409339777273, After: 0.28236372...</td>\n",
       "      <td>Before: 0.07363428182232754, After: 0.16077850...</td>\n",
       "      <td>Before: 0.4308149649767604, After: 0.344486463...</td>\n",
       "      <td>Before: 0.378466332225932, After: 1.0000000000...</td>\n",
       "      <td>Before: 0.1708752362535986, After: 0.544714533...</td>\n",
       "      <td>Before: 0.1264203000118017, After: 0.409129275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>Before: 0.241072229246228, After: 0.5940531024...</td>\n",
       "      <td>Before: 0.10199943514005788, After: 0.19102605...</td>\n",
       "      <td>Before: 0.0730022470894344, After: 0.234808759...</td>\n",
       "      <td>Before: 0.15911448638969528, After: 0.27407454...</td>\n",
       "      <td>Before: 0.09505575751009387, After: 0.32852584...</td>\n",
       "      <td>Before: 0.11200247669649024, After: 0.24947266...</td>\n",
       "      <td>Before: 0.14771102908578468, After: 0.33078374...</td>\n",
       "      <td>Before: 0.3033847603340788, After: 0.544714533...</td>\n",
       "      <td>Before: 0.6110191309495465, After: 0.999999999...</td>\n",
       "      <td>Before: 0.30399420253175347, After: 0.36164712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>Before: 0.225917972105598, After: 0.2598908452...</td>\n",
       "      <td>Before: 0.1655264826766013, After: 0.188078452...</td>\n",
       "      <td>Before: 0.036750539503579295, After: 0.0769532...</td>\n",
       "      <td>Before: 0.09429740737651135, After: 0.10639914...</td>\n",
       "      <td>Before: 0.10212970436490482, After: 0.11862340...</td>\n",
       "      <td>Before: 0.13403080874265905, After: 0.06629354...</td>\n",
       "      <td>Before: 0.1436405909297276, After: 0.175547118...</td>\n",
       "      <td>Before: 0.26868181343357167, After: 0.40912927...</td>\n",
       "      <td>Before: 0.3049306009871644, After: 0.361647126...</td>\n",
       "      <td>Before: 0.4896246955281065, After: 1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        cat  \\\n",
       "cat       Before: 0.29245239862991185, After: 1.00000000...   \n",
       "tiger     Before: 0.17347637872059773, After: 0.20200897...   \n",
       "computer  Before: 0.18984798734723213, After: 0.19719178...   \n",
       "keyboard  Before: 0.20546589125510897, After: 0.20547455...   \n",
       "plane     Before: 0.16761576142114898, After: 0.39445950...   \n",
       "car       Before: 0.15859645192729996, After: 0.26348610...   \n",
       "doctor    Before: 0.25878907406446877, After: 0.34338708...   \n",
       "nurse     Before: 0.18217682480604822, After: 0.54854860...   \n",
       "love      Before: 0.241072229246228, After: 0.5940531024...   \n",
       "sex       Before: 0.225917972105598, After: 0.2598908452...   \n",
       "\n",
       "                                                      tiger  \\\n",
       "cat       Before: 0.35037322527996934, After: 0.20200897...   \n",
       "tiger     Before: 0.48802072485209846, After: 1.00000000...   \n",
       "computer  Before: 0.05360514929979338, After: 0.09025702...   \n",
       "keyboard  Before: 0.18314156317766062, After: 0.18313975...   \n",
       "plane     Before: 0.06730278214710199, After: 0.07307479...   \n",
       "car       Before: -0.049076379792710186, After: 0.100330...   \n",
       "doctor    Before: 0.017639064485218965, After: 0.0890835...   \n",
       "nurse     Before: 0.07222178145580241, After: 0.20259713...   \n",
       "love      Before: 0.10199943514005788, After: 0.19102605...   \n",
       "sex       Before: 0.1655264826766013, After: 0.188078452...   \n",
       "\n",
       "                                                   computer  \\\n",
       "cat       Before: 0.06136659928459301, After: 0.19719178...   \n",
       "tiger     Before: 0.02942476361382193, After: 0.09025702...   \n",
       "computer             Before: 0.3047689400402037, After: 1.0   \n",
       "keyboard  Before: 0.2393287663091698, After: 0.239342157...   \n",
       "plane     Before: 0.06606645196552532, After: 0.40858220...   \n",
       "car       Before: 0.13968323112249992, After: 0.22732308...   \n",
       "doctor    Before: 0.0978054983995635, After: 0.209869514...   \n",
       "nurse     Before: 0.11697573887301807, After: 0.12068024...   \n",
       "love      Before: 0.0730022470894344, After: 0.234808759...   \n",
       "sex       Before: 0.036750539503579295, After: 0.0769532...   \n",
       "\n",
       "                                                   keyboard  \\\n",
       "cat       Before: 0.18344955625364173, After: 0.20547455...   \n",
       "tiger     Before: 0.06542581824273716, After: 0.18313975...   \n",
       "computer  Before: 0.39639163439495995, After: 0.23934215...   \n",
       "keyboard             Before: 0.9999999999999996, After: 1.0   \n",
       "plane     Before: 0.10055138151211143, After: 0.28970944...   \n",
       "car       Before: 0.14983822223318854, After: 0.15750846...   \n",
       "doctor    Before: 0.08500327165730943, After: 0.13804958...   \n",
       "nurse     Before: 0.12199094008346709, After: 0.16678347...   \n",
       "love      Before: 0.15911448638969528, After: 0.27407454...   \n",
       "sex       Before: 0.09429740737651135, After: 0.10639914...   \n",
       "\n",
       "                                                      plane  \\\n",
       "cat       Before: 0.21134097025538556, After: 0.39445950...   \n",
       "tiger     Before: 0.15090617196611955, After: 0.07307479...   \n",
       "computer  Before: 0.28314903703275535, After: 0.40858220...   \n",
       "keyboard  Before: 0.28971014677665063, After: 0.28970944...   \n",
       "plane     Before: 0.38405259310022044, After: 1.00000000...   \n",
       "car       Before: 0.2597415410728325, After: 0.354037032...   \n",
       "doctor    Before: 0.13439994429915442, After: 0.21833826...   \n",
       "nurse     Before: 0.12790409339777273, After: 0.28236372...   \n",
       "love      Before: 0.09505575751009387, After: 0.32852584...   \n",
       "sex       Before: 0.10212970436490482, After: 0.11862340...   \n",
       "\n",
       "                                                        car  \\\n",
       "cat       Before: 0.136072027917602, After: 0.2634861035...   \n",
       "tiger     Before: 0.16119598769364896, After: 0.10033040...   \n",
       "computer  Before: 0.26826894486108244, After: 0.22732308...   \n",
       "keyboard  Before: 0.15750632650840493, After: 0.15750846...   \n",
       "plane     Before: 0.3219357387657039, After: 0.354037032...   \n",
       "car       Before: 0.6158574248530795, After: 1.000000000...   \n",
       "doctor    Before: 0.09431570687955, After: 0.19190468704...   \n",
       "nurse     Before: 0.07363428182232754, After: 0.16077850...   \n",
       "love      Before: 0.11200247669649024, After: 0.24947266...   \n",
       "sex       Before: 0.13403080874265905, After: 0.06629354...   \n",
       "\n",
       "                                                     doctor  \\\n",
       "cat       Before: 0.16547475026405636, After: 0.34338708...   \n",
       "tiger     Before: 0.0774108002811773, After: 0.089083542...   \n",
       "computer  Before: 0.18039329196329013, After: 0.20986951...   \n",
       "keyboard  Before: 0.13804955762057758, After: 0.13804958...   \n",
       "plane     Before: 0.15146728859985834, After: 0.21833826...   \n",
       "car       Before: 0.13778205919388814, After: 0.19190468...   \n",
       "doctor               Before: 0.6128107065755533, After: 1.0   \n",
       "nurse     Before: 0.4308149649767604, After: 0.344486463...   \n",
       "love      Before: 0.14771102908578468, After: 0.33078374...   \n",
       "sex       Before: 0.1436405909297276, After: 0.175547118...   \n",
       "\n",
       "                                                      nurse  \\\n",
       "cat       Before: 0.24690899138830727, After: 0.54854860...   \n",
       "tiger     Before: 0.1697249630498177, After: 0.202597136...   \n",
       "computer  Before: 0.09297679298707379, After: 0.12068024...   \n",
       "keyboard  Before: 0.16678031434047627, After: 0.16678347...   \n",
       "plane     Before: 0.10589313234551631, After: 0.28236372...   \n",
       "car       Before: 0.0865221800713605, After: 0.160778501...   \n",
       "doctor    Before: 0.27845097194129365, After: 0.34448646...   \n",
       "nurse     Before: 0.378466332225932, After: 1.0000000000...   \n",
       "love      Before: 0.3033847603340788, After: 0.544714533...   \n",
       "sex       Before: 0.26868181343357167, After: 0.40912927...   \n",
       "\n",
       "                                                       love  \\\n",
       "cat       Before: 0.2989067535773432, After: 0.594053102...   \n",
       "tiger     Before: 0.17516455047450735, After: 0.19102605...   \n",
       "computer  Before: 0.1168711356729625, After: 0.234808759...   \n",
       "keyboard  Before: 0.27407501845767246, After: 0.27407454...   \n",
       "plane     Before: 0.1936940498815496, After: 0.328525840...   \n",
       "car       Before: 0.22133439390681858, After: 0.24947266...   \n",
       "doctor    Before: 0.19325643013428553, After: 0.33078374...   \n",
       "nurse     Before: 0.1708752362535986, After: 0.544714533...   \n",
       "love      Before: 0.6110191309495465, After: 0.999999999...   \n",
       "sex       Before: 0.3049306009871644, After: 0.361647126...   \n",
       "\n",
       "                                                        sex  \n",
       "cat       Before: 0.1093022564011111, After: 0.259890845...  \n",
       "tiger     Before: 0.14518818082220147, After: 0.18807845...  \n",
       "computer  Before: 0.1158779845883439, After: 0.076953257...  \n",
       "keyboard  Before: 0.10639718565916091, After: 0.10639914...  \n",
       "plane     Before: 0.05697047025775614, After: 0.11862340...  \n",
       "car       Before: 0.059270287407368415, After: 0.0662935...  \n",
       "doctor    Before: 0.0745189116843016, After: 0.175547118...  \n",
       "nurse     Before: 0.1264203000118017, After: 0.409129275...  \n",
       "love      Before: 0.30399420253175347, After: 0.36164712...  \n",
       "sex                  Before: 0.4896246955281065, After: 1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(index=wordList, columns=wordList)\n",
    "\n",
    "# Loop over each word pair and calculate similarity scores\n",
    "for word1 in wordList:\n",
    "    for word2 in wordList:\n",
    "        word1_index = wordList.index(word1)\n",
    "        word2_index = wordList.index(word2)\n",
    "        \n",
    "        # Calculate similarity score before retrofitting\n",
    "        similarity_before = cosine_sim[word1_index, word2_index]\n",
    "        \n",
    "        # Calculate similarity score after retrofitting\n",
    "        retrofit_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "        word1_vec = retrofit_toy_vec[word1_index].reshape(1, -1)\n",
    "        word2_vec = retrofit_toy_vec[word2_index].reshape(1, -1)\n",
    "        similarity_after = cosine_similarity(word1_vec, word2_vec)[0, 0]\n",
    "        \n",
    "        # Store the scores in the DataFrame\n",
    "        results_df.loc[word1, word2] = f\"Before: {similarity_before}, After: {similarity_after}\"\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>tiger</th>\n",
       "      <th>computer</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>plane</th>\n",
       "      <th>car</th>\n",
       "      <th>doctor</th>\n",
       "      <th>nurse</th>\n",
       "      <th>love</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>Retrofitting: 0.20, Human: 0.73</td>\n",
       "      <td>Retrofitting: 1.00, Human: 1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.24, Human: 0.76</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyboard</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plane</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.35, Human: 0.58</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doctor</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.34, Human: 0.70</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Retrofitting: 0.36, Human: 0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cat                            tiger  \\\n",
       "cat                                  None                             None   \n",
       "tiger     Retrofitting: 0.20, Human: 0.73  Retrofitting: 1.00, Human: 1.00   \n",
       "computer                             None                             None   \n",
       "keyboard                             None                             None   \n",
       "plane                                None                             None   \n",
       "car                                  None                             None   \n",
       "doctor                               None                             None   \n",
       "nurse                                None                             None   \n",
       "love                                 None                             None   \n",
       "sex                                  None                             None   \n",
       "\n",
       "         computer                         keyboard plane  \\\n",
       "cat          None                             None  None   \n",
       "tiger        None                             None  None   \n",
       "computer     None  Retrofitting: 0.24, Human: 0.76  None   \n",
       "keyboard     None                             None  None   \n",
       "plane        None                             None  None   \n",
       "car          None                             None  None   \n",
       "doctor       None                             None  None   \n",
       "nurse        None                             None  None   \n",
       "love         None                             None  None   \n",
       "sex          None                             None  None   \n",
       "\n",
       "                                      car doctor  \\\n",
       "cat                                  None   None   \n",
       "tiger                                None   None   \n",
       "computer                             None   None   \n",
       "keyboard                             None   None   \n",
       "plane     Retrofitting: 0.35, Human: 0.58   None   \n",
       "car                                  None   None   \n",
       "doctor                               None   None   \n",
       "nurse                                None   None   \n",
       "love                                 None   None   \n",
       "sex                                  None   None   \n",
       "\n",
       "                                    nurse  love  \\\n",
       "cat                                  None  None   \n",
       "tiger                                None  None   \n",
       "computer                             None  None   \n",
       "keyboard                             None  None   \n",
       "plane                                None  None   \n",
       "car                                  None  None   \n",
       "doctor    Retrofitting: 0.34, Human: 0.70  None   \n",
       "nurse                                None  None   \n",
       "love                                 None  None   \n",
       "sex                                  None  None   \n",
       "\n",
       "                                      sex  \n",
       "cat                                  None  \n",
       "tiger                                None  \n",
       "computer                             None  \n",
       "keyboard                             None  \n",
       "plane                                None  \n",
       "car                                  None  \n",
       "doctor                               None  \n",
       "nurse                                None  \n",
       "love      Retrofitting: 0.36, Human: 0.68  \n",
       "sex                                  None  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the evaluation scores from the file\n",
    "eval_scores = {}\n",
    "with open(eval_file_path, 'r') as eval_file:\n",
    "    for line in eval_file:\n",
    "        word1, word2, score = line.strip().split('\\t')\n",
    "        eval_scores[(word1, word2)] = float(score)\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(index=wordList, columns=wordList)\n",
    "\n",
    "# Loop over each word pair and calculate similarity scores\n",
    "for word1 in wordList:\n",
    "    for word2 in wordList:\n",
    "        word1_index = wordList.index(word1)\n",
    "        word2_index = wordList.index(word2)\n",
    "        \n",
    "        # Calculate similarity score after retrofitting\n",
    "        retrofit_toy_vec, _ = retrofitting_wordVecs(wordVecMat, neighbors_matrix, alpha, beta, nb_iter)\n",
    "        word1_vec = retrofit_toy_vec[word1_index].reshape(1, -1)\n",
    "        word2_vec = retrofit_toy_vec[word2_index].reshape(1, -1)\n",
    "        similarity_after = cosine_similarity(word1_vec, word2_vec)[0, 0]\n",
    "        \n",
    "        # Retrieve the evaluation score for the word pair\n",
    "        score = eval_scores.get((word1, word2))\n",
    "\n",
    "        # Scale the human score between 0 and 1\n",
    "        if score is not None:\n",
    "            scaled_score = score / 10.0\n",
    "        else:\n",
    "            scaled_score = None\n",
    "        \n",
    "        # Store the scores in the DataFrame\n",
    "        results_df.loc[word1, word2] = f\"Retrofitting: {similarity_after:.2f}, Human: {scaled_score:.2f}\" if scaled_score is not None else None\n",
    "\n",
    "# Print the results DataFrame\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectors read from: ../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean \n"
     ]
    }
   ],
   "source": [
    "wordVecs_gensim = read_word_vecs(\"../data/English/wordEmbeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\")\n",
    "lexical_similarity = read_lexicon(\"../data/English/lexicon/ws353_lexical_similarity.txt\")\n",
    "output_file_gensim = \"../data/English/output_vectors/output_vectors.txt\"\n",
    "outFileName_gensim = output_file_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_neighbors_embedding_matrix0(wordVecs, wordList, relation_type):\n",
    "    # Retrieve synonyms for each word\n",
    "    neighbors_dict = get_wordnet_lexicon(wordList, relation_type)\n",
    "\n",
    "    # Create a set of valid neighbors\n",
    "    valid_neighbors = set(neighbor for neighbors in neighbors_dict.values() for neighbor in neighbors) & set(wordList)\n",
    "    \n",
    "    # Get the embedding size\n",
    "    embedding_size = 250 #wordVecs[next(iter(wordVecs))].shape[0]\n",
    "    \n",
    "    # Compute average embedding\n",
    "    average_embeddings = []\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        if neighbors and any(neighbor in valid_neighbors for neighbor in neighbors):\n",
    "            embeddings = np.array([\n",
    "                wordVecs[wordList.index(neighbor)]\n",
    "                for neighbor in neighbors\n",
    "                if neighbor in valid_neighbors\n",
    "            ])\n",
    "            average_embedding = np.mean(embeddings, axis=0)\n",
    "            average_embeddings.append(average_embedding)\n",
    "    \n",
    "    # Create the word embedding matrix\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "\n",
    "    return neighbors_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_dict = get_wordnet_lexicon(wordList, \"synononys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_neighbors_embedding_matrix(wordVecMat, wordList, neighbors_dict):\n",
    "    valid_neighbors = set(neighbor for neighbors in neighbors_dict.values() for neighbor in neighbors) & set(wordList)\n",
    "\n",
    "    embedding_size = wordVecMat.shape[1]\n",
    "    average_embeddings = []\n",
    "\n",
    "    for word in wordList:\n",
    "        neighbors = neighbors_dict.get(word, [])\n",
    "        if neighbors and any(neighbor in valid_neighbors for neighbor in neighbors):\n",
    "            embeddings = np.array([\n",
    "                wordVecMat[wordList.index(neighbor)] if neighbor in wordList else np.zeros(embedding_size)\n",
    "                for neighbor in neighbors\n",
    "                if neighbor in valid_neighbors\n",
    "            ])\n",
    "            if embeddings.size > 0:\n",
    "                average_embedding = np.mean(embeddings, axis=0)\n",
    "                average_embeddings.append(average_embedding)\n",
    "        else:\n",
    "            average_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "    neighbors_embedding_matrix = np.vstack(average_embeddings)\n",
    "    return neighbors_embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_gensim = get_embeddings_words(wordVecs_gensim)\n",
    "wordVecMat_gensim = convert_dict_to_matrix(wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix_gensim = retrieve_neighbors_embedding_matrix(wordVecMat_gensim, wordList_gensim, neighbors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create a small subset of wordVecs dictionary\n",
    "subset_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:100]}\n",
    "# subset_neighbors_dict = {word: neighbors_dict[word] for word in neighbors_dict[:100]}\n",
    "subset_wordVecMat = wordVecMat_gensim[:100] \n",
    "\n",
    "# Create a small subset of wordList\n",
    "subset_wordList = wordList_gensim[:100]\n",
    "\n",
    "# Test the function on the subset\n",
    "neighbors_matrix = retrieve_neighbors_embedding_matrix(subset_wordVecMat, subset_wordList, neighbors_dict)\n",
    "\n",
    "# Print the result\n",
    "print(neighbors_matrix)\n",
    "print(type(neighbors_matrix))  \n",
    "print(neighbors_matrix.shape)  \n",
    "print(neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(125776, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(neighbors_matrix_gensim))  \n",
    "print(neighbors_matrix_gensim.shape)  \n",
    "print(neighbors_matrix_gensim.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(125776, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat_gensim))  \n",
    "print(wordVecMat_gensim.shape) \n",
    "print(wordVecMat_gensim.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10, 300)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(wordVecMat))  \n",
    "print(wordVecMat.shape) \n",
    "print(wordVecMat.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_gensim = get_wordnet_lexicon(wordList_gensim, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix_gensim = generate_cosine_similarity_matrix(wordVecs_gensim)\n",
    "# retrofitted_similarity_matrix_gensim = generate_cosine_similarity_matrix(wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofitting_wordVecs_test(wordVecMat, neighbors_embedding_matrix, alpha=1, beta=1, nb_iter=10):\n",
    "    newWordVecMat = np.copy(wordVecMat)\n",
    "    for _ in range(nb_iter):\n",
    "        updates = alpha * neighbors_embedding_matrix + beta * newWordVecMat\n",
    "        newWordVecMat = updates / (alpha + beta)\n",
    "\n",
    "    return newWordVecMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 250)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "test_subset_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:50]}\n",
    "test_subset_wordVecMat = wordVecMat_gensim[:50] \n",
    "\n",
    "test_subset_wordList = wordList_gensim[:50]\n",
    "\n",
    "test_subset_neighbors_matrix= retrieve_neighbors_embedding_matrix(test_subset_wordVecMat, test_subset_wordList, neighbors_dict)\n",
    "\n",
    "print(test_subset_neighbors_matrix)\n",
    "print(type(test_subset_neighbors_matrix))  \n",
    "print(test_subset_neighbors_matrix.shape)  \n",
    "print(test_subset_neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with \",\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"the\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \".\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"of\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"-\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"and\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"in\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"to\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"'\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"a\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \")\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"(\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"is\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"s\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"for\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"was\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"on\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"that\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"as\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"it\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"with\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"by\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"\"\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"at\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"he\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"from\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"be\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"this\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"i\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"an\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"his\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"are\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"not\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"has\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"have\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"but\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"or\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"utc\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"which\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"were\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"–\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"said\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"they\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"also\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"one\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"who\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"had\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"talk\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"new\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n",
      "Similarities with \"their\":\n",
      "  - \",\": nan\n",
      "  - \"the\": nan\n",
      "  - \".\": nan\n",
      "  - \"of\": nan\n",
      "  - \"-\": nan\n",
      "  - \"and\": nan\n",
      "  - \"in\": nan\n",
      "  - \"to\": nan\n",
      "  - \"'\": nan\n",
      "  - \"a\": nan\n",
      "  - \")\": nan\n",
      "  - \"(\": nan\n",
      "  - \"is\": nan\n",
      "  - \"s\": nan\n",
      "  - \"for\": nan\n",
      "  - \"was\": nan\n",
      "  - \"on\": nan\n",
      "  - \"that\": nan\n",
      "  - \"as\": nan\n",
      "  - \"it\": nan\n",
      "  - \"with\": nan\n",
      "  - \"by\": nan\n",
      "  - \"\"\": nan\n",
      "  - \"at\": nan\n",
      "  - \"he\": nan\n",
      "  - \"from\": nan\n",
      "  - \"be\": nan\n",
      "  - \"this\": nan\n",
      "  - \"i\": nan\n",
      "  - \"an\": nan\n",
      "  - \"his\": nan\n",
      "  - \"are\": nan\n",
      "  - \"not\": nan\n",
      "  - \"has\": nan\n",
      "  - \"have\": nan\n",
      "  - \"but\": nan\n",
      "  - \"or\": nan\n",
      "  - \"utc\": nan\n",
      "  - \"which\": nan\n",
      "  - \"were\": nan\n",
      "  - \"–\": nan\n",
      "  - \"said\": nan\n",
      "  - \"they\": nan\n",
      "  - \"also\": nan\n",
      "  - \"one\": nan\n",
      "  - \"who\": nan\n",
      "  - \"had\": nan\n",
      "  - \"talk\": nan\n",
      "  - \"new\": nan\n",
      "  - \"their\": nan\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ninan\\AppData\\Local\\Temp\\ipykernel_10784\\4190515266.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / norm_product\n"
     ]
    }
   ],
   "source": [
    "test_before_retrofitted_gensim_dict = convert_matrix_to_dict(test_subset_neighbors_matrix, test_subset_wordList)\n",
    "test_before_retrofitted_gensim_similarity_matrix = generate_cosine_similarity_matrix(test_before_retrofitted_gensim_dict)\n",
    "print_vec_similarities(test_subset_wordList, test_before_retrofitted_gensim_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 240. MiB for an array with shape (125776, 250) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retrofitted_wordVecs_gensim \u001b[39m=\u001b[39m retrofitting_wordVecs(wordVecMat_gensim, neighbors_matrix_gensim, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, nb_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[58], line 11\u001b[0m, in \u001b[0;36mretrofitting_wordVecs\u001b[1;34m(wordVecMat, neighbors_mean_matrix, alpha, beta, nb_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m updates \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_iter):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# Calculate the number of neighbors for each word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[39m# numNeighbors = np.sum(neighbors_mean_matrix != 0, axis=1)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m     \u001b[39m# Update the word embeddings using retrofitting formula\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     newWordVecMat \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39;49m newWordVecMat \u001b[39m+\u001b[39;49m beta \u001b[39m*\u001b[39;49m neighbors_mean_matrix) \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m beta)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Calculate the updates\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     update \u001b[39m=\u001b[39m newWordVecMat \u001b[39m-\u001b[39m wordVecMat\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 240. MiB for an array with shape (125776, 250) and data type float64"
     ]
    }
   ],
   "source": [
    "retrofitted_wordVecs_gensim = retrofitting_wordVecs(wordVecMat_gensim, neighbors_matrix_gensim, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_cosine_similarity(X, Y, sample_size=10000):\n",
    "    np.random.seed(42)  # Set a random seed for reproducibility\n",
    "    sample_X = X[np.random.choice(X.shape[0], sample_size, replace=False)]\n",
    "    sample_Y = Y[np.random.choice(Y.shape[0], sample_size, replace=False)]\n",
    "    similarities = cosine_similarity(sample_X, sample_Y)\n",
    "    avg_cos_similarity = np.mean(similarities)\n",
    "    return avg_cos_similarity\n",
    "\n",
    "# Compute the average cosine similarity using a random sample\n",
    "avg_cos_similarity = calculate_average_cosine_similarity(wordVecMat_gensim, retrofitted_wordVecs_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_retrofitted_wordVecs = {word: wordVecs_gensim[word] for word in wordList_gensim[:50]}\n",
    "subset_retrofitted_wordVecMat = retrofitted_wordVecs_gensim[:50] \n",
    "\n",
    "subset_retrofitted_wordList = wordList_gensim[:50]\n",
    "\n",
    "neighbors_matrix = retrieve_neighbors_embedding_matrix(subset_retrofitted_wordVecMat, subset_retrofitted_wordList, neighbors_dict)\n",
    "\n",
    "print(neighbors_matrix)\n",
    "print(type(neighbors_matrix))  \n",
    "print(neighbors_matrix.shape)  \n",
    "print(neighbors_matrix.ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_new_retrofitted_gensim_dict = convert_matrix_to_dict(subset_retrofitted_wordVecMat, subset_retrofitted_wordList)\n",
    "new_retrofitted_gensim_similarity_matrix = generate_cosine_similarity_matrix(subset_new_retrofitted_gensim_dict)\n",
    "print_vec_similarities(wordList_gensim, new_retrofitted_gensim_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only considering the words in the lexicon similarity file\n",
    "# Retrive words from the lexical similarity file\n",
    "\n",
    "def print_lexical_similarities(wordVecs, lines):\n",
    "    # Create a list to store the words\n",
    "    word_list = []\n",
    "\n",
    "    # Iterate over the lines and extract the words\n",
    "    for line in lines:\n",
    "        words = line.strip().split('\\t')\n",
    "        word1 = words[0]\n",
    "        word2 = words[1]\n",
    "        word_list.append((word1, word2))  # Store the words as a tuple\n",
    "\n",
    "    # Determine the subset of words present in the wordVecs file while preserving the order\n",
    "    subset = [word for word in word_list if word[0] in wordVecs and word[1] in wordVecs]\n",
    "\n",
    "    # Create a dictionary to map words to indices\n",
    "    w2i = {word: index for index, word in enumerate(wordVecs)}\n",
    "\n",
    "    # Create an empty list to store the similarities\n",
    "    similarities = []\n",
    "\n",
    "    # Iterate over each tuple in the subset\n",
    "    for word1, word2 in subset:\n",
    "        if word1 in wordVecs and word2 in wordVecs:\n",
    "            # Retrieve the embeddings for the words\n",
    "            embedding1 = wordVecs[word1]\n",
    "            embedding2 = wordVecs[word2]\n",
    "\n",
    "            # Calculate the similarity between the embeddings\n",
    "            similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "            # Append the similarity value to the list of similarities\n",
    "            similarities.append(similarity_score)\n",
    "\n",
    "    # Print the similarities\n",
    "    for i, similarity_score in enumerate(similarities):\n",
    "        print(f\"Similarity between {subset[i][0]} and {subset[i][1]}: {similarity_score[0][0]}\")\n",
    "\n",
    "\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print_lexical_similarities(wordVecs_gensim, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_similarities(wordVecs, lines, subset):\n",
    "    similarities = []\n",
    "\n",
    "    for word1, word2 in subset:\n",
    "        if word1 in wordVecs and word2 in wordVecs:\n",
    "            embedding1 = wordVecs[word1]\n",
    "            embedding2 = wordVecs[word2]\n",
    "\n",
    "            similarity_score = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "            similarities.append((word1, word2, similarity_score[0][0]))\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "with open('../data/English/lexicon/ws353_lexical_similarity.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "word_list = []\n",
    "for line in lines:\n",
    "    words = line.strip().split('\\t')\n",
    "    word1 = words[0]\n",
    "    word2 = words[1]\n",
    "    word_list.append((word1, word2))\n",
    "\n",
    "subset = [(word1, word2) for word1, word2 in word_list if word1 in wordVecs_gensim and word2 in wordVecs_gensim]\n",
    "\n",
    "similarities = get_lexical_similarities(wordVecs_gensim, lines, subset)\n",
    "\n",
    "for word1, word2, similarity_score in similarities:\n",
    "    print(f\"Similarity between {word1} and {word2}: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update_EN = -1  # Variable to store the best similarity score\n",
    "best_params_EN = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_EN_vec = retrofitting_wordVecs_test(wordVecMat_gensim, neighbors_matrix_gensim, alpha, beta, nb_iter)\n",
    "            embed_update_EN = measure_embedding_updates(wordVecMat_gensim, retrofitted_EN_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update_EN > best_embed_update_EN:\n",
    "                best_embed_update_EN = embed_update_EN\n",
    "                best_params_EN = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params_EN)\n",
    "print(\"Best embedding update:\", best_embed_update_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gensim = convert_matrix_to_dict(retrofitted_wordVecs_gensim, wordList_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_word_vecs(output_gensim, outFileName_gensim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs_FR = read_word_vecs(\"../data/French/word_embeddings/vecs100-linear-frwiki\")\n",
    "lexical_similarity_FR = read_lexicon(\"../data/French/lexicon/rg65_french.txt\")\n",
    "output_file_FR = \"../data/French/output_vectors/output_vectors.txt\"\n",
    "outFileName_FR = output_file_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_lexicon_FR(target_words, relation_types):\n",
    "    lexicon = {}\n",
    "        \n",
    "    for word in target_words:\n",
    "        related_words = []\n",
    "        word_synsets = wordnet.synsets(word)\n",
    "        \n",
    "        # Skip word if no synsets found\n",
    "        if not word_synsets:\n",
    "            continue\n",
    "\n",
    "        for syn in word_synsets:\n",
    "            for lemma in syn.lemmas('fra'):\n",
    "                if lemma.name() != word:\n",
    "                    if \"synonyms\" in relation_types:\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"antonyms\" in relation_types:\n",
    "                if syn.lemmas('fra')[0].antonyms():\n",
    "                    related_words.append(syn.lemmas()[0].antonyms()[0].name())\n",
    "            if \"hyponyms\" in relation_types:\n",
    "                for hypo in syn.hyponyms():\n",
    "                    for lemma in hypo.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"hypernyms\" in relation_types:\n",
    "                for hyper in syn.hypernyms():\n",
    "                    for lemma in hyper.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"meronyms\" in relation_types:\n",
    "                for part in syn.part_meronyms():\n",
    "                    for lemma in part.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"holonyms\" in relation_types:\n",
    "                for whole in syn.part_holonyms():\n",
    "                    for lemma in whole.lemmas('fra'):\n",
    "                        related_words.append(lemma.name())\n",
    "            if \"homonyms\" in relation_types:\n",
    "                for lemma in syn.lemmas('fra'):\n",
    "                    if lemma.name() != word:\n",
    "                        homonyms = wordnet.lemmas(lemma.name())\n",
    "                        for homonym in homonyms:\n",
    "                            related_words.append(homonym.name())\n",
    "        lexicon[word] = related_words\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_dict = get_wordnet_lexicon_FR(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList_FR = get_embeddings_words(wordVecs_FR)\n",
    "wordVecMat_FR = convert_dict_to_matrix(wordVecs_FR)\n",
    "neighbors_dict_FR = get_wordnet_lexicon(wordList, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix_FR = retrieve_neighbors_embedding_matrix(wordVecMat_FR, wordList_FR, neighbors_dict_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrofitted_wordVecs_FR, updates_FR= retrofitting_wordVecs_test(wordVecMat_FR, neighbors_matrix_FR, alpha=1, beta=1, nb_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cos_similarity = calculate_average_cosine_similarity(wordVecMat_FR, retrofitted_wordVecs_FR)\n",
    "avg_cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/French/lexicon/rg65_french.txt', 'r') as file:\n",
    "    lines_FR = file.readlines()\n",
    "\n",
    "print_lexical_similarities(wordVecs_FR, lines_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best values for hyperparameters\n",
    "best_embed_update_FR = -1  # Variable to store the best similarity score\n",
    "best_params_FR = {}  # Dictionary to store the best hyperparameter values\n",
    "\n",
    "for alpha in np.arange(0.1, 5.1, 0.2):\n",
    "    for beta in np.arange(0.1, 5.1, 0.2):\n",
    "        for nb_iter in range(1,16):\n",
    "            retrofitted_FR_vec = retrofitting_wordVecs_test(wordVecMat_FR, neighbors_matrix_FR, alpha, beta, nb_iter)\n",
    "            embed_update_FR = measure_embedding_updates(wordVecMat_FR, retrofitted_FR_vec)\n",
    "            # print(\" alpha =\", alpha, \" beta=\", beta, \"nb_iter =\", nb_iter, \" similarity score =\", similarity_score)\n",
    "            if embed_update_FR > best_embed_update_FR:\n",
    "                best_embed_update_FR = embed_update_FR\n",
    "                best_params_FR = {'alpha': alpha, 'beta': beta, 'nb_iter': nb_iter}\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params_FR)\n",
    "print(\"Best embedding update:\", best_embed_update_FR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_FR = convert_matrix_to_dict(retrofitted_wordVecs_FR, wordList_FR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
